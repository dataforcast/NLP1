{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "is_w2vec_saved = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "def get_trained_w2vec(model_filename, X, epoch):\n",
    "    #-------------------------------------------------------------\n",
    "    # Check if a model with same name still exists\n",
    "    #-------------------------------------------------------------\n",
    "    if model_filename.split('/')[-1] in os.listdir('./data'):\n",
    "        is_w2vec_saved = True\n",
    "    else :\n",
    "        is_w2vec_saved = False\n",
    "\n",
    "    if is_w2vec_saved is False :        \n",
    "        #-------------------------------------------------------------------------------------\n",
    "        # Some words are obfuscated. Then they may appear once in all corpus.\n",
    "        # In order to avoid this, then min_count is fixed to 1.\n",
    "        # Window is fixed to 4 : 2 words before and 2 words after central word.\n",
    "        #-------------------------------------------------------------------------------------\n",
    "        print(\"Training W2VEC model...\")\n",
    "        model_w2vec = Word2Vec(min_count=2, workers=6)\n",
    "        model_w2vec.build_vocab(X)  # prepare the model vocabulary\n",
    "        model_w2vec.train(X, total_examples=model_w2vec.corpus_count, epochs=epochs) \n",
    "        print(\"Done!\\n\")\n",
    "        model_w2vec.save(model_filename)\n",
    "        print(\"Model saved!\\n\")\n",
    "    else :\n",
    "        print(\"Loading W2VEC model...\")\n",
    "        model_w2vec = Word2Vec.load(model_filename)\n",
    "        print(\"Done!\\n\")\n",
    "    return model_w2vec\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def clean_X_label(X, label) :\n",
    "    \n",
    "    ser_ = pd.Series(X)\n",
    "\n",
    "    list_index = [i for i in ser_.index if len(ser_[i])==0]\n",
    "    \n",
    "    ser_.drop(list_index, inplace=True)\n",
    "    list_to_clean_1 = ser_.tolist()\n",
    "    \n",
    "    print(\"Cleaned empty text = {}\".format(len(list_index)))\n",
    "    ser_ = pd.Series(label)\n",
    "    ser_.drop(list_index, inplace=True)\n",
    "\n",
    "    list_to_clean_2 = ser_.tolist()\n",
    "    \n",
    "    return list_to_clean_1, list_to_clean_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def build_embedding_matrix(w2vec, list_X):\n",
    "    dim = w2vec.wv.vectors.view().shape[1]\n",
    "    matrix = np.zeros(dim)\n",
    "    zero_vec = np.zeros(dim)\n",
    "\n",
    "    # Words from that are not in vocabulary are replaced with a zero vector.\n",
    "    # Empty text, mean `list_corpus[i]` is empty is replaced with a zero vector.\n",
    "\n",
    "    for i in range(0, len(list_X),1) :\n",
    "        #---------------------------------------------------------------------------------------\n",
    "        # Text vectorization : mean of words components for text components\n",
    "        # In case word does not belongs to vacabulatory, then zero vector replace it.\n",
    "        #---------------------------------------------------------------------------------------\n",
    "        arr1 = np.mean( [w2vec.wv[word] for word in list_X[i]  if word in w2vec.wv.vocab], axis=0 )\n",
    "\n",
    "        #---------------------------------------------------------------------------------------\n",
    "        # Check empty text (empty list of words) and replace it with ezro vector, when required\n",
    "        #---------------------------------------------------------------------------------------\n",
    "        if 0 == len(arr1.shape) :\n",
    "            print(i)\n",
    "            arr1 = zero_vec\n",
    "        else :\n",
    "            pass\n",
    "\n",
    "        #---------------------------------------------------------------------------------------\n",
    "        # Matrix of corpus is computed.\n",
    "        # Each raw is a text from corpus while each column is a feature of the corpus.\n",
    "        #---------------------------------------------------------------------------------------\n",
    "        matrix = np.vstack((matrix,arr1))\n",
    "    #--------------------------------------------------------\n",
    "    # Remove first raw from matrix\n",
    "    #--------------------------------------------------------\n",
    "    matrix = matrix[1:]\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample(ser_X, ser_y, ratio=-1) :\n",
    "    sample_length = int(len(ser_X)*ratio)\n",
    "    return ser_X.sample(sample_length).values,ser_y.sample(sample_length).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanW2VecEmbedding():\n",
    "    def __init__(self, w2vec) :\n",
    "        self._w2vec = w2vec\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self,X,y):\n",
    "        range_index = range(0, len(X),1)\n",
    "        list_X_y = [ (gensim.utils.simple_preprocess(X[index], deacc=False, min_len=2), y[index]) for index in range_index]\n",
    "        list_X = [list_X_y[i][0] for i in range(0, len(list_X_y),1)]\n",
    "        list_y = [list_X_y[i][1] for i in range(0, len(list_X_y),1)]\n",
    "        list_X, list_y = clean_X_label(list_X, list_y)\n",
    "        X = build_embedding_matrix(self._w2vec, list_X)\n",
    "        return X, np.array(list_y)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=blus>1. Loading data</font>\n",
    "\n",
    "All text is lowered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('./data/test.csv.zip', compression='zip', header=0, sep=',', quotechar='\"')\n",
    "df_train = pd.read_csv('./data/train.csv.zip', compression='zip', header=0, sep=',', quotechar='\"')\n",
    "\n",
    "df_train['comment_text'] = df_train['comment_text'].apply(lambda x : x.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=blus>1.1 Data is splitted into Train and Test datasets</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset: X = (1209265,) Label= (1209265,)\n",
      "Test dataset: X = (595609,) Label= (595609,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_train['comment_text'],df_train['target'],test_size=0.33, random_state=42)\n",
    "\n",
    "print(\"Train dataset: X = {} Label= {}\".format(X_train.shape, y_train.shape))\n",
    "print(\"Test dataset: X = {} Label= {}\".format(X_test.shape, y_test.shape))\n",
    "\n",
    "#X_train = X_train[:10000]\n",
    "#y_train = y_train[:10000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For testing models, a ratio of train and test datasets are used.\n",
    "\n",
    "When value is -1, then the whole dataset is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((362779,), (362779,))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratio=0.3\n",
    "\n",
    "X_train, y_train  = get_sample(X_train, y_train, ratio=ratio)\n",
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Same ratio is used for test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((178682,), (178682,))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test, y_test  = get_sample(X_test, y_test, ratio=ratio)\n",
    "X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build a corpus sample from train dataset\n",
    "\n",
    "All texts from corpus are tokenized.\n",
    "\n",
    "Too short tokens (< 2 characters) and too long tokens (> 15 characters) are igonred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_train_corpus_label = [ (gensim.utils.simple_preprocess(X_train[index], deacc=False, min_len=2), y_train[index]) for index in X_train.index]\n",
    "\n",
    "X_train = [list_train_corpus_label[i][0] for i in range(0, len(list_train_corpus_label),1)]\n",
    "y_train = [list_train_corpus_label[i][1] for i in range(0, len(list_train_corpus_label),1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build a corpus sample from test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_test_corpus_label = [ (gensim.utils.simple_preprocess(X_test[index], deacc=False, min_len=2), y_test[index]) for index in X_test.index]\n",
    "\n",
    "X_test = [list_test_corpus_label[i][0] for i in range(0, len(list_test_corpus_label),1)]\n",
    "y_test = [list_test_corpus_label[i][1] for i in range(0, len(list_test_corpus_label),1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Corpus is cleaned.\n",
    "\n",
    "Some text are empty, due to previous pre-processing. They are then removed.\n",
    "Corresponding y value are also removed, based on indexes values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned empty text = 313\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = clean_X_label(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned empty text = 161\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test = clean_X_label(X_test, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=blue>2.Words embeddings with W2VEC</font>\n",
    "\n",
    "W2VEC model is trained then saved.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training W2VEC model...\n",
      "Done!\n",
      "\n",
      "Model saved!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epochs=10\n",
    "model_filename = './data/model_w2vec_'+str(epochs)+'_epochs'\n",
    "\n",
    "w2vec = get_trained_w2vec(model_filename, X_train, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### X_train is transformed into a matrix thanks to W2VEC.\n",
    "\n",
    "In W2VEC, each word is represented as a vector. \n",
    "\n",
    "In this context, a text composed from words may be represented \n",
    "as a linear combination of vectors of words.\n",
    "\n",
    "By default, w2vec model is build as Countinuous Bag of Word (CBOW). This means that by default, the model will be able to predict a word from a given context.\n",
    "\n",
    "\n",
    "The result is a matrix with : \n",
    " * N raws : number of texts into the corpus.\n",
    " * M columns : dimension of W2VEC vectorial space.\n",
    "\n",
    "Number of columns is provided with attribute `w2vec.wv.vector_size`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = build_embedding_matrix(w2vec, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorized X_train and y_train shapes : (362466, 100) / (362466,)\n"
     ]
    }
   ],
   "source": [
    "y_train =  np.array(y_train)\n",
    "print(\"Vectorized X_train and y_train shapes : {} / {}\".format(X_train.shape,y_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bangui/.local/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2920: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/bangui/.local/lib/python3.6/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5350\n",
      "6600\n",
      "7627\n",
      "7861\n",
      "11350\n",
      "15967\n",
      "17606\n",
      "20185\n",
      "23796\n",
      "26204\n",
      "33351\n",
      "39953\n",
      "42592\n",
      "46615\n",
      "47373\n",
      "48015\n",
      "48238\n",
      "51659\n",
      "56403\n",
      "56526\n",
      "57062\n",
      "57858\n",
      "63469\n",
      "63594\n",
      "64064\n",
      "64477\n",
      "64571\n",
      "64797\n",
      "67950\n",
      "71508\n",
      "74502\n",
      "75261\n",
      "76608\n",
      "76776\n",
      "77289\n",
      "77625\n",
      "79163\n",
      "89305\n",
      "91294\n",
      "91468\n",
      "91665\n",
      "93132\n",
      "93159\n",
      "93962\n",
      "95026\n",
      "96100\n"
     ]
    }
   ],
   "source": [
    "X_test = build_embedding_matrix(w2vec, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=blue> 3. Applying estimators</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2_score(y_test, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_predict))\n",
    "print(\"RMSE for Linear Regression: %.4f\" % rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EPOCHS=10\n",
    "#RMSE for Linear Regression: 0.1870\n",
    "\n",
    "#EPOCHS=100\n",
    "#RMSE for Linear Regression: 0.1833\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 XgBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbr = xgb.XGBRegressor(objective ='reg:squarederror', colsample_bytree = 0.3, learning_rate = 0.01,\n",
    "                max_depth = 10, reg_lambda = 5, n_estimators = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = xgbr.predict(X_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_preds))\n",
    "print(\"RMSE: %f\" % (rmse))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#n_estimators=100, learning_rate = 0.1, max_depth = 5\n",
    "#RMSE: 0.179606\n",
    "\n",
    "#n_estimators=100, learning_rate = 0.1, max_depth = 10\n",
    "#RMSE: 0.188252\n",
    "\n",
    "#n_estimators=500, learning_rate = 0.1, max_depth = 5\n",
    "#RMSE: 0.182817\n",
    "#---------------------------------------------------------------------\n",
    "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "       colsample_bynode=1, colsample_bytree=0.3, gamma=0,\n",
    "       importance_type='gain', learning_rate=0.01, max_delta_step=0,\n",
    "       max_depth=5, min_child_weight=1, missing=None, n_estimators=500,\n",
    "       n_jobs=1, nthread=None, objective='reg:squarederror',\n",
    "       random_state=0, reg_alpha=0, reg_lambda=5, scale_pos_weight=1,\n",
    "       seed=None, silent=None, subsample=1, verbosity=1)\n",
    "RMSE: 0.179199\n",
    "#---------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2Vec is trained over `list_corpus` in order to retrieve words vectors from corpus.\n",
    "\n",
    "In Word2Vec, 3 matrices occupy memory.\n",
    "Then wize is : \n",
    "\n",
    "vocabulary * matrices * NN_layers * 8 bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.listdir('./data'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_w2vec.epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import p5_util\n",
    "file_name = './data/matrix.dump'\n",
    "p5_util.object_dump(matrix, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "pipeline_lr = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf',  TfidfTransformer()),\n",
    "    ('scale', preprocessing.StandardScaler()),\n",
    "    ('lr', LinearRegression()),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lr = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.fit(matrix, list_label_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(matrix == zero_vec)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_corpus_label = [ (gensim.utils.simple_preprocess(X_test[index], deacc=False, min_len=2), y_test[index]) for index in X_test.index[:3000]]\n",
    "\n",
    "list_corpus = [list_corpus_label[i][0] for i in range(0, len(list_corpus_label),1)]\n",
    "list_label  = [list_corpus_label[i][1] for i in range(0, len(list_corpus_label),1)]\n",
    "\n",
    "list_corpus_cleaned,list_label_cleaned = clean_empty_text(list_corpus, list_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_matrix = build_embedding_matrix(model_w2vec, list_corpus_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = lr.predict(test_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2_score(list_label_cleaned, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(list_label_cleaned, y_predict))\n",
    "print(\"RMSE for Linear Regression: %.4f\" % rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using MeanW2VecEmbedding class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.tolist()\n",
    "y_train = y_train.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=50\n",
    "model_filename = './data/model_w2vec_'+str(epochs)+'_epochs'\n",
    "\n",
    "w2vec = get_trained_w2vec(model_filename, X_train, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2vec.wv.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meanW2VecEmbedding =  MeanW2VecEmbedding(w2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_train\n",
    "y = y_train\n",
    "range_index = range(0, len(X),1)\n",
    "\n",
    "list_X_y = [ (gensim.utils.simple_preprocess(X[index], deacc=False, min_len=2), y[index]) \\\n",
    "            for index in range_index]\n",
    "X = [list_X_y[i][0] for i in range(0, len(list_X_y),1)]\n",
    "y = [list_X_y[i][1] for i in range(0, len(list_X_y),1)]\n",
    "list_X, list_y = clean_X_label(X,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2vec.wv.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list_X[0]\n",
    "i=0\n",
    "np.mean( [w2vec.wv[word] for word in list_X[i]  if word in w2vec.wv.vocab], axis=0 )\n",
    "word = list_X[i][1]\n",
    "w2vec.wv[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = build_embedding_matrix(w2vec, list_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_, y_ = meanW2VecEmbedding.transform(X_train, y_train)\n",
    "\n",
    "X_.shape, y_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = meanW2VecEmbedding.transform(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = lr_model.predict(X_test[:3000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test[:3000], y_predict))\n",
    "print(\"RMSE for Linear Regression: %.4f\" % rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "pipeline_lr = Pipeline([\n",
    "    ('meanW2VecEmbedding', MeanW2VecEmbedding(w2vec)),\n",
    "    ('lr', LinearRegression()),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_lr = pipeline_lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#min_count=1\n",
    "#(19454680, 24806990)\n",
    "\n",
    "#min_count=2\n",
    "#(19288764, 24806990)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get dictionary where each word is a key from corpus and each of the value is the word vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_word_vector = dict(zip(model_w2vec.wv.index2word, model_w2vec.wv.vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in dict_word_vector.items() :\n",
    "    print(\"Word2Vec sampling: word= '{}' / Vector length= {}\".format(key,len(value)))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Also, a vector may be retrieved from any word with operation : `word_vector = model_w2vec[word]`"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Skip Gram values : \n",
    "#sg=0 : CBOW is used\n",
    "#sg=1 : skip gram is used\n",
    "model_w2vec = gensim.models.Word2Vec(min_count=1,\n",
    "                     window=3,\n",
    "                     size=300,# Number of NN layers; default is 100\n",
    "                     #sample=6e-5, \n",
    "                     alpha=0.03, \n",
    "                     min_alpha=0.0007, \n",
    "                     negative=20,\n",
    "                     workers=-1,\n",
    "                     #sg=0,\n",
    "                                    )\n",
    "model_w2vec.build_vocab(list_corpus)                 # can be a non-repeatable, 1-pass generator     \n",
    "model_w2vec.train(list_corpus, total_examples=model_w2vec.corpus_count, epochs=model_w2vec.iter)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(model_w2vec.wv.vocab)\n",
    "print(\"Vocabulary size= {}\".format(vocab_size))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "np_matrix = np.array([ [ model_w2vec.wv.get_vector(word) for word in list_corpus[i] if word in model_w2vec.wv.vocab ] for i in range(0,len(list_corpus),1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "sequences = [\n",
    "[1, 2, 3, 4],\n",
    "   [1, 2, 3],\n",
    "     [1]]\n",
    "# pad sequence\n",
    "padded = pad_sequences(sequences, padding='post')\n",
    "print(padded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Building embedding matrix for each text"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dir(model_w2vec.wv.wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_embedding = model_w2vec.wv.get_keras_embedding(train_embeddings=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "from keras.layers import Flatten, Dense, Dropout, Embedding\n",
    "\n",
    "vocab_size = len(model_w2vec.wv.vocab)\n",
    "embedding_dim = model_w2vec.wv.vectors.view().shape[1]\n",
    "\n",
    "max_len = 0\n",
    "for i, j in ser_corpus.items():\n",
    "    max_len = max(len(j),max_len)\n",
    "max_len    \n",
    "\n",
    "model = Sequential()\n",
    "if False :\n",
    "    model.add(Embedding(vocab_size, \n",
    "                        embedding_dim, \n",
    "                        input_length=max_len, \n",
    "                        weights = keras_embedding.get_weights(), \n",
    "                        trainable = False))\n",
    "else :\n",
    "    model.add(keras_embedding)\n",
    "\n",
    "model.add(Conv1D(128, 3, activation='relu'))\n",
    "model.add(MaxPooling1D(3))\n",
    "\n",
    "model.add(Conv1D(128, 3, activation='relu'))\n",
    "model.add(MaxPooling1D(3))\n",
    "\n",
    "model.add(Conv1D(128, 3, activation='relu'))\n",
    "model.add(MaxPooling1D(3))\n",
    "\n",
    "if False :\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(20, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make memory more efficient; this implies no more training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_w2vec.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_w2vec.most_similar(positive=['remember'], topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_w2vec.predict_output_word(['gaf'], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_corpus[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "list_array_cnn_input = [[model_w2vec[word] for word in list_word] for list_word in list_corpus]\n",
    "array_array_cnn_input = np.array(list_array_cnn_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(array_array_cnn_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(array_array_cnn_input))\n",
    "array_array_cnn_input[10][100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(model_w2vec.wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(model_w2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier, LinearRegression\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "pipeline_gbr = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf',  TfidfTransformer()),\n",
    "    ('scale', preprocessing.StandardScaler(with_mean=False)),\n",
    "    ('gbr', GradientBoostingRegressor()),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipeline_gbr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(TfidfTransformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import p6_util\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(p6_util.get_list_tag_stat_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
