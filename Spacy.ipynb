{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Spacy</b>\n",
       "<div id=\"my_id_menu_nb\">run previous cell, wait for 2 seconds</div>\n",
       "<script>\n",
       "function repeat_indent_string(n){\n",
       "    var a = \"\" ;\n",
       "    for ( ; n > 0 ; --n)\n",
       "        a += \"    \";\n",
       "    return a;\n",
       "}\n",
       "// look up into all sections and builds an automated menu //\n",
       "var update_menu_string = function(begin, lfirst, llast, sformat, send, keep_item, begin_format, end_format) {\n",
       "    var anchors = document.getElementsByClassName(\"section\");\n",
       "    if (anchors.length == 0) {\n",
       "        anchors = document.getElementsByClassName(\"text_cell_render rendered_html\");\n",
       "    }\n",
       "    var i,t;\n",
       "    var text_menu = begin;\n",
       "    var text_memo = \"<pre>\\nlength:\" + anchors.length + \"\\n\";\n",
       "    var ind = \"\";\n",
       "    var memo_level = 1;\n",
       "    var href;\n",
       "    var tags = [];\n",
       "    var main_item = 0;\n",
       "    var format_open = 0;\n",
       "    for (i = 0; i <= llast; i++)\n",
       "        tags.push(\"h\" + i);\n",
       "\n",
       "    for (i = 0; i < anchors.length; i++) {\n",
       "        text_memo += \"**\" + anchors[i].id + \"--\\n\";\n",
       "\n",
       "        var child = null;\n",
       "        for(t = 0; t < tags.length; t++) {\n",
       "            var r = anchors[i].getElementsByTagName(tags[t]);\n",
       "            if (r.length > 0) {\n",
       "child = r[0];\n",
       "break;\n",
       "            }\n",
       "        }\n",
       "        if (child == null) {\n",
       "            text_memo += \"null\\n\";\n",
       "            continue;\n",
       "        }\n",
       "        if (anchors[i].hasAttribute(\"id\")) {\n",
       "            // when converted in RST\n",
       "            href = anchors[i].id;\n",
       "            text_memo += \"#1-\" + href;\n",
       "            // passer à child suivant (le chercher)\n",
       "        }\n",
       "        else if (child.hasAttribute(\"id\")) {\n",
       "            // in a notebook\n",
       "            href = child.id;\n",
       "            text_memo += \"#2-\" + href;\n",
       "        }\n",
       "        else {\n",
       "            text_memo += \"#3-\" + \"*\" + \"\\n\";\n",
       "            continue;\n",
       "        }\n",
       "        var title = child.textContent;\n",
       "        var level = parseInt(child.tagName.substring(1,2));\n",
       "\n",
       "        text_memo += \"--\" + level + \"?\" + lfirst + \"--\" + title + \"\\n\";\n",
       "\n",
       "        if ((level < lfirst) || (level > llast)) {\n",
       "            continue ;\n",
       "        }\n",
       "        if (title.endsWith('¶')) {\n",
       "            title = title.substring(0,title.length-1).replace(\"<\", \"&lt;\")\n",
       "         .replace(\">\", \"&gt;\").replace(\"&\", \"&amp;\");\n",
       "        }\n",
       "        if (title.length == 0) {\n",
       "            continue;\n",
       "        }\n",
       "\n",
       "        while (level < memo_level) {\n",
       "            text_menu += end_format + \"</ul>\\n\";\n",
       "            format_open -= 1;\n",
       "            memo_level -= 1;\n",
       "        }\n",
       "        if (level == lfirst) {\n",
       "            main_item += 1;\n",
       "        }\n",
       "        if (keep_item != -1 && main_item != keep_item + 1) {\n",
       "            // alert(main_item + \" - \" + level + \" - \" + keep_item);\n",
       "            continue;\n",
       "        }\n",
       "        while (level > memo_level) {\n",
       "            text_menu += \"<ul>\\n\";\n",
       "            memo_level += 1;\n",
       "        }\n",
       "        text_menu += repeat_indent_string(level-2);\n",
       "        text_menu += begin_format + sformat.replace(\"__HREF__\", href).replace(\"__TITLE__\", title);\n",
       "        format_open += 1;\n",
       "    }\n",
       "    while (1 < memo_level) {\n",
       "        text_menu += end_format + \"</ul>\\n\";\n",
       "        memo_level -= 1;\n",
       "        format_open -= 1;\n",
       "    }\n",
       "    text_menu += send;\n",
       "    //text_menu += \"\\n\" + text_memo;\n",
       "\n",
       "    while (format_open > 0) {\n",
       "        text_menu += end_format;\n",
       "        format_open -= 1;\n",
       "    }\n",
       "    return text_menu;\n",
       "};\n",
       "var update_menu = function() {\n",
       "    var sbegin = \"\";\n",
       "    var sformat = '<a href=\"#__HREF__\">__TITLE__</a>';\n",
       "    var send = \"\";\n",
       "    var begin_format = '<li>';\n",
       "    var end_format = '</li>';\n",
       "    var keep_item = -1;\n",
       "    var text_menu = update_menu_string(sbegin, 1, 4, sformat, send, keep_item,\n",
       "       begin_format, end_format);\n",
       "    var menu = document.getElementById(\"my_id_menu_nb\");\n",
       "    menu.innerHTML=text_menu;\n",
       "};\n",
       "window.setTimeout(update_menu,2000);\n",
       "            </script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from jyquickhelper import add_notebook_menu\n",
    "add_notebook_menu(first_level=1, last_level=4, header=\"Spacy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading prepared data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 bangui bangui 133M août  19 20:44 ./data/DataPreparator_train.dump\r\n",
      "-rw-r--r-- 1 bangui bangui 134M août  19 20:44 ./data/DataPreparator_test.dump\r\n",
      "-rw-r--r-- 1 bangui bangui 137M août  23 00:43 ./data/DataPreparator_train_spacy_20000.dump\r\n",
      "-rw-r--r-- 1 bangui bangui 137M août  23 01:35 ./data/DataPreparator_test_spacy_20000.dump\r\n",
      "-rw-r--r-- 1 bangui bangui 339M août  26 22:57 ./data/DataPreparator_train_spacy_15000.dump\r\n",
      "-rw-r--r-- 1 bangui bangui 339M août  26 23:32 ./data/DataPreparator_test_spacy_15000.dump\r\n",
      "-rw-r--r-- 1 bangui bangui 1,5G août  30 17:08 ./data/DataPreparator_valid_v2_spacy_5000.dump\r\n",
      "-rw-r--r-- 1 bangui bangui 335M sept.  1 22:28 ./data/DataPreparator_valid_v2_spacy_15000.dump\r\n",
      "-rw-r--r-- 1 bangui bangui 615M sept.  2 18:21 ./data/DataPreparator_train_v2_spacy_15000.dill\r\n",
      "-rw-r--r-- 1 bangui bangui 615M sept.  2 22:14 ./data/DataPreparator_valid_v2_spacy_15000.dill\r\n",
      "-rw-r--r-- 1 bangui bangui 2,8G sept.  3 14:16 ./data/DataPreparator_valid_v2_spacy_notfidf_15000.dill\r\n",
      "-rw-r--r-- 1 bangui bangui 2,8G sept.  3 15:34 ./data/DataPreparator_train_v2_spacy_notfidf_15000.dill\r\n"
     ]
    }
   ],
   "source": [
    "!ls -alrth ./data/DataPreparator*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sample=15000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p5_util.object_load : fileName= ./data/DataPreparator_train_spacy_15000.dump\n"
     ]
    }
   ],
   "source": [
    "import p5_util\n",
    "filename = './data/DataPreparator_train_spacy_'+str(n_sample)+'.dump'\n",
    "dataPreparator_train = p5_util.object_load(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "docs        thank you for stating the fact that the israel...\n",
       "tokens      [thank, state, fact, israeli, wall, protect, i...\n",
       "counting                                                   14\n",
       "vector      [-0.07805210709571839, -0.022358824014663697, ...\n",
       "target                                                    0.4\n",
       "Name: 12926, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataPreparator_train.df_data.loc[12926]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p5_util.object_load : fileName= ./data/DataPreparator_train_v2_spacy_15000.dill\n"
     ]
    }
   ],
   "source": [
    "import p5_util\n",
    "filename = './data/DataPreparator_train_v2_spacy_'+str(n_sample)+'.dill'\n",
    "dataPreparator_v2_train = p5_util.object_load(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80, 39)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataPreparator_v2_train.df_data.loc[12926]['matrix_padded_truncated'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thank you for stating the fact that the israeli wall is there to protect the inhabitants of israel , some of whom are israeli arabs , from palestinian and other arab terrorist attacks .\n"
     ]
    }
   ],
   "source": [
    "dataPreparator_train.df_data.columns\n",
    "original_text = dataPreparator_train.df_data['docs'].loc[12926]\n",
    "print(original_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/bangui/.local/lib/python3.6/site-packages/matplotlib/__init__.py:886: MatplotlibDeprecationWarning: \n",
      "examples.directory is deprecated; in the future, examples will be found relative to the 'datapath' directory.\n",
      "  \"found relative to the 'datapath' directory.\".format(key))\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0903 16:11:24.317681 140008605685568 lazy_loader.py:50] \n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "W0903 16:11:24.318279 140008605685568 deprecation_wrapper.py:119] From /home/bangui/anaconda3/envs/python36/lib/python3.6/site-packages/adanet/tf_compat/__init__.py:96: The name tf.losses.Reduction is deprecated. Please use tf.compat.v1.losses.Reduction instead.\n",
      "\n",
      "W0903 16:11:24.328448 140008605685568 deprecation_wrapper.py:119] From /home/bangui/Dropbox/Perso/Formation/openclassrooms/OC_Datascientist/Kaggle/p8_util_config.py:137: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p5_util.object_load : fileName= ./data/DataPreparator_train_v2_spacy_notfidf_15000.dill\n"
     ]
    }
   ],
   "source": [
    "import dill\n",
    "import p5_util\n",
    "import DataPreparator_v2\n",
    "\n",
    "filename = './data/DataPreparator_train_v2_spacy_notfidf_'+str(n_sample)+'.dill'\n",
    "dataPreparator_v2_train = p5_util.object_load(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['tokens', 'counting', 'target', 'matrix_padded'], dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataPreparator_v2_train.df_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_sample = dataPreparator_v2_train.df_data.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import p9_util_spacy\n",
    "ser_vector = p9_util_spacy.spacy_vectorizer(None, \\\n",
    "                                            df_data_sample['tokens'],\\\n",
    "                                            dict_token_tfidf = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11246    [0.1863966086942431, 0.442137657533749, -1.125...\n",
       "9236     [-0.4308615626521512, -0.03277142757886193, 0....\n",
       "8593     [-0.16642390889483655, -0.0811255122910464, 0....\n",
       "Name: tokens, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ser_vector.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens           [maybe, unknown, seriously, story, wrong, unkn...\n",
      "counting                                                         6\n",
      "target                                                           0\n",
      "matrix_padded    [[-0.7441577315330505, 1.815933108329773, -1.9...\n",
      "Name: 11246, dtype: object\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(6,)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_data_sample.loc[11246])\n",
    "word_vector = ser_vector.loc[11246]\n",
    "word_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import p9_util_spacy\n",
    "p9_util_spacy.SPACY_LANGUAGE_MODEL.vocab[''].vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "man = spacy_nlp.vocab['man'].vector\n",
    "woman = spacy_nlp.vocab['woman'].vector\n",
    "queen = spacy_nlp.vocab['queen'].vector\n",
    "king = spacy_nlp.vocab['king'].vector\n",
    " \n",
    "# We now need to find the closest vector in the vocabulary to the result of \"man\" - \"woman\" + \"queen\"\n",
    "word_vector = man - woman + queen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import spatial\n",
    "\n",
    "def list_word_silimar(word_vector):\n",
    "\n",
    "    cosine_similarity = lambda x, y: 1 - spatial.distance.cosine(x, y)\n",
    "\n",
    "    list_similarity = list()\n",
    "    for word in spacy_nlp.vocab:\n",
    "        # Ignore words without vectors\n",
    "        if not word.has_vector:\n",
    "            continue\n",
    "\n",
    "        similarity = cosine_similarity(word_vector, word.vector)\n",
    "        list_similarity.append((word, similarity))\n",
    " \n",
    "\n",
    "    list_similarity = sorted(list_similarity, key=lambda item: -item[1])\n",
    "    return list_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['israelian', 'Isreali', 'KIBBUTZ', 'ISREALI', 'Israelian', 'sabra', 'ISRAELI', 'isreali', 'kibbutz', 'israeli']\n"
     ]
    }
   ],
   "source": [
    "print([w[0].text for w in computed_similarities[:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>counting</th>\n",
       "      <th>target</th>\n",
       "      <th>matrix_padded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12148</th>\n",
       "      <td>[doubt, airbus, pay, actual, cash, company, le...</td>\n",
       "      <td>25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[[-0.9378427863121033, 0.3909517228603363, -1....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12800</th>\n",
       "      <td>[news, sure, country, indicative, type, thing,...</td>\n",
       "      <td>11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[[-1.8933311700820923, 3.626978635787964, 0.18...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12926</th>\n",
       "      <td>[thank, state, fact, israeli, wall, protect, i...</td>\n",
       "      <td>14</td>\n",
       "      <td>0.4</td>\n",
       "      <td>[[-1.5131877660751343, 1.051204800605774, -2.8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9236</th>\n",
       "      <td>[irt, isle, shipping, need, cooperative, secur...</td>\n",
       "      <td>18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[[1.6886780261993408, -0.6662434339523315, -1....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11246</th>\n",
       "      <td>[maybe, unknown, seriously, story, wrong, unkn...</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[[-0.7441577315330505, 1.815933108329773, -1.9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7925</th>\n",
       "      <td>[poor, albertan, hit, jan, 1st, hope, tax, met...</td>\n",
       "      <td>13</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[[-6.224758148193359, 1.3011060953140259, -2.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>[red, meat, guy, house, ok]</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[[-1.5555469989776611, -1.4506696462631226, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9170</th>\n",
       "      <td>[know, certainly, run, usccb, program, include...</td>\n",
       "      <td>51</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[[-0.08945110440254211, 1.4008657932281494, -1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>573</th>\n",
       "      <td>[condemn, fr, let, think, sexual, activity, no...</td>\n",
       "      <td>9</td>\n",
       "      <td>0.4</td>\n",
       "      <td>[[-1.4674835205078125, -1.8247777223587036, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8593</th>\n",
       "      <td>[question, irrelevant, tuition, increase, main...</td>\n",
       "      <td>31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[[-1.059801697731018, 1.064276933670044, -1.13...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tokens  counting  target  \\\n",
       "12148  [doubt, airbus, pay, actual, cash, company, le...        25     0.0   \n",
       "12800  [news, sure, country, indicative, type, thing,...        11     0.0   \n",
       "12926  [thank, state, fact, israeli, wall, protect, i...        14     0.4   \n",
       "9236   [irt, isle, shipping, need, cooperative, secur...        18     0.0   \n",
       "11246  [maybe, unknown, seriously, story, wrong, unkn...         6     0.0   \n",
       "7925   [poor, albertan, hit, jan, 1st, hope, tax, met...        13     0.0   \n",
       "153                          [red, meat, guy, house, ok]         5     0.0   \n",
       "9170   [know, certainly, run, usccb, program, include...        51     0.0   \n",
       "573    [condemn, fr, let, think, sexual, activity, no...         9     0.4   \n",
       "8593   [question, irrelevant, tuition, increase, main...        31     0.0   \n",
       "\n",
       "                                           matrix_padded  \n",
       "12148  [[-0.9378427863121033, 0.3909517228603363, -1....  \n",
       "12800  [[-1.8933311700820923, 3.626978635787964, 0.18...  \n",
       "12926  [[-1.5131877660751343, 1.051204800605774, -2.8...  \n",
       "9236   [[1.6886780261993408, -0.6662434339523315, -1....  \n",
       "11246  [[-0.7441577315330505, 1.815933108329773, -1.9...  \n",
       "7925   [[-6.224758148193359, 1.3011060953140259, -2.0...  \n",
       "153    [[-1.5555469989776611, -1.4506696462631226, -0...  \n",
       "9170   [[-0.08945110440254211, 1.4008657932281494, -1...  \n",
       "573    [[-1.4674835205078125, -1.8247777223587036, -0...  \n",
       "8593   [[-1.059801697731018, 1.064276933670044, -1.13...  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thank state fact israeli wall protect inhabitant israel israeli arab palestinian arab terrorist attack "
     ]
    },
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "word_vector = np.zeros(300)\n",
    "for token in df_data_sample['tokens'].loc[12926] :\n",
    "    print(token, end=' ')\n",
    "    word_vector += spacy_nlp.vocab[token].vector\n",
    "\n",
    "len(df_data_sample['tokens'].loc[12926])\n",
    "#word_vector /= len(df_data_sample['tokens'].loc[12926])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_word = list_word_silimar(word_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['israelian', 'Isreali', 'KIBBUTZ', 'ISREALI', 'Israelian', 'sabra', 'ISRAELI', 'isreali', 'kibbutz', 'israeli']\n"
     ]
    }
   ],
   "source": [
    "print([w[0].text for w in list_word[:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a list of textes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "spacy_nlp = spacy.load('en_core_web_md')\n",
    "doc_string = u\"this is Apple is looking at buying U.K. startup for $1 billion dollars in cash!\"\n",
    "doc1 = \"The 20 big grey dogs all named John don't vsbsbs ate all of the chocolate, but fortunately he wasn't sick!\"\n",
    "doc2 = \"Hello world! Get in touch on http://fr.ayoo.com or mail@me.fr\"\n",
    "list_doc = [doc_string, doc1, doc2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>this is Apple is looking at buying U.K. startu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The 20 big grey dogs all named John don't vsbs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hello world! Get in touch on http://fr.ayoo.co...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                docs\n",
       "0  this is Apple is looking at buying U.K. startu...\n",
       "1  The 20 big grey dogs all named John don't vsbs...\n",
       "2  Hello world! Get in touch on http://fr.ayoo.co..."
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import p9_util_spacy\n",
    "\n",
    "df = pd.DataFrame(list_doc)\n",
    "df.rename(columns={0:'docs'}, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docs</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>this is Apple is looking at buying U.K. startu...</td>\n",
       "      <td>[look, buy, startup, dollar, cash]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The 20 big grey dogs all named John don't vsbs...</td>\n",
       "      <td>[big, grey, dog, name, vsbsbs, ate, chocolate,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hello world! Get in touch on http://fr.ayoo.co...</td>\n",
       "      <td>[hello, world, touch]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                docs  \\\n",
       "0  this is Apple is looking at buying U.K. startu...   \n",
       "1  The 20 big grey dogs all named John don't vsbs...   \n",
       "2  Hello world! Get in touch on http://fr.ayoo.co...   \n",
       "\n",
       "                                              tokens  \n",
       "0                 [look, buy, startup, dollar, cash]  \n",
       "1  [big, grey, dog, name, vsbsbs, ate, chocolate,...  \n",
       "2                              [hello, world, touch]  "
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tokens'] = df.docs.apply(lambda document: [token.lemma_ for token in spacy_nlp(document) if p9_util_spacy.spacy_is_token_valid(token)]  )\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docs</th>\n",
       "      <th>tokens</th>\n",
       "      <th>tokens2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>this is Apple is looking at buying U.K. startu...</td>\n",
       "      <td>[look, buy, startup, dollar, cash]</td>\n",
       "      <td>[look, buy, startup, dollar, cash]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The 20 big grey dogs all named John don't vsbs...</td>\n",
       "      <td>[big, grey, dog, name, vsbsbs, ate, chocolate,...</td>\n",
       "      <td>[big, grey, dog, vsbsbs, eat, chocolate, fortu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hello world! Get in touch on http://fr.ayoo.co...</td>\n",
       "      <td>[hello, world, touch]</td>\n",
       "      <td>[hello, world, touch]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                docs  \\\n",
       "0  this is Apple is looking at buying U.K. startu...   \n",
       "1  The 20 big grey dogs all named John don't vsbs...   \n",
       "2  Hello world! Get in touch on http://fr.ayoo.co...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0                 [look, buy, startup, dollar, cash]   \n",
       "1  [big, grey, dog, name, vsbsbs, ate, chocolate,...   \n",
       "2                              [hello, world, touch]   \n",
       "\n",
       "                                             tokens2  \n",
       "0                 [look, buy, startup, dollar, cash]  \n",
       "1  [big, grey, dog, vsbsbs, eat, chocolate, fortu...  \n",
       "2                              [hello, world, touch]  "
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tokens2'] = df.tokens.apply(lambda list_token: [token.lemma_ for token in spacy_nlp(\" \".join(list_token)) if p9_util_spacy.spacy_is_token_valid(token)]  )\n",
    "df"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from numpy import dot \n",
    "from numpy.linalg import norm \n",
    "from spacy.lang.en import English\n",
    "parser = English()\n",
    "\n",
    "#Generate word vector of the word - apple  \n",
    "#apple = parser.vocab[u'apple']\n",
    "apple = spacy_nlp('apple')\n",
    "#Cosine similarity function \n",
    "cosine = lambda v1, v2: dot(v1, v2) / (norm(v1) * norm(v2))\n",
    "set_other = {w for w in spacy_nlp.vocab if w.has_vector and w.orth_.islower() and w.lower_ != unicode(\"apple\")}\n",
    "print(type(set_other))\n",
    "others = list(set_other)\n",
    "# sort by similarity score\n",
    "others.sort(key=lambda w: cosine(w.vector, apple.vector)) \n",
    "others.reverse()\n",
    "print(others)\n",
    "\n",
    "\n",
    "print (\"top most similar words to apple:\" )\n",
    "for word in others[:10]:\n",
    "    print (word.orth_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scapy entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scapy\n",
    "\n",
    "#nlp.tokenizer = spacy.tokenizer.Tokenizer(nlp.vocab)\n",
    "nlp_doc = nlp(doc_string)\n",
    "for ent in nlp_doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scapy sentences processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_doc = nlp(doc_string)\n",
    "print(\" \")\n",
    "print(\"Sentence issued from Scapy NLP:\")\n",
    "for sent in nlp_doc.sents :\n",
    "    print(sent.text)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scapy tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello-world.']\n",
      "['jalkalakz', 'is', 'great!']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.tokenizer = spacy.tokenizer.Tokenizer(nlp.vocab)\n",
    "scapy_doc = nlp(u\"hello-world.\")\n",
    "print([scapy_token.text for scapy_token in scapy_doc])\n",
    "\n",
    "scapy_doc = nlp(u\"jalkalakz is great!\")\n",
    "print([scapy_token.text for scapy_token in scapy_doc])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizer issued from p9_util_spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p5_util.object_load : fileName= ./data/df_dataprep_141651.dill\n"
     ]
    }
   ],
   "source": [
    "import p5_util\n",
    "filename = './data/df_dataprep_141651.dill'\n",
    "df_dataprep = p5_util.object_load(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docs</th>\n",
       "      <th>counting</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>670424</th>\n",
       "      <td>precisely! about an hour ago i posted a joke c...</td>\n",
       "      <td>29</td>\n",
       "      <td>[precisely, hour, ago, post, joke, comment, re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162615</th>\n",
       "      <td>thanks for supporting and validating my point....</td>\n",
       "      <td>18</td>\n",
       "      <td>[thank, support, validate, point, nyt, article...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430440</th>\n",
       "      <td>tv ratings for this event will be weak because...</td>\n",
       "      <td>26</td>\n",
       "      <td>[tv, rating, event, weak, nfl, raptor, highest...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     docs  counting  \\\n",
       "670424  precisely! about an hour ago i posted a joke c...        29   \n",
       "162615  thanks for supporting and validating my point....        18   \n",
       "430440  tv ratings for this event will be weak because...        26   \n",
       "\n",
       "                                                   tokens  \n",
       "670424  [precisely, hour, ago, post, joke, comment, re...  \n",
       "162615  [thank, support, validate, point, nyt, article...  \n",
       "430440  [tv, rating, event, weak, nfl, raptor, highest...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dataprep.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1525770</th>\n",
       "      <td>us policy on north korea has not changed since...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478487</th>\n",
       "      <td>when you have a leader who is so detached from...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1530557</th>\n",
       "      <td>when we run out of writing about christ, then ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      docs\n",
       "1525770  us policy on north korea has not changed since...\n",
       "478487   when you have a leader who is so detached from...\n",
       "1530557  when we run out of writing about christ, then ..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del(df_dataprep['counting'])\n",
    "del(df_dataprep['tokens'])\n",
    "df_dataprep.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello-world.']\n"
     ]
    }
   ],
   "source": [
    "from spacy.tokenizer import Tokenizer\n",
    "\n",
    "import spacy\n",
    "import p9_util_spacy\n",
    "\n",
    "spacy_nlp = p9_util_spacy.SPACY_LANGUAGE_MODEL\n",
    "spacy_nlp.tokenizer = spacy.tokenizer.Tokenizer(spacy_nlp.vocab)\n",
    "\n",
    "spacy_doc = spacy_nlp(u\"hello-world.\")\n",
    "print([scapy_token.text for scapy_token in spacy_doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a sentence what's happen this a realy good good\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['this',\n",
       " 'is',\n",
       " 'a',\n",
       " 'sentence',\n",
       " \"what's\",\n",
       " 'happen',\n",
       " 'this',\n",
       " 'a',\n",
       " 'realy',\n",
       " 'good',\n",
       " 'good']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_doc = spacy_nlp.tokenizer(\"This is a sentence what's happen this a realy good good\")\n",
    "print(spacy_doc)\n",
    "list_token = [token.text.lower() for token in spacy_doc]\n",
    "list_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a sentence what's happen this a realy good good\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"jalkalakz's\",\n",
       " 'home',\n",
       " 'is',\n",
       " 'great',\n",
       " 'and',\n",
       " 'this',\n",
       " 'is',\n",
       " 'a',\n",
       " 'good',\n",
       " 'day',\n",
       " '!']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_doc2 = spacy_nlp(u\"jalkalakz's home is great and this is a good day !\")\n",
    "print(spacy_doc2)\n",
    "list_token_2 = [token.text.lower() for token in spacy_doc2]\n",
    "list_token_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, {'a', 'good', 'happen', 'is', 'realy', 'sentence', 'this', \"what's\"})"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "set_counter = set()\n",
    "set_counter = set_counter.union(Counter(list_token))\n",
    "len(Counter(list_token)),set_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'!',\n",
       " 'a',\n",
       " 'and',\n",
       " 'day',\n",
       " 'good',\n",
       " 'great',\n",
       " 'happen',\n",
       " 'home',\n",
       " 'is',\n",
       " \"jalkalakz's\",\n",
       " 'realy',\n",
       " 'sentence',\n",
       " 'this',\n",
       " \"what's\"}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_counter = set_counter.union(Counter(list_token_2))\n",
    "set_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not token.is_stop and not token.is_punct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_word_index = { word:index for (word, index) in zip(set_counter, range(len(set_counter))) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'great': 0,\n",
       " 'this': 1,\n",
       " '!': 2,\n",
       " 'good': 3,\n",
       " 'happen': 4,\n",
       " \"jalkalakz's\": 5,\n",
       " 'day': 6,\n",
       " 'is': 7,\n",
       " 'a': 8,\n",
       " 'and': 9,\n",
       " 'home': 10,\n",
       " 'sentence': 11,\n",
       " \"what's\": 12,\n",
       " 'realy': 13}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p5_util.object_load : fileName= ./data/benchmark/df_sample_benchmark_valid_FULL.dill\n"
     ]
    }
   ],
   "source": [
    "import p5_util\n",
    "import p9_util_benchmark\n",
    "\n",
    "filename_valid_dataset = p9_util_benchmark.build_filename_benchmark()\n",
    "filename_valid_dataset\n",
    "df_validate = p5_util.object_load(filename_valid_dataset,is_verbose=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import p9_util_spacy\n",
    "\n",
    "set_counter = set()\n",
    "it = 0\n",
    "for text in df_validate.comment_text :\n",
    "    list_token = [token.text.lower() for token in spacy_nlp(text) if p9_util_spacy.spacy_is_token_valid(token, \\\n",
    "                                  min_token_length=2,\\\n",
    "                                  max_token_length=200)]\n",
    "    set_counter = set_counter.union(Counter(list_token))\n",
    "    it +=1\n",
    "    if 0 == 10000%it :\n",
    "        print(\"Processed texts = {}/{}\".format(it,len(df_validate)), end='\\r')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ado',\n",
       " 'airlines',\n",
       " 'airport',\n",
       " 'alaska',\n",
       " 'appeared',\n",
       " 'apples',\n",
       " 'article',\n",
       " 'big',\n",
       " 'board',\n",
       " 'break.',\n",
       " 'canada',\n",
       " 'canada.',\n",
       " 'canucks,',\n",
       " 'center?',\n",
       " 'charge',\n",
       " 'cheering',\n",
       " 'combo.',\n",
       " 'commons.',\n",
       " 'condemn',\n",
       " 'constantly',\n",
       " 'criminalized',\n",
       " 'dec',\n",
       " 'decision',\n",
       " 'dumping',\n",
       " 'eco',\n",
       " 'especially',\n",
       " 'fire.',\n",
       " 'fun',\n",
       " 'gets',\n",
       " 'good',\n",
       " 'guy',\n",
       " 'hair',\n",
       " 'head',\n",
       " 'highlights',\n",
       " 'hire',\n",
       " 'hire,',\n",
       " 'hockey.',\n",
       " 'house',\n",
       " 'huge',\n",
       " 'incident',\n",
       " 'indoor',\n",
       " 'involved',\n",
       " 'is,',\n",
       " 'is...?',\n",
       " \"isn't\",\n",
       " 'job',\n",
       " 'leftist',\n",
       " 'liberal',\n",
       " 'like',\n",
       " 'makes',\n",
       " 'men',\n",
       " 'misappropriation',\n",
       " 'money?',\n",
       " 'months',\n",
       " 'motion',\n",
       " 'muni',\n",
       " 'near',\n",
       " 'needs',\n",
       " 'not',\n",
       " 'nothing!',\n",
       " 'on.',\n",
       " 'opinion',\n",
       " 'outdoor',\n",
       " 'parrot?',\n",
       " 'passed',\n",
       " 'pay',\n",
       " 'payer',\n",
       " 'piece?',\n",
       " 'politics',\n",
       " 'press.',\n",
       " 'puff',\n",
       " 'question',\n",
       " 'realized',\n",
       " 'school',\n",
       " 'seat?',\n",
       " 'shooting,',\n",
       " \"shouldn't\",\n",
       " 'shut',\n",
       " 'sickness.',\n",
       " 'skiing',\n",
       " 'slippery',\n",
       " 'slope',\n",
       " 'snow',\n",
       " 'so-much',\n",
       " 'solution',\n",
       " 'spin',\n",
       " 'sports',\n",
       " 'state',\n",
       " 'sulley',\n",
       " 'sum',\n",
       " 'superintendents',\n",
       " 'system',\n",
       " 'tards',\n",
       " 'tax',\n",
       " 'them.',\n",
       " 'them?',\n",
       " 'toronto',\n",
       " 'track',\n",
       " 'turn',\n",
       " 'unanimously',\n",
       " 'university',\n",
       " 'use',\n",
       " 'vilified',\n",
       " 'wants',\n",
       " 'wasting',\n",
       " 'watch.',\n",
       " 'website',\n",
       " 'weeks',\n",
       " 'white'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "531185    atf ....i did some past conference checking, t...\n",
       "Name: comment_text, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "len(Counter(list_token))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Series Tokenization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser_token = df_dataprep.docs.apply(lambda doc :[scapy_token.text for scapy_token in spacy_nlp(doc)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "527155     [bill, clinton, was, a, transparent, sexist, p...\n",
       "1171324    [bloody, right, it's, sick, and, the, blame, i...\n",
       "884099     [it's, nice, to, be, compassionate, toward, 70...\n",
       "Name: docs, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ser_token.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bill\n",
      "clinton\n",
      "be\n",
      "a\n",
      "transparent\n",
      "sexist\n",
      "pig.\n",
      "-PRON-\n",
      "can't\n",
      "argue\n",
      "with\n",
      "that.\n"
     ]
    }
   ],
   "source": [
    "list_token = ser_token.loc[527155]\n",
    "for token in list_token:\n",
    "    for spacy_token in spacy_nlp(token):\n",
    "        print(spacy_token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tokens lemmatization and filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_index_token = dict()\n",
    "counter = 0\n",
    "total = ser_token.shape[0]\n",
    "\n",
    "min_token_len = 2\n",
    "max_token_len = 50\n",
    "\n",
    "for index, list_token in ser_token.items() :\n",
    "    list_token_filtered = list()\n",
    "    for token in list_token :\n",
    "        list_token_filtered += [spacy_token.lemma_ for spacy_token in spacy_nlp(token) if (\\\n",
    "                                      p9_util_spacy.spacy_is_token_valid(spacy_token, \\\n",
    "                                      min_token_length=min_token_len,\\\n",
    "                                      max_token_length=max_token_len) )]\n",
    "    counter += 1\n",
    "    if 0 == counter%1000 :\n",
    "        print(\"Processed rows= {}/{}\".format(counter, total), end='\\r')\n",
    "    dict_index_token[index] = list_token_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entity recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking labels from entities TAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " No stop words in labels replacing entities\n"
     ]
    }
   ],
   "source": [
    "import p9_util_spacy\n",
    "lang = 'en'\n",
    "DICT_SCAPY_LANG_ENTITY_LABEL = p9_util_spacy.DDICT_SPACY_LANG_ENTITY_LABEL[lang]\n",
    "is_stop_word = False\n",
    "for entity_label in DICT_SCAPY_LANG_ENTITY_LABEL.values() :\n",
    "    scapy_doc = p9_util_spacy.SPACY_NLP_SM(entity_label)\n",
    "    for scapy_token in scapy_doc :\n",
    "        if scapy_token.is_stop :\n",
    "            print(\"***WARN : Label= {} is a stop word!\".format(entity_label))\n",
    "            is_stop_word = True\n",
    "if not is_stop_word :\n",
    "    print(\"\\n No stop words in labels replacing entities\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking labels in vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " No labels replacing entities out of vocabulary!\n"
     ]
    }
   ],
   "source": [
    "import p9_util_spacy\n",
    "lang = 'en'\n",
    "DICT_SCAPY_LANG_ENTITY_LABEL = p9_util_spacy.DDICT_SPACY_LANG_ENTITY_LABEL[lang]\n",
    "is_oov = False\n",
    "for entity_label in DICT_SCAPY_LANG_ENTITY_LABEL.values() :\n",
    "    scapy_doc = p9_util_spacy.SPACY_NLP_MD(entity_label)\n",
    "    for scapy_token in scapy_doc :\n",
    "        if scapy_token.is_oov :\n",
    "            print(\"***WARN : Label= {} is out of vocabuary\".format(entity_label))\n",
    "            is_oov = True\n",
    "if not is_oov :\n",
    "    print(\"\\n No labels replacing entities out of vocabulary!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking labels vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " All labels replacing entities are vectorized!\n"
     ]
    }
   ],
   "source": [
    "import p9_util_spacy\n",
    "lang = 'en'\n",
    "DICT_SCAPY_LANG_ENTITY_LABEL = p9_util_spacy.DDICT_SPACY_LANG_ENTITY_LABEL[lang]\n",
    "has_vector = True\n",
    "for entity_label in DICT_SCAPY_LANG_ENTITY_LABEL.values() :\n",
    "    scapy_doc = p9_util_spacy.SPACY_NLP_MD(entity_label)\n",
    "    for scapy_token in scapy_doc :\n",
    "        if not scapy_token.has_vector :\n",
    "            print(\"***WARN : Label= {} has no vector!\".format(entity_label))\n",
    "            has_vector = False\n",
    "if has_vector :\n",
    "    print(\"\\n All labels replacing entities are vectorized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "has_vector & (not is_oov) & (not is_stop_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All labels from entities checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Is labels-Spacy entities dictionary valid : True\n"
     ]
    }
   ],
   "source": [
    "import p9_util_spacy\n",
    "print(\"\\nIs labels-Spacy entities dictionary valid : {}\".format(p9_util_spacy.spacy_entity_label_is_valid(is_verbose=False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replacing entities with entities TAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PERSON': 'people',\n",
       " 'NORP': 'religious',\n",
       " 'FAC': 'building',\n",
       " 'ORG': 'agency',\n",
       " 'GPE': 'country',\n",
       " 'LOC': 'location',\n",
       " 'PRODUCT': 'product',\n",
       " 'EVENT': 'event',\n",
       " 'WORK_OF_ART': 'title',\n",
       " 'LAW': 'law',\n",
       " 'LANGUAGE': 'language',\n",
       " 'DATE': 'period',\n",
       " 'TIME': 'time',\n",
       " 'PERCENT': 'percent',\n",
       " 'MONEY': 'money',\n",
       " 'QUANTITY': 'measurement',\n",
       " 'ORDINAL': 'ordinal',\n",
       " 'CARDINAL': 'numeral'}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DICT_SPACY_ENTITY_LABEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this\n",
      "is\n",
      "Apple\n",
      "is\n",
      "looking\n",
      "at\n",
      "buying\n",
      "U.K.\n",
      "startup\n",
      "for\n",
      "$\n",
      "1\n",
      "billion\n",
      "dollars\n",
      "in\n",
      "cash\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "import p9_util_spacy\n",
    "\n",
    "document = doc_string\n",
    "spacy_doc = p9_util_spacy.SPACY_NLP_SM(document)\n",
    "\n",
    "if 0 == len(spacy_doc.ents) :\n",
    "    new_doc = document\n",
    "else :\n",
    "    lang = spacy_doc.lang_\n",
    "\n",
    "    if lang in p9_util_spacy.DDICT_SPACY_LANG_ENTITY_LABEL.keys() : \n",
    "        DICT_SPACY_ENTITY_LABEL = p9_util_spacy.DDICT_SPACY_LANG_ENTITY_LABEL['en']\n",
    "    else :\n",
    "        print(\"\\n*** ERROR : language not supported : {}\".format(lang))\n",
    "\n",
    "    list_ents= [spacy_doc.ents[i] for i in range(len(spacy_doc.ents))]\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    new_doc=''\n",
    "    for token in spacy_doc:\n",
    "        if True:#not token.is_stop : \n",
    "            print(token.text)\n",
    "            if token.text in [ent.text for ent in list_ents] :\n",
    "                ent = [ent for ent in list_ents if token.text==ent.text][0]\n",
    "                \n",
    "                label = DICT_SPACY_ENTITY_LABEL[ent.label_]\n",
    "                new_doc += label.lower()+' '\n",
    "            else :\n",
    "                #new_string += token.lemma_+' '\n",
    "                new_doc += token.text+' '\n",
    "        else :\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is agency is looking at buying country startup for $ 1 billion dollars in cash ! '"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is Apple is looking at buying U.K. startup for $1 billion dollars in cash! this is in foxx\n",
      "\n",
      "this is agency is looking at buying country startup for money in cash! this is in foxx\n"
     ]
    }
   ],
   "source": [
    "import p9_util_spacy\n",
    "doc_string = u\"this is Apple is looking at buying U.K. startup for $1 billion dollars in cash! this is in foxx\"\n",
    "print(doc_string)\n",
    "print()\n",
    "new_doc = p9_util_spacy.spacy_entity_replace(doc_string)\n",
    "print(new_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import p9_util_spacy\n",
    "spacy_nlp = p9_util_spacy.SPACY_NLP_MD\n",
    "spacy_doc = spacy_nlp(' ')\n",
    "for spacy_token in spacy_doc:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_',\n",
       " '__bytes__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__pyx_vtable__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__unicode__',\n",
       " 'ancestors',\n",
       " 'check_flag',\n",
       " 'children',\n",
       " 'cluster',\n",
       " 'conjuncts',\n",
       " 'dep',\n",
       " 'dep_',\n",
       " 'doc',\n",
       " 'ent_id',\n",
       " 'ent_id_',\n",
       " 'ent_iob',\n",
       " 'ent_iob_',\n",
       " 'ent_kb_id',\n",
       " 'ent_kb_id_',\n",
       " 'ent_type',\n",
       " 'ent_type_',\n",
       " 'get_extension',\n",
       " 'has_extension',\n",
       " 'has_vector',\n",
       " 'head',\n",
       " 'i',\n",
       " 'idx',\n",
       " 'is_alpha',\n",
       " 'is_ancestor',\n",
       " 'is_ascii',\n",
       " 'is_bracket',\n",
       " 'is_currency',\n",
       " 'is_digit',\n",
       " 'is_left_punct',\n",
       " 'is_lower',\n",
       " 'is_oov',\n",
       " 'is_punct',\n",
       " 'is_quote',\n",
       " 'is_right_punct',\n",
       " 'is_sent_start',\n",
       " 'is_space',\n",
       " 'is_stop',\n",
       " 'is_title',\n",
       " 'is_upper',\n",
       " 'lang',\n",
       " 'lang_',\n",
       " 'left_edge',\n",
       " 'lefts',\n",
       " 'lemma',\n",
       " 'lemma_',\n",
       " 'lex_id',\n",
       " 'like_email',\n",
       " 'like_num',\n",
       " 'like_url',\n",
       " 'lower',\n",
       " 'lower_',\n",
       " 'n_lefts',\n",
       " 'n_rights',\n",
       " 'nbor',\n",
       " 'norm',\n",
       " 'norm_',\n",
       " 'orth',\n",
       " 'orth_',\n",
       " 'pos',\n",
       " 'pos_',\n",
       " 'prefix',\n",
       " 'prefix_',\n",
       " 'prob',\n",
       " 'rank',\n",
       " 'remove_extension',\n",
       " 'right_edge',\n",
       " 'rights',\n",
       " 'sent',\n",
       " 'sent_start',\n",
       " 'sentiment',\n",
       " 'set_extension',\n",
       " 'shape',\n",
       " 'shape_',\n",
       " 'similarity',\n",
       " 'string',\n",
       " 'subtree',\n",
       " 'suffix',\n",
       " 'suffix_',\n",
       " 'tag',\n",
       " 'tag_',\n",
       " 'tensor',\n",
       " 'text',\n",
       " 'text_with_ws',\n",
       " 'vector',\n",
       " 'vector_norm',\n",
       " 'vocab',\n",
       " 'whitespace_']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(spacy_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New doc :\n",
      "agency is looking at buying country startup for money in cash!\n"
     ]
    }
   ],
   "source": [
    "print(\"New doc :\")\n",
    "new_doc = p9_util_spacy.spacy_entity_replace(new_doc)\n",
    "print(new_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 20 big grey dogs all named John don't vsbsbs ate all of the chocolate, but fortunately he wasn't sick!\n",
      "\n",
      "numeral big grey dogs all named people don't vsbsbs ate all of the chocolate, but fortunately he wasn't sick!\n"
     ]
    }
   ],
   "source": [
    "import p9_util_spacy\n",
    "\n",
    "doc1 = \"The 20 big grey dogs all named John don't vsbsbs ate all of the chocolate, but fortunately he wasn't sick!\"\n",
    "doc2 = \"Hello world!\"\n",
    "print(doc1)\n",
    "\n",
    "new_doc = p9_util_spacy.spacy_entity_replace(doc1)\n",
    "print()\n",
    "print(new_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scapy tokens filtering : stop words and punctuation\n",
    "\n",
    "    Filter stop-words and punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_token = [token for token in spacy_nlp(new_doc) if not token.is_stop and not token.is_punct]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scapy vectorization exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "numeral big grey dogs all named people don't vsbsbs ate all of the chocolate, but fortunately he wasn't sick!\n",
      " \n",
      "numeral True 7.5410404 False\n",
      "big True 5.7743006 False\n",
      "grey True 6.496034 False\n",
      "dogs True 7.193099 False\n",
      "all True 4.932093 False\n",
      "named True 6.019446 False\n",
      "people True 5.853792 False\n",
      "do True 5.6477365 False\n",
      "n't True 5.2911263 False\n",
      "vsbsbs False 0.0 True\n",
      "ate True 6.7929115 False\n",
      "all True 4.932093 False\n",
      "of True 4.97793 False\n",
      "the True 4.70935 False\n",
      "chocolate True 7.4879966 False\n",
      ", True 5.094723 False\n",
      "but True 4.903002 False\n",
      "fortunately True 4.5155973 False\n",
      "he True 6.080851 False\n",
      "was True 5.4562387 False\n",
      "n't True 5.2911263 False\n",
      "sick True 6.052851 False\n",
      "! True 5.620569 False\n"
     ]
    }
   ],
   "source": [
    "import p9_util_spacy\n",
    "\n",
    "new_doc = p9_util_spacy.spacy_entity_replace(doc1)\n",
    "print()\n",
    "print(new_doc)\n",
    "print(\" \")\n",
    "for token in p9_util_spacy.spacy_nlp_md(new_doc):\n",
    "    print(token.text, token.has_vector, token.vector_norm, token.is_oov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text = !\n",
      " \n",
      "Vector= \n",
      "[-0.43819472 -0.15179151  0.6293961   2.4820278   0.04084247  0.08902603\n",
      "  2.1026154  -1.1063069  -3.831193   -2.61315     4.24136     0.26691943\n",
      "  0.27482885 -0.52809405 -4.138955    0.8584378  -0.95962286 -1.5518194\n",
      "  3.773027    1.0714208  -0.86820805  4.2855787   0.5026762   0.18378294\n",
      " -0.8183113   1.9246938  -3.5159698   1.3231446   3.5567977  -0.67705935\n",
      "  1.2564518  -1.2928632  -3.659481   -2.7772036  -0.9468338   2.104879\n",
      "  3.0953705  -4.43173    -4.0342417   0.7393956  -0.836823   -1.7227135\n",
      " -2.2815852   2.1140075   0.9151178   0.7378924   2.4087853  -0.43940118\n",
      " -1.009265    4.1392903  -2.7631278   1.9412398  -1.6945884  -0.7922518\n",
      " -4.152145   -0.9153952  -0.9478934  -0.7915096   1.0925504   0.61461663\n",
      "  0.18562198  2.3990839   0.805433   -0.41270816 -1.0921472  -1.6743193\n",
      "  1.7598526   1.0449028  -2.8417292   0.15773976 -3.263421    0.71981573\n",
      "  2.9071891  -3.1515841   0.14974713  3.014898    1.1181083  -0.31590992\n",
      " -1.1255305  -2.3033364   7.3138294   1.5994241  -0.46250653  2.0892353\n",
      "  7.5911283  -4.708711   -2.2122054   0.97776437  2.5404289  -1.7930715\n",
      " -1.8371508   3.268545    3.4427724  -2.9447982  -1.2671707   0.23274335]\n"
     ]
    }
   ],
   "source": [
    "print(\"Text = {}\".format(token.text))\n",
    "print(\" \")\n",
    "print(\"Vector= \")\n",
    "print(token.vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.43819472 -0.15179151  0.6293961   2.4820278   0.04084247  0.08902603\n",
      "  2.1026154  -1.1063069  -3.831193   -2.61315     4.24136     0.26691943\n",
      "  0.27482885 -0.52809405 -4.138955    0.8584378  -0.95962286 -1.5518194\n",
      "  3.773027    1.0714208  -0.86820805  4.2855787   0.5026762   0.18378294\n",
      " -0.8183113   1.9246938  -3.5159698   1.3231446   3.5567977  -0.67705935\n",
      "  1.2564518  -1.2928632  -3.659481   -2.7772036  -0.9468338   2.104879\n",
      "  3.0953705  -4.43173    -4.0342417   0.7393956  -0.836823   -1.7227135\n",
      " -2.2815852   2.1140075   0.9151178   0.7378924   2.4087853  -0.43940118\n",
      " -1.009265    4.1392903  -2.7631278   1.9412398  -1.6945884  -0.7922518\n",
      " -4.152145   -0.9153952  -0.9478934  -0.7915096   1.0925504   0.61461663\n",
      "  0.18562198  2.3990839   0.805433   -0.41270816 -1.0921472  -1.6743193\n",
      "  1.7598526   1.0449028  -2.8417292   0.15773976 -3.263421    0.71981573\n",
      "  2.9071891  -3.1515841   0.14974713  3.014898    1.1181083  -0.31590992\n",
      " -1.1255305  -2.3033364   7.3138294   1.5994241  -0.46250653  2.0892353\n",
      "  7.5911283  -4.708711   -2.2122054   0.97776437  2.5404289  -1.7930715\n",
      " -1.8371508   3.268545    3.4427724  -2.9447982  -1.2671707   0.23274335]\n"
     ]
    }
   ],
   "source": [
    "for token in p9_util_spacy.spacy_nlp_sm('!') :\n",
    "    print(token.vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scapy vectorization with 2D matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "import p9_util_spacy\n",
    "#-------------------------------------------------------------------------------\n",
    "#\n",
    "#-------------------------------------------------------------------------------\n",
    "def spacy_list_token_2_matrix_2D(list_token_string) :\n",
    "    '''Transforms a list of tokens into a mtrix where :\n",
    "        *   The number of rows is the number of tokens in the given list \n",
    "        *   The number of columns is the dimension vector of any token assigned \n",
    "            from Scapy model 'spacy_nlp_md'\n",
    "    Input : \n",
    "        *   list_token_string : a list of tokens as strings\n",
    "    output :\n",
    "        *   2D matrix (nb tokens X Spacy word dimension)\n",
    "    '''\n",
    "    if 0 == len(list_token_string) : \n",
    "        print(\"\\n***ERROR : empy list of tokens\")\n",
    "        return None\n",
    "    else :\n",
    "        pass\n",
    "\n",
    "    spacy_token = p9_util_spacy.SPACY_NLP_MD(list_token_string[0])\n",
    "    \n",
    "    nb_row = len(list_token_string)\n",
    "    nb_col = spacy_token.vector.shape[0]\n",
    "    vector_shape = (nb_row, nb_col)\n",
    "    matrix_2D = np.zeros(vector_shape)\n",
    "    index = 0\n",
    "    for token_string in list_token_string :\n",
    "        matrix_2D[index] = p9_util_spacy.SPACY_NLP_MD(token_string).vector\n",
    "        index +=1\n",
    "    return matrix_2D\n",
    "#-------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_list_token_2_matrix_2D(['asasss'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300,) (300,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-3.65519394e-02,  2.67574514e-02, -2.58605190e-02,  7.34347009e-02,\n",
       "        3.51218387e-02, -3.66729280e-02,  3.41177913e-02, -5.81839487e-02,\n",
       "        3.92813713e-02,  5.35514405e-01, -4.71596064e-02,  3.93592339e-02,\n",
       "        5.45215436e-02, -1.01632446e-02, -7.98095648e-02,  2.82833856e-03,\n",
       "       -2.54874705e-02,  1.05008537e-01,  2.94234954e-02,  3.57175968e-02,\n",
       "       -2.03564349e-02, -5.35384628e-02, -2.91548629e-02, -7.11596801e-03,\n",
       "        1.28174068e-02, -7.12944380e-03, -1.77854274e-02,  3.94528115e-02,\n",
       "       -3.97305450e-04, -4.24893661e-02, -2.82754006e-02,  2.82749012e-02,\n",
       "        4.13755134e-02, -2.93844623e-02,  1.44670247e-01, -7.09859753e-03,\n",
       "        3.97551437e-02, -3.50973808e-02, -2.13810446e-02, -5.77521024e-02,\n",
       "        3.74535652e-02, -9.70604275e-03, -2.32369022e-02, -1.74135763e-02,\n",
       "        1.42650749e-03, -3.32932345e-02,  8.32445605e-03, -4.36121046e-02,\n",
       "        4.91769818e-02,  1.33937989e-02, -3.92304596e-02,  3.23414949e-02,\n",
       "       -5.13326175e-02, -4.05491555e-03, -3.70912022e-03,  3.31800307e-02,\n",
       "       -1.00643167e-02,  1.56830999e-02, -3.18713174e-02, -1.43223776e-02,\n",
       "       -6.86899775e-03, -2.14262655e-02,  2.86908746e-02,  5.11107044e-02,\n",
       "        6.15988020e-02, -4.17497575e-02,  3.98959986e-02,  7.68048126e-02,\n",
       "        8.76967889e-03,  3.30745151e-02,  2.04542669e-02, -5.09859218e-02,\n",
       "       -4.14595667e-02,  1.99598284e-02,  6.31806152e-03,  3.24563937e-02,\n",
       "        1.38782531e-02,  1.60701231e-02, -3.55818348e-02, -5.23874725e-02,\n",
       "        1.87588265e-02,  4.23237547e-02, -2.61216631e-02,  3.36212670e-03,\n",
       "       -2.40372036e-02, -8.18982768e-02,  1.00142043e-01, -2.00719333e-02,\n",
       "        1.75009241e-02, -2.09033800e-02, -5.12115291e-02,  2.25487050e-02,\n",
       "       -4.90453127e-02, -2.39549090e-02,  2.55438716e-02, -3.59541849e-02,\n",
       "       -2.38333602e-02, -1.41353054e-02,  6.66036602e-04,  3.20462108e-02,\n",
       "        1.13861087e-02,  1.15440310e-02,  5.42212689e-02, -6.75939064e-02,\n",
       "       -5.84432955e-02, -1.15456283e-01, -2.04921986e-02, -2.97280475e-03,\n",
       "        4.00616867e-02, -6.23730253e-03, -1.94410341e-02,  2.43604396e-02,\n",
       "       -8.10812403e-02, -1.06297281e-02,  3.57494431e-02, -6.28911220e-02,\n",
       "        2.73831580e-02,  4.90460130e-02,  8.91352732e-02,  4.09526507e-02,\n",
       "       -3.64159765e-02, -1.76529595e-02, -9.58325564e-04, -7.11600773e-02,\n",
       "       -2.97650004e-02,  3.13542207e-02, -4.10163401e-03, -5.97825530e-02,\n",
       "       -6.01638870e-02, -4.53078415e-02, -5.95160122e-04,  2.40919086e-02,\n",
       "       -2.31395509e-03, -2.33964845e-02,  2.29748604e-02, -4.63959426e-02,\n",
       "       -2.97736823e-02,  1.32189055e-02,  3.46792131e-02,  1.16646196e-02,\n",
       "       -5.06734682e-01, -5.27767948e-03,  4.14151424e-02, -1.04122089e-02,\n",
       "       -7.84888726e-02, -4.63540147e-02, -6.98323975e-02,  8.00191954e-03,\n",
       "       -2.66534832e-04, -1.08290796e-02, -4.44079135e-02,  4.16576207e-02,\n",
       "        4.16452410e-03, -1.04464489e-02, -3.75813414e-02, -7.11626715e-02,\n",
       "       -5.34286542e-02, -2.84632703e-02,  8.26496056e-03, -5.60490788e-02,\n",
       "        1.25910025e-03, -1.89468942e-03, -8.17400907e-02,  1.12962649e-02,\n",
       "       -2.07896782e-02, -1.86064923e-02,  1.67036168e-02,  3.94198309e-02,\n",
       "        8.58150707e-02,  2.32553690e-03, -5.75973708e-02, -3.10617317e-02,\n",
       "        3.04458073e-02,  2.54890673e-02,  3.37510335e-02, -4.78788009e-02,\n",
       "       -7.76782887e-02,  7.69318923e-02, -5.80937027e-02,  3.27453915e-02,\n",
       "        1.40618376e-02, -1.01316986e-02, -2.43930832e-02, -2.26424405e-02,\n",
       "       -1.30501995e-02,  2.45041888e-02, -2.78852814e-02, -7.95470221e-03,\n",
       "       -1.52922830e-02, -4.33869986e-02, -4.29399796e-02, -2.94252909e-02,\n",
       "       -4.91511288e-02, -1.72259048e-02,  1.19915500e-02, -3.03101463e-02,\n",
       "        5.41903226e-02,  3.92324573e-02,  6.28401671e-04,  1.33486780e-02,\n",
       "       -3.02431621e-02, -2.15213999e-02, -1.65101550e-02, -4.66954192e-02,\n",
       "       -1.89969105e-02, -7.25652203e-03,  2.40082556e-02, -4.57400851e-03,\n",
       "        6.04044691e-04, -3.08061777e-02, -1.23118901e-01, -2.17547921e-02,\n",
       "       -3.61872767e-02,  3.46721250e-02,  3.12753556e-03, -4.39631921e-02,\n",
       "       -4.61613514e-02, -3.74039513e-02, -1.97981106e-02, -3.78049510e-02,\n",
       "       -2.13220473e-02, -5.00503567e-02,  1.07376106e-02,  7.30134353e-03,\n",
       "        4.53952890e-02, -3.42401783e-03,  5.33124550e-02,  2.03774983e-02,\n",
       "       -3.26100282e-02,  2.14223736e-02,  4.11221531e-02, -4.06270211e-02,\n",
       "        3.63432045e-02,  2.17599828e-02, -6.52899333e-03, -1.31749821e-02,\n",
       "       -5.85576941e-03, -3.91436153e-03, -1.09754246e-02, -2.06639949e-04,\n",
       "       -3.62766192e-03,  4.78214512e-02, -2.72404064e-03,  3.73927722e-02,\n",
       "        5.54241676e-02, -1.57021669e-02, -3.89679202e-02, -4.28151992e-02,\n",
       "        2.24387983e-03,  2.54405512e-02, -1.19098918e-02,  2.74789928e-02,\n",
       "       -2.22511238e-02, -2.16520710e-02, -4.55554107e-03,  3.32517060e-02,\n",
       "       -4.33827049e-02,  1.06218428e-02, -5.72839182e-02,  2.76437026e-02,\n",
       "       -3.19821226e-02,  5.46924474e-02, -3.95009884e-02,  5.28097355e-02,\n",
       "       -5.38958447e-03,  1.50816520e-02, -1.35495274e-02,  2.73022936e-03,\n",
       "       -1.02589862e-01,  5.64613212e-04, -8.41839217e-02, -3.47620678e-02,\n",
       "       -2.75040471e-02, -9.49592975e-02, -3.44847514e-02, -7.63068842e-03,\n",
       "       -3.25502221e-02,  4.87258714e-02,  2.51247041e-02,  1.87260834e-02,\n",
       "        1.20339760e-02,  5.37039660e-02, -8.72875134e-03, -4.59357456e-02,\n",
       "       -1.86874508e-02,  3.99722649e-02,  1.94327490e-02, -6.92330433e-02,\n",
       "       -3.09268663e-02, -1.53500819e-02, -2.52459051e-03,  2.05004833e-02,\n",
       "       -1.55326622e-02,  1.96656417e-02,  3.35214350e-02,  4.49995796e-02,\n",
       "        1.42230805e-02, -4.16172822e-03,  8.09296080e-03, -3.90446845e-02])"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import p9_util_spacy\n",
    "list_token_string_1 = ['big','empty','strange']\n",
    "list_token_string_2 = ['big','mother','asasss']\n",
    "vector_1 = p9_util_spacy.spacy_list_token_2_vector(list_token_string_1)\n",
    "vector_2 = p9_util_spacy.spacy_list_token_2_vector(list_token_string_2)\n",
    "\n",
    "print(vector_1.shape, vector_1.shape)\n",
    "vector_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import dot \n",
    "\n",
    "from numpy.linalg import norm \n",
    "\n",
    "#Generate word vector of the word - apple  \n",
    "\n",
    "#Cosine similarity function \n",
    "def cosine_similarity(v1,v2) :\n",
    "    return dot(v1, v2) / (norm(v1) * norm(v2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([33.34254924, 33.34254924, 33.34254924, 33.34254924, 33.34254924,\n",
       "       33.34254924])"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dot_matrix = np.zeros((3,3))\n",
    "for i in range(dot_matrix.shape[0]):\n",
    "    dot_matrix[i] = dot(matrix,matrix[i])\n",
    "arr_index = [0,1,2,4,5,8]    \n",
    "dot_matrix.flatten()[arr_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[33.34254924, 33.34254924, 33.34254924],\n",
       "       [33.34254924, 33.34254924, 33.34254924],\n",
       "       [33.34254924, 33.34254924, 33.34254924]])"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dot_matrix[0] = dot(matrix,matrix[0])\n",
    "dot_matrix[1] = dot(matrix,matrix[1])\n",
    "dot_matrix[2] = dot(matrix,matrix[2])\n",
    "#dot_matrix[0] = dot(matrix,matrix[0])\n",
    "#,dot(matrix,matrix[1]),dot(matrix,matrix[2])\n",
    "dot_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33.34254923919462, 11.459233077662075, 13.952208965501594)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_smilarity(matrix[0],matrix[0]),cosine_smilarity(matrix[0],matrix[1]),cosine_smilarity(matrix[0],matrix[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "others = list({w for w in parser.vocab if w.has_vector and w.orth_.islower() and w.lower_ != unicode(\"apple\")})\n",
    "\n",
    "# sort by similarity score\n",
    "others.sort(key=lambda w: cosine(w.vector, apple.vector)) \n",
    "others.reverse()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace out of vocabylary tokens with `supicious` token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 20 big grey dogs all named John don't vsbsbs ate all of the chocolate, but fortunately he wasn't sick!\n",
      "\n",
      "but fortunately of he wasn't 20 dogs vsbsbs sick the ate all big chocolate named grey don't john "
     ]
    }
   ],
   "source": [
    "import keras\n",
    "\n",
    "print(doc1)\n",
    "print()\n",
    "tokenizer = keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts([doc1])\n",
    "for token in tokenizer.word_docs :\n",
    "    print(token, end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hahahaha\n",
      "\n",
      "hahahaha "
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'am The 20 big grey dogs all named John don't vsbsbs ate all of the chocolate, but fortunately he wasn't sick!\n",
      "I'am The 20 big grey dogs all named John do n't toxic ate all of the chocolate , but fortunately he was n't sick !\n"
     ]
    }
   ],
   "source": [
    "import p9_util_spacy\n",
    "\n",
    "\n",
    "doc2 = \"I'am \"+doc1\n",
    "print(doc2)\n",
    "#new_doc = p9_util_spacy.spacy_oov_replace_suspicious(doc2)\n",
    "new_doc = p9_util_spacy.spacy_oov_replace_suspicious(doc2)\n",
    "print(new_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True [-0.49042   -0.3612    -0.17477   -0.63891   -0.031717  -0.016347\n",
      "  0.084408  -0.79719   -0.010443   0.11819   -0.37256    0.078303\n",
      " -0.057644   0.14062    0.22016    0.11733    0.42063    0.18421\n",
      "  0.073564  -0.097299   0.40872   -0.1423     0.43328   -0.3906\n",
      "  0.14511   -0.074233   0.019569  -0.34583    0.24154    0.066463\n",
      "  0.23082   -0.1781     0.14803    0.39453   -0.15667    0.11784\n",
      "  0.11859    0.51545   -0.40458    0.11039   -0.24179   -0.11072\n",
      "  0.0053935  0.29367    0.21391   -0.34037   -0.28134   -0.037268\n",
      "  0.25933    0.74726    0.42207    0.28843    0.3146    -0.085073\n",
      " -0.20981    0.29297   -0.22475    0.17854    0.56515    0.17686\n",
      " -0.45058    0.12036   -0.31861   -0.45192   -0.4926    -0.17866\n",
      "  0.0073433 -0.014333  -0.0095462 -0.026523  -0.1766    -0.062698\n",
      "  0.54997    0.26035   -0.78004   -0.075454   0.070443   0.52346\n",
      " -0.2994     0.51047    0.09974    0.64716   -0.053865   0.090406\n",
      " -0.11197    0.14479    1.5519    -1.7525    -0.51215    0.018492\n",
      " -0.19206   -0.067682  -0.14573    0.074656  -0.0062214  0.13368\n",
      " -0.36613   -0.06642    0.011256  -0.3414    -0.024573  -0.3267\n",
      "  0.28544    0.1194     0.6064    -0.075747   0.046203  -0.58596\n",
      " -0.13589   -0.094168   0.067018  -0.52274   -0.31544   -0.13995\n",
      "  0.081556   0.17413    0.14232   -0.25278   -0.35563   -0.046641\n",
      "  0.098537   0.18785    0.033028  -0.29679   -0.23592   -0.096575\n",
      " -0.3509    -0.044013   0.39536    0.14432   -0.36326   -0.37385\n",
      " -0.051564   0.11917    0.14397   -0.23662    0.10959    0.14608\n",
      " -0.5584     0.43888   -2.9751     0.32753    0.39521    0.019356\n",
      " -0.29448    0.14151   -0.20223    0.27056   -0.62018   -0.13518\n",
      " -0.30733    0.44658    0.033739  -0.26026    0.78382    0.16331\n",
      "  0.40637   -0.5354     0.28777   -0.29752    0.057033  -0.12142\n",
      " -0.19657   -0.063025  -0.012501  -0.086029  -0.40746    0.12274\n",
      " -0.05741    0.41014   -0.05631   -0.28366   -0.23363   -0.35184\n",
      " -0.1807    -0.42707   -0.40061    0.13605   -0.14143   -0.38371\n",
      " -0.10554   -0.2249    -0.54238   -0.24005   -0.0063797 -0.22216\n",
      " -0.25212   -0.2064     0.3739     0.11151    0.38834   -0.019393\n",
      " -0.11155   -0.07869    0.21216   -0.16452    0.11456   -1.0268\n",
      "  0.12905    0.089302  -0.14014    0.63827    0.051702  -0.48068\n",
      "  0.36471   -0.24702    0.12354    0.39149    0.21015   -0.36371\n",
      " -0.18566    0.21665    0.40983    0.1986     0.24892   -0.24377\n",
      "  0.12849   -0.26922    0.17118   -0.12452   -0.080495  -0.10058\n",
      " -0.20295   -0.21411   -0.057328  -0.095927  -0.6615     0.52966\n",
      "  0.064069  -0.57353    0.016379   0.080874  -0.062027  -0.24076\n",
      " -0.12965    0.10432   -0.045866  -0.058378  -0.017695  -0.003762\n",
      " -0.14708   -0.085455   0.11473   -0.096112   0.10899   -0.26528\n",
      " -0.099808  -0.12242    0.032592   0.8733    -0.0067434 -0.17017\n",
      " -0.47109    0.33805    0.19472   -0.054963   0.20971    0.43422\n",
      " -0.24698   -0.0065517 -0.069613   0.28466   -0.053524  -0.071551\n",
      " -0.70887   -0.36967   -0.36493    0.24646    0.5047     0.54571\n",
      " -0.39535   -0.21096   -0.55704    0.49523   -0.40161    0.47641\n",
      " -0.061112  -0.14352   -0.11734    0.023503   0.12317    0.12551\n",
      " -0.43269   -0.011004   0.21614    0.41618    0.25505   -0.20656\n",
      " -0.044061   0.6975     0.66238   -0.017398  -0.39941   -0.45199\n",
      "  0.64218   -0.0046849  0.25224    0.019256  -0.15528    0.38552  ]\n"
     ]
    }
   ],
   "source": [
    "doc = \"hahahaha\"\n",
    "\n",
    "spacy_doc = p9_util_spacy.SPACY_NLP_MD(doc)\n",
    "for token in spacy_doc:\n",
    "    print(token.has_vector,token.vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hahahaha\n",
      "hahahaha\n"
     ]
    }
   ],
   "source": [
    "import p9_util_spacy\n",
    "\n",
    "\n",
    "doc2 = \"hahahaha\"\n",
    "print(doc2)\n",
    "#new_doc = p9_util_spacy.spacy_oov_replace_suspicious(doc2)\n",
    "new_doc = p9_util_spacy.spacy_oov_replace_suspicious(doc2)\n",
    "print(new_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{I'am: (False, False, \"I'am\"),\n",
       " 20: (False, False, '20'),\n",
       " big: (False, False, 'big'),\n",
       " grey: (False, False, 'grey'),\n",
       " dog: (False, False, 'dog'),\n",
       " name: (True, False, 'name'),\n",
       " John: (False, False, 'John'),\n",
       " toxic: (False, False, 'toxic'),\n",
       " eat: (False, False, 'eat'),\n",
       " chocolate: (False, False, 'chocolate'),\n",
       " ,: (False, False, ','),\n",
       " fortunately: (False, False, 'fortunately'),\n",
       " sick: (False, False, 'sick'),\n",
       " !: (False, False, '!')}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_doc = p9_util_spacy.SPACY_NLP_MD(new_doc)\n",
    "dict_ = dict()\n",
    "for token in spacy_doc :\n",
    "    dict_[token] = (token.is_stop, token.is_oov, token.lemma_)\n",
    "dict_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'am The 20 big grey dogs all named John do n't toxic ate all of the chocolate , but fortunately he was n't sick !\n",
      "I'am The 20 big grey dogs all named John do n't toxic ate all of the chocolate , but fortunately he was n't sick !\n",
      "I'am The 20 big grey dogs all named John do n't toxic ate all of the chocolate , but fortunately he was n't sick !\n"
     ]
    }
   ],
   "source": [
    "import p9_util_spacy\n",
    "\n",
    "print(new_doc)\n",
    "new_doc2 = p9_util_spacy.spacy_oov_replace_suspicious(new_doc)\n",
    "print(new_doc2)\n",
    "print(p9_util_spacy.spacy_oov_replace_suspicious(new_doc2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 False False\n",
      "0.0 True False\n",
      "0.0 False True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(spacy.tokens.token.Token, 'http://www.mial.com')"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import p9_util_spacy\n",
    "document = ':-) fbt_telecom@toto.com http://www.mial.com' \n",
    "spacy_doc = p9_util_spacy.SPACY_NLP_SM(document)\n",
    "for token in spacy_doc :\n",
    "    print(token.sentiment, token.like_email, token.like_url)\n",
    "\n",
    "type(token), token.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
