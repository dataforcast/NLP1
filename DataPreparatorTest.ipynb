{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook allows to test `DataPreparator` class.\n",
    "\n",
    "Such object allows to process the whole pipeline for texts tokenization and vectorization.\n",
    "\n",
    "Tokenization part of transformation process includes : \n",
    "\n",
    "    * Entities detection and replacement with a key word\n",
    "    * Unknown words from vocabulary replacement with a key word.\n",
    "\n",
    "A transformed dataset is tested with the linear classifier `LinearSVC` from `scikit-learn` against 2 transformations : \n",
    "\n",
    "    * BOW transformation using Keras vectorizer\n",
    "    * DataPreparator transformation\n",
    "\n",
    "    TBD : \n",
    "* Rework tokenization process from inside DataPreparator : most frequent words out of stop-words must not be applied to lemmatization. DONE!\n",
    "* Write method `tokenizer` allowing to fix parameter `tokenizer` into `CountVectorizer` object: DONE\n",
    "* Apply TF-IDF coefficients to vectors coefficients issued from customized spacy process: DONE\n",
    "* Implements classification transformation for targets : DONE!\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>DataPreparator test</b>\n",
       "<div id=\"my_id_menu_nb\">run previous cell, wait for 2 seconds</div>\n",
       "<script>\n",
       "function repeat_indent_string(n){\n",
       "    var a = \"\" ;\n",
       "    for ( ; n > 0 ; --n)\n",
       "        a += \"    \";\n",
       "    return a;\n",
       "}\n",
       "// look up into all sections and builds an automated menu //\n",
       "var update_menu_string = function(begin, lfirst, llast, sformat, send, keep_item, begin_format, end_format) {\n",
       "    var anchors = document.getElementsByClassName(\"section\");\n",
       "    if (anchors.length == 0) {\n",
       "        anchors = document.getElementsByClassName(\"text_cell_render rendered_html\");\n",
       "    }\n",
       "    var i,t;\n",
       "    var text_menu = begin;\n",
       "    var text_memo = \"<pre>\\nlength:\" + anchors.length + \"\\n\";\n",
       "    var ind = \"\";\n",
       "    var memo_level = 1;\n",
       "    var href;\n",
       "    var tags = [];\n",
       "    var main_item = 0;\n",
       "    var format_open = 0;\n",
       "    for (i = 0; i <= llast; i++)\n",
       "        tags.push(\"h\" + i);\n",
       "\n",
       "    for (i = 0; i < anchors.length; i++) {\n",
       "        text_memo += \"**\" + anchors[i].id + \"--\\n\";\n",
       "\n",
       "        var child = null;\n",
       "        for(t = 0; t < tags.length; t++) {\n",
       "            var r = anchors[i].getElementsByTagName(tags[t]);\n",
       "            if (r.length > 0) {\n",
       "child = r[0];\n",
       "break;\n",
       "            }\n",
       "        }\n",
       "        if (child == null) {\n",
       "            text_memo += \"null\\n\";\n",
       "            continue;\n",
       "        }\n",
       "        if (anchors[i].hasAttribute(\"id\")) {\n",
       "            // when converted in RST\n",
       "            href = anchors[i].id;\n",
       "            text_memo += \"#1-\" + href;\n",
       "            // passer à child suivant (le chercher)\n",
       "        }\n",
       "        else if (child.hasAttribute(\"id\")) {\n",
       "            // in a notebook\n",
       "            href = child.id;\n",
       "            text_memo += \"#2-\" + href;\n",
       "        }\n",
       "        else {\n",
       "            text_memo += \"#3-\" + \"*\" + \"\\n\";\n",
       "            continue;\n",
       "        }\n",
       "        var title = child.textContent;\n",
       "        var level = parseInt(child.tagName.substring(1,2));\n",
       "\n",
       "        text_memo += \"--\" + level + \"?\" + lfirst + \"--\" + title + \"\\n\";\n",
       "\n",
       "        if ((level < lfirst) || (level > llast)) {\n",
       "            continue ;\n",
       "        }\n",
       "        if (title.endsWith('¶')) {\n",
       "            title = title.substring(0,title.length-1).replace(\"<\", \"&lt;\")\n",
       "         .replace(\">\", \"&gt;\").replace(\"&\", \"&amp;\");\n",
       "        }\n",
       "        if (title.length == 0) {\n",
       "            continue;\n",
       "        }\n",
       "\n",
       "        while (level < memo_level) {\n",
       "            text_menu += end_format + \"</ul>\\n\";\n",
       "            format_open -= 1;\n",
       "            memo_level -= 1;\n",
       "        }\n",
       "        if (level == lfirst) {\n",
       "            main_item += 1;\n",
       "        }\n",
       "        if (keep_item != -1 && main_item != keep_item + 1) {\n",
       "            // alert(main_item + \" - \" + level + \" - \" + keep_item);\n",
       "            continue;\n",
       "        }\n",
       "        while (level > memo_level) {\n",
       "            text_menu += \"<ul>\\n\";\n",
       "            memo_level += 1;\n",
       "        }\n",
       "        text_menu += repeat_indent_string(level-2);\n",
       "        text_menu += begin_format + sformat.replace(\"__HREF__\", href).replace(\"__TITLE__\", title);\n",
       "        format_open += 1;\n",
       "    }\n",
       "    while (1 < memo_level) {\n",
       "        text_menu += end_format + \"</ul>\\n\";\n",
       "        memo_level -= 1;\n",
       "        format_open -= 1;\n",
       "    }\n",
       "    text_menu += send;\n",
       "    //text_menu += \"\\n\" + text_memo;\n",
       "\n",
       "    while (format_open > 0) {\n",
       "        text_menu += end_format;\n",
       "        format_open -= 1;\n",
       "    }\n",
       "    return text_menu;\n",
       "};\n",
       "var update_menu = function() {\n",
       "    var sbegin = \"\";\n",
       "    var sformat = '<a href=\"#__HREF__\">__TITLE__</a>';\n",
       "    var send = \"\";\n",
       "    var begin_format = '<li>';\n",
       "    var end_format = '</li>';\n",
       "    var keep_item = -1;\n",
       "    var text_menu = update_menu_string(sbegin, 1, 4, sformat, send, keep_item,\n",
       "       begin_format, end_format);\n",
       "    var menu = document.getElementById(\"my_id_menu_nb\");\n",
       "    menu.innerHTML=text_menu;\n",
       "};\n",
       "window.setTimeout(update_menu,2000);\n",
       "            </script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jyquickhelper\n",
    "jyquickhelper.add_notebook_menu(first_level=1, last_level=4, header=\"DataPreparator test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=blus>1. Loading Data</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=blus>1.1. Loading dataset </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p5_util.object_load : fileName= ./data/df_train.dump\n",
      "p5_util.object_load : fileName= ./data/df_test.dump\n",
      "\n",
      "Shape of train corpus : = (1209265, 2)\n",
      "Shape of test corpus  : = (595609, 2)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import p5_util\n",
    "\n",
    "filename_train = './data/df_train.dump'\n",
    "filename_test  = './data/df_test.dump'\n",
    "\n",
    "if False: \n",
    "    df_train = pd.DataFrame({'X':X_train, 'y':y_train})\n",
    "    df_test  = pd.DataFrame({'X':X_test, 'y':y_test})\n",
    "    \n",
    "    p5_util.object_dump(df_train,filename_train)\n",
    "    p5_util.object_dump(df_test,filename_test)\n",
    "    \n",
    "else : \n",
    "    df_train = p5_util.object_load(filename_train)\n",
    "    df_test  = p5_util.object_load(filename_test)\n",
    "\n",
    "print(\"\")\n",
    "print(\"Shape of train corpus : = {}\".format(df_train.shape))\n",
    "print(\"Shape of test corpus  : = {}\".format(df_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['X', 'y'], dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a sample with all targets range from train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sample = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>some city council members support her.... .......</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i am not sure i understand this article. i agr...</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>what is repugnant of her attitude and the ungr...</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>just a non-related coincidence...................</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>not that much of a problem. he shot at and hit...</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>if individually they too fit the unappetizing ...</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>you say it’s racist to see black men as being ...</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>i wouldn't equate being ignorant to being a so...</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>no, they're noting that roger ailes is a despi...</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>how about \"bully in blue suit gets drop on una...</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>i already do exactly that. but thank you for t...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    X    y\n",
       "0   some city council members support her.... .......  0.0\n",
       "1   i am not sure i understand this article. i agr...  0.1\n",
       "2   what is repugnant of her attitude and the ungr...  0.2\n",
       "3   just a non-related coincidence...................  0.3\n",
       "4   not that much of a problem. he shot at and hit...  0.4\n",
       "5   if individually they too fit the unappetizing ...  0.5\n",
       "6   you say it’s racist to see black men as being ...  0.6\n",
       "7   i wouldn't equate being ignorant to being a so...  0.7\n",
       "8   no, they're noting that roger ailes is a despi...  0.8\n",
       "9   how about \"bully in blue suit gets drop on una...  0.9\n",
       "10  i already do exactly that. but thank you for t...  1.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "df_sample = pd.DataFrame()\n",
    "for val in range(n_sample):\n",
    "    arr_index = np.where(df_train['y']==val/10)[0][0]\n",
    "    X = df_train.iloc[arr_index]['X']\n",
    "    y = df_train.iloc[arr_index]['y']\n",
    "    \n",
    "    df = pd.DataFrame({'X':X, 'y':y}, index=[0])\n",
    "    df_sample = pd.concat([df_sample,df], axis=0,ignore_index=True,)\n",
    "df_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=blus>1.2. Building a dataset</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4]\n",
      "{'X': ['hello, Ah! gaomlebdo...', 'Hello world! Hello trumpets! The world is grey...', \"hello, The 20, big grey dogs all named John don't ate all of the chocolate, but fortunately he wasn't sick!\", \"hello, dady's home is a big area that is'nt pleasant.\", \"hello america, like santiago 's great fish, is now inert and being circled by sharks : russia, china, north korea.   this time , unlike any time since the civil war , it 's existential .   and who do we have protecting our way of life ?   i 'd say the keystone kops , but they actually always   got their man .   this crew is better described by macbeth :    . . . a tale , told by an idiot , full of sound and fury , and signifying nothing . \"], 'y': [0, 1, 2, 3, 4]}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hello, Ah! gaomlebdo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hello world! Hello trumpets! The world is grey...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hello, The 20, big grey dogs all named John do...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hello, dady's home is a big area that is'nt pl...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hello america, like santiago 's great fish, is...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   X  y\n",
       "0                            hello, Ah! gaomlebdo...  0\n",
       "1  Hello world! Hello trumpets! The world is grey...  1\n",
       "2  hello, The 20, big grey dogs all named John do...  2\n",
       "3  hello, dady's home is a big area that is'nt pl...  3\n",
       "4  hello america, like santiago 's great fish, is...  4"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "doc1 = \"hello, The 20, big grey dogs all named John don't ate all of the chocolate, but fortunately he wasn't sick!\"\n",
    "doc2 = \"Hello world! Hello trumpets! The world is grey...\"\n",
    "doc3 =\"hello, Ah! gaomlebdo...\"\n",
    "doc4 =\"hello, dady's home is a big area that is'nt pleasant.\"\n",
    "doc5 =\"hello america, like santiago 's great fish, is now inert and being circled by sharks : russia, china, north korea.   this time , unlike any time since the civil war , it 's existential .   and who do we have protecting our way of life ?   i 'd say the keystone kops , but they actually always   got their man .   this crew is better described by macbeth :   \"\" . . . a tale , told by an idiot , full of sound and fury , and signifying nothing . \"\"\"\n",
    "corpus = [doc3, doc2, doc1, doc4, doc5]\n",
    "\n",
    "y=np.asarray([i for i in range(len(corpus))])\n",
    "\n",
    "print(y)\n",
    "\n",
    "dict_corpus={'X':corpus, 'y':[y for y in range(0,len(corpus))]}\n",
    "print(dict_corpus)\n",
    "df_corpus = pd.DataFrame(dict_corpus, index=[y for y in range(0,len(corpus))])\n",
    "\n",
    "df_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=blus>2. Testing `DataPreparator`</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=blus>2.1. Unit test : `DataPreparator.fit` when target is `None`</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 704,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hello, Ah! gaomlebdo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hello world! The world is grey...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hello, The 20, big grey dogs all named John do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hello, dady's home is a big area that is'nt pl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hello america, like santiago 's great fish, is...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                docs\n",
       "0                            hello, Ah! gaomlebdo...\n",
       "1                  Hello world! The world is grey...\n",
       "2  hello, The 20, big grey dogs all named John do...\n",
       "3  hello, dady's home is a big area that is'nt pl...\n",
       "4  hello america, like santiago 's great fish, is..."
      ]
     },
     "execution_count": 704,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import DataPreparator\n",
    "\n",
    "dataPreparator = DataPreparator.DataPreparator(min_doc_len=4)\n",
    "self = dataPreparator\n",
    "\n",
    "self.fit(corpus)\n",
    "self.df_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=blus>2.2. Unit test : `DataPreparator.fit` with target</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 705,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docs</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hello, Ah! gaomlebdo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hello world! The world is grey...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hello, The 20, big grey dogs all named John do...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hello, dady's home is a big area that is'nt pl...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hello america, like santiago 's great fish, is...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                docs  target\n",
       "0                            hello, Ah! gaomlebdo...       0\n",
       "1                  Hello world! The world is grey...       1\n",
       "2  hello, The 20, big grey dogs all named John do...       2\n",
       "3  hello, dady's home is a big area that is'nt pl...       3\n",
       "4  hello america, like santiago 's great fish, is...       4"
      ]
     },
     "execution_count": 705,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=np.asarray([i for i in range(len(corpus))])\n",
    "\n",
    "print(y)\n",
    "\n",
    "#import DataPreparator\n",
    "\n",
    "dataPreparator = DataPreparator.DataPreparator(min_doc_len=4)\n",
    "self = dataPreparator\n",
    "\n",
    "self.fit(corpus,y)\n",
    "self.df_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=blus>2.3. Unit test : `DataPreparator.fit` + `DataPreparator.transform`</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spacy_clean : list_word_most_frequent length= 55\n",
      "spacy_clean : list_word_most_frequent length= 3\n",
      "spacy_clean : list_word_most_frequent length= 4\n",
      "spacy_clean : list_word_most_frequent length= 11\n",
      "spacy_clean : list_word_most_frequent length= 7\n",
      "spacy_clean : list_word_most_frequent length= 36\n",
      "[[ 2.84044176e-03  4.52739162e-03 -1.38072992e-03 ...  5.59997385e-03\n",
      "  -2.90898862e-03  1.55821764e-03]\n",
      " [-1.57047858e-03 -3.94367588e-03 -6.88491004e-03 ... -6.17553101e-03\n",
      "   4.45910196e-04  1.05588149e-03]\n",
      " [ 3.57008855e-03  5.47934189e-03 -1.21641764e-02 ... -8.35349162e-04\n",
      "  -4.62114010e-03 -7.43390943e-04]\n",
      " [-2.66431244e-02  1.28222519e-03  5.78991603e-05 ...  2.07061447e-02\n",
      "  -1.96218033e-02 -4.28440660e-03]]\n",
      "\n",
      "[ 2.  3.  4. nan]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docs</th>\n",
       "      <th>tokens</th>\n",
       "      <th>counting</th>\n",
       "      <th>vector</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hello world ! Hello trumpets ! The world is gr...</td>\n",
       "      <td>[hello, world, hello, trumpets, world, grey]</td>\n",
       "      <td>6</td>\n",
       "      <td>[0.0028404417634010316, 0.004527391617496808, ...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hello , The 20 , big grey dogs all named John ...</td>\n",
       "      <td>[hello, big, grey, dogs, named, ate, chocolate...</td>\n",
       "      <td>9</td>\n",
       "      <td>[-0.0015704785784085593, -0.003943675880630811...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hello , dady 's home is a big area that is'nt ...</td>\n",
       "      <td>[hello, dady, home, big, area, is'nt, pleasant]</td>\n",
       "      <td>7</td>\n",
       "      <td>[0.003570088545481364, 0.0054793418943881985, ...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hello america , like santiago 's great fish , ...</td>\n",
       "      <td>[hello, like, great, fish, inert, circled, sha...</td>\n",
       "      <td>35</td>\n",
       "      <td>[-0.026643124396602314, 0.0012822251891096432,...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                docs  \\\n",
       "1  Hello world ! Hello trumpets ! The world is gr...   \n",
       "2  hello , The 20 , big grey dogs all named John ...   \n",
       "3  hello , dady 's home is a big area that is'nt ...   \n",
       "4  hello america , like santiago 's great fish , ...   \n",
       "\n",
       "                                              tokens  counting  \\\n",
       "1       [hello, world, hello, trumpets, world, grey]         6   \n",
       "2  [hello, big, grey, dogs, named, ate, chocolate...         9   \n",
       "3    [hello, dady, home, big, area, is'nt, pleasant]         7   \n",
       "4  [hello, like, great, fish, inert, circled, sha...        35   \n",
       "\n",
       "                                              vector  target  \n",
       "1  [0.0028404417634010316, 0.004527391617496808, ...     2.0  \n",
       "2  [-0.0015704785784085593, -0.003943675880630811...     3.0  \n",
       "3  [0.003570088545481364, 0.0054793418943881985, ...     4.0  \n",
       "4  [-0.026643124396602314, 0.0012822251891096432,...     NaN  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import DataPreparator\n",
    "\n",
    "dataPreparator = DataPreparator.DataPreparator(min_doc_len=4)\n",
    "self = dataPreparator\n",
    "\n",
    "self.fit(corpus,y)\n",
    "\n",
    "#print(self.X)\n",
    "self.df_data\n",
    "\n",
    "X,y = self.transform()\n",
    "print(X)\n",
    "print(\"\")\n",
    "print(y)\n",
    "self.df_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=blus>2.4. Unit test : `DataPreparator.transform` after corpus transformation</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    This test allwos to validate a document transformation DataPreparator to be fit and transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 707,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['docs', 'tokens', 'counting', 'vector', 'target'], dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"hello america, like santiago 's great fish, is now inert and being circled by sharks : russia, china, north korea.   this time , unlike any time since the civil war , it 's existential .   and who do we have protecting our way of life ?   i 'd say the keystone kops , but they actually always   got their man .   this crew is better described by macbeth :    . . . a tale , told by an idiot , full of sound and fury , and signifying nothing . \""
      ]
     },
     "execution_count": 707,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(self.df_data.columns)\n",
    "corpus[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 715,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 300)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.04145703,  0.00957224, -0.01037137, -0.02114722,  0.01743454])"
      ]
     },
     "execution_count": 715,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The last document from corpus is selected\n",
    "doc = corpus[-1]\n",
    "X = self.transform(doc)\n",
    "print(X.shape)\n",
    "X[0][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=blus>2.5. Unit test : `DataPreparator.transform` with no parameter</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Should return vectorized corpus as well as target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 719,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello, Ah! gaomlebdo...\n",
      "Hello world! The world is grey...\n",
      "hello, The 20, big grey dogs all named John don't ate all of the chocolate, but fortunately he wasn't sick!\n",
      "hello, dady's home is a big area that is'nt pleasant.\n",
      "hello america, like santiago 's great fish, is now inert and being circled by sharks : russia, china, north korea.   this time , unlike any time since the civil war , it 's existential .   and who do we have protecting our way of life ?   i 'd say the keystone kops , but they actually always   got their man .   this crew is better described by macbeth :    . . . a tale , told by an idiot , full of sound and fury , and signifying nothing . \n"
     ]
    }
   ],
   "source": [
    "for document in corpus :\n",
    "    print(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 718,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 300) (4,)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docs</th>\n",
       "      <th>tokens</th>\n",
       "      <th>counting</th>\n",
       "      <th>vector</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hello world ! The world is grey ...</td>\n",
       "      <td>[hello, world, world, grey]</td>\n",
       "      <td>4</td>\n",
       "      <td>[0.0023407859851916632, 0.004395464559396108, ...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hello , The 20 , big grey dogs all named John ...</td>\n",
       "      <td>[hello, big, grey, dog, name, ate, chocolate, ...</td>\n",
       "      <td>9</td>\n",
       "      <td>[-0.002206428349018097, -0.0029030857731898625...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hello , dady 's home is a big area that is'nt ...</td>\n",
       "      <td>[hello, dady, home, big, area, is'nt, pleasant]</td>\n",
       "      <td>7</td>\n",
       "      <td>[0.003570088545481364, 0.0054793418943881985, ...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hello america , like santiago 's great fish , ...</td>\n",
       "      <td>[hello, like, great, fish, inert, circle, shar...</td>\n",
       "      <td>35</td>\n",
       "      <td>[-0.0414570323874553, 0.009572240822017193, -0...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                docs  \\\n",
       "1                Hello world ! The world is grey ...   \n",
       "2  hello , The 20 , big grey dogs all named John ...   \n",
       "3  hello , dady 's home is a big area that is'nt ...   \n",
       "4  hello america , like santiago 's great fish , ...   \n",
       "\n",
       "                                              tokens  counting  \\\n",
       "1                        [hello, world, world, grey]         4   \n",
       "2  [hello, big, grey, dog, name, ate, chocolate, ...         9   \n",
       "3    [hello, dady, home, big, area, is'nt, pleasant]         7   \n",
       "4  [hello, like, great, fish, inert, circle, shar...        35   \n",
       "\n",
       "                                              vector target  \n",
       "1  [0.0023407859851916632, 0.004395464559396108, ...   None  \n",
       "2  [-0.002206428349018097, -0.0029030857731898625...   None  \n",
       "3  [0.003570088545481364, 0.0054793418943881985, ...   None  \n",
       "4  [-0.0414570323874553, 0.009572240822017193, -0...   None  "
      ]
     },
     "execution_count": 718,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X,y = self.transform()\n",
    "print(X.shape, y.shape)\n",
    "self.df_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=blus>2.6. Unit test : `DataPreparator.fit_transform` </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    fit_transform method should result in calling fit() then transform() methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 808,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 300)\n",
      "\n",
      "(4,)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docs</th>\n",
       "      <th>tokens</th>\n",
       "      <th>counting</th>\n",
       "      <th>vector</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hello world ! The world is grey ...</td>\n",
       "      <td>[hello, world, world, grey]</td>\n",
       "      <td>4</td>\n",
       "      <td>[0.0023407859851916632, 0.004395464559396108, ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hello , The 20 , big grey dogs all named John ...</td>\n",
       "      <td>[hello, big, grey, dog, name, ate, chocolate, ...</td>\n",
       "      <td>9</td>\n",
       "      <td>[-0.002206428349018097, -0.0029030857731898625...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hello , dady 's home is a big area that is'nt ...</td>\n",
       "      <td>[hello, dady, home, big, area, is'nt, pleasant]</td>\n",
       "      <td>7</td>\n",
       "      <td>[0.003570088545481364, 0.0054793418943881985, ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hello america , like santiago 's great fish , ...</td>\n",
       "      <td>[hello, like, great, fish, inert, circle, shar...</td>\n",
       "      <td>35</td>\n",
       "      <td>[-0.0414570323874553, 0.009572240822017193, -0...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                docs  \\\n",
       "1                Hello world ! The world is grey ...   \n",
       "2  hello , The 20 , big grey dogs all named John ...   \n",
       "3  hello , dady 's home is a big area that is'nt ...   \n",
       "4  hello america , like santiago 's great fish , ...   \n",
       "\n",
       "                                              tokens  counting  \\\n",
       "1                        [hello, world, world, grey]         4   \n",
       "2  [hello, big, grey, dog, name, ate, chocolate, ...         9   \n",
       "3    [hello, dady, home, big, area, is'nt, pleasant]         7   \n",
       "4  [hello, like, great, fish, inert, circle, shar...        35   \n",
       "\n",
       "                                              vector  target  \n",
       "1  [0.0023407859851916632, 0.004395464559396108, ...     1.0  \n",
       "2  [-0.002206428349018097, -0.0029030857731898625...     1.0  \n",
       "3  [0.003570088545481364, 0.0054793418943881985, ...     1.0  \n",
       "4  [-0.0414570323874553, 0.009572240822017193, -0...     1.0  "
      ]
     },
     "execution_count": 808,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import DataPreparator\n",
    "import numpy as np\n",
    "dataPreparator = DataPreparator.DataPreparator(min_doc_len=4)\n",
    "self = dataPreparator\n",
    "y = np.ones(len(corpus))\n",
    "X,y = self.fit_transform(corpus,y)\n",
    "print(X.shape)\n",
    "print(\"\")\n",
    "print(y.shape)\n",
    "self.df_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=blus>2.7. Unit test : `DataPreparator.X` and `DataPreparator.y` </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=blus>2.6. Unit test : `DataPreparator.fit_transform` </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Returns data prepared X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 809,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 300)\n",
      "(4,)\n",
      "[1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(self.X.shape)\n",
    "print(self.y.shape)\n",
    "print(self.y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=blus>2.8. Unit test : `DataPreparator.fit_transform` with no parameter</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 810,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"hello america, like santiago 's great fish, is now inert and being circled by sharks : russia, china, north korea.   this time , unlike any time since the civil war , it 's existential .   and who do we have protecting our way of life ?   i 'd say the keystone kops , but they actually always   got their man .   this crew is better described by macbeth :    . . . a tale , told by an idiot , full of sound and fury , and signifying nothing . \""
      ]
     },
     "execution_count": 810,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 811,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** ERROR : X parameter is None!\n",
      "\n",
      "***ERROR : transform() : apply method fit first!\n"
     ]
    }
   ],
   "source": [
    "import DataPreparator\n",
    "\n",
    "dataPreparator = DataPreparator.DataPreparator(min_doc_len=4)\n",
    "self = dataPreparator\n",
    "X,y = self.fit_transform(None,None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=blus>2.9. Unit test : `DataPreparator.fit_transform` with a array parameter</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 813,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"hello america, like santiago 's great fish, is now inert and being circled by sharks : russia, china, north korea.   this time , unlike any time since the civil war , it 's existential .   and who do we have protecting our way of life ?   i 'd say the keystone kops , but they actually always   got their man .   this crew is better described by macbeth :    . . . a tale , told by an idiot , full of sound and fury , and signifying nothing . \"]\n",
      "(1,)\n",
      "\n",
      "(1, 300)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "arr_doc = np.array([doc])\n",
    "print(arr_doc)\n",
    "print(arr_doc.shape)\n",
    "\n",
    "\n",
    "dataPreparator = DataPreparator.DataPreparator(min_doc_len=4)\n",
    "self = dataPreparator\n",
    "X = self.fit_transform(arr_doc,None)\n",
    "print()\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=blus>2.10. Unit test : `p9_util_spacy.spacy_tokenizer`  process applied to a corpus</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 824,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X returned= hello\n",
      "y returned= ah\n",
      "\n",
      "df_data= unkonwn\n"
     ]
    }
   ],
   "source": [
    "import p9_util_spacy\n",
    "\n",
    "\n",
    "dict_param={\n",
    "'min_token_len': 2,\n",
    "'max_token_len':15,\n",
    "'min_word_len' : 1,\n",
    "'max_word_len' : 500,\n",
    "'max_length'   : 10,\n",
    "'is_spacy_data_prep' : True,\n",
    "'is_matrix_2D' : True,\n",
    "'oov_keyword' :'unkonwn',\n",
    "'entity_keyword' :None,#'entity',\n",
    "'return_df_data' : True,\n",
    "}\n",
    "df_data = None\n",
    "if dict_param['return_df_data']:\n",
    "    X,y_t,df_data = p9_util_spacy.spacy_tokenizer(corpus, y, **dict_param)\n",
    "else :\n",
    "    X,y_t = p9_util_spacy.spacy_tokenizer(corpus, y, **dict_param)\n",
    "\n",
    "\n",
    "print(\"X returned= {}\".format(X))\n",
    "\n",
    "if y_t is not None : print(\"y returned= {}\".format(y_t))\n",
    "print()\n",
    "if df_data is not None :\n",
    "    print(\"df_data= {}\".format(df_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 825,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello, Ah! gaomlebdo...\n",
      "Hello world! The world is grey...\n",
      "hello, The 20, big grey dogs all named John don't ate all of the chocolate, but fortunately he wasn't sick!\n",
      "hello, dady's home is a big area that is'nt pleasant.\n",
      "hello america, like santiago 's great fish, is now inert and being circled by sharks : russia, china, north korea.   this time , unlike any time since the civil war , it 's existential .   and who do we have protecting our way of life ?   i 'd say the keystone kops , but they actually always   got their man .   this crew is better described by macbeth :    . . . a tale , told by an idiot , full of sound and fury , and signifying nothing . \n"
     ]
    }
   ],
   "source": [
    "for text in corpus :\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=blus>3. Testing the `DataPreparator` pipeline</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 687,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4]\n",
      "\n",
      "hello, Ah! gaomlebdo...\n",
      "Hello world! The world is grey...\n",
      "hello, The 20, big grey dogs all named John don't ate all of the chocolate, but fortunately he wasn't sick!\n",
      "hello, dady's home is a big area that is'nt pleasant.\n",
      "hello america, like santiago 's great fish, is now inert and being circled by sharks : russia, china, north korea.   this time , unlike any time since the civil war , it 's existential .   and who do we have protecting our way of life ?   i 'd say the keystone kops , but they actually always   got their man .   this crew is better described by macbeth :    . . . a tale , told by an idiot , full of sound and fury , and signifying nothing . \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "y=np.asarray([i for i in range(len(corpus))])\n",
    "\n",
    "print(y)\n",
    "print(\"\")\n",
    "for text in corpus :\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 688,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "import DataPreparator\n",
    "\n",
    "classifier = LinearSVC()\n",
    "dataPreparator = DataPreparator.DataPreparator(min_doc_len=4)\n",
    "#help(classifier.fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 689,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([(\"dataPreparator\", dataPreparator),\n",
    "                 ('classifier', classifier)])\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pipe.fit(corpus,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=blus>3. Testing an estimator feeded with `DataPreparator` instance</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 691,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello, Ah! gaomlebdo...',\n",
       " 'Hello world! The world is grey...',\n",
       " \"hello, The 20, big grey dogs all named John don't ate all of the chocolate, but fortunately he wasn't sick!\",\n",
       " \"hello, dady's home is a big area that is'nt pleasant.\",\n",
       " \"hello america, like santiago 's great fish, is now inert and being circled by sharks : russia, china, north korea.   this time , unlike any time since the civil war , it 's existential .   and who do we have protecting our way of life ?   i 'd say the keystone kops , but they actually always   got their man .   this crew is better described by macbeth :    . . . a tale , told by an idiot , full of sound and fury , and signifying nothing . \"]"
      ]
     },
     "execution_count": 691,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 826,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "hello, Ah! gaomlebdo...\n",
      "Hello world! The world is grey...\n",
      "hello, The 20, big grey dogs all named John don't ate all of the chocolate, but fortunately he wasn't sick!\n",
      "hello, dady's home is a big area that is'nt pleasant.\n",
      "hello america, like santiago 's great fish, is now inert and being circled by sharks : russia, china, north korea.   this time , unlike any time since the civil war , it 's existential .   and who do we have protecting our way of life ?   i 'd say the keystone kops , but they actually always   got their man .   this crew is better described by macbeth :    . . . a tale , told by an idiot , full of sound and fury , and signifying nothing . \n",
      "\n",
      "(array([[ 0.00234079,  0.00439546,  0.00214962, ...,  0.00251769,\n",
      "        -0.00260394,  0.00259491],\n",
      "       [-0.00220643, -0.00290309, -0.00965031, ..., -0.00460167,\n",
      "         0.00182482,  0.00622651],\n",
      "       [ 0.00357009,  0.00547934, -0.01216418, ..., -0.00083535,\n",
      "        -0.00462114, -0.00074339],\n",
      "       [-0.04145703,  0.00957224, -0.01037137, ...,  0.01675679,\n",
      "        -0.01112593, -0.00126493]]), array([1, 2, 3, 4]))\n",
      "\n",
      "[0 1 2 3 4]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "import DataPreparator\n",
    "\n",
    "y=np.asarray([i for i in range(len(corpus))])\n",
    "\n",
    "print(\"\")\n",
    "for text in corpus :\n",
    "    print(text)\n",
    "print(\"\")\n",
    "\n",
    "dataPreparator = DataPreparator.DataPreparator(min_doc_len=4)\n",
    "dataPreparator.fit(corpus,y)\n",
    "\n",
    "X = dataPreparator.transform()\n",
    "print(\"\")\n",
    "print(X)\n",
    "print(\"\")\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 828,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.00111523  0.00317241 -0.00402532 ...  0.00214893 -0.00391827\n",
      "  -0.00010981]\n",
      " [ 0.00234079  0.00439546  0.00214962 ...  0.00251769 -0.00260394\n",
      "   0.00259491]\n",
      " [-0.00220643 -0.00290309 -0.00965031 ... -0.00460167  0.00182482\n",
      "   0.00622651]\n",
      " [ 0.00357009  0.00547934 -0.01216418 ... -0.00083535 -0.00462114\n",
      "  -0.00074339]\n",
      " [-0.04145703  0.00957224 -0.01037137 ...  0.01675679 -0.01112593\n",
      "  -0.00126493]]\n",
      "\n",
      "[0 1 2 3 4]\n"
     ]
    }
   ],
   "source": [
    "X = dataPreparator.transform(corpus,y)\n",
    "print(X)\n",
    "print(\"\")\n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 829,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docs</th>\n",
       "      <th>tokens</th>\n",
       "      <th>counting</th>\n",
       "      <th>vector</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hello world ! The world is grey ...</td>\n",
       "      <td>[hello, world, world, grey]</td>\n",
       "      <td>4</td>\n",
       "      <td>[0.0023407859851916632, 0.004395464559396108, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hello , The 20 , big grey dogs all named John ...</td>\n",
       "      <td>[hello, big, grey, dog, name, ate, chocolate, ...</td>\n",
       "      <td>9</td>\n",
       "      <td>[-0.002206428349018097, -0.0029030857731898625...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hello , dady 's home is a big area that is'nt ...</td>\n",
       "      <td>[hello, dady, home, big, area, is'nt, pleasant]</td>\n",
       "      <td>7</td>\n",
       "      <td>[0.003570088545481364, 0.0054793418943881985, ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hello america , like santiago 's great fish , ...</td>\n",
       "      <td>[hello, like, great, fish, inert, circle, shar...</td>\n",
       "      <td>35</td>\n",
       "      <td>[-0.0414570323874553, 0.009572240822017193, -0...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                docs  \\\n",
       "1                Hello world ! The world is grey ...   \n",
       "2  hello , The 20 , big grey dogs all named John ...   \n",
       "3  hello , dady 's home is a big area that is'nt ...   \n",
       "4  hello america , like santiago 's great fish , ...   \n",
       "\n",
       "                                              tokens  counting  \\\n",
       "1                        [hello, world, world, grey]         4   \n",
       "2  [hello, big, grey, dog, name, ate, chocolate, ...         9   \n",
       "3    [hello, dady, home, big, area, is'nt, pleasant]         7   \n",
       "4  [hello, like, great, fish, inert, circle, shar...        35   \n",
       "\n",
       "                                              vector  target  \n",
       "1  [0.0023407859851916632, 0.004395464559396108, ...       1  \n",
       "2  [-0.002206428349018097, -0.0029030857731898625...       2  \n",
       "3  [0.003570088545481364, 0.0054793418943881985, ...       3  \n",
       "4  [-0.0414570323874553, 0.009572240822017193, -0...       4  "
      ]
     },
     "execution_count": 829,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataPreparator.df_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=blus>4. Testing `DataPreparator` over sentiment analysis</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=blus>4.1. Dataset definition</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 830,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10\n",
      "6 6\n"
     ]
    }
   ],
   "source": [
    "import DataPreparator\n",
    "\n",
    "# Load sample data\n",
    "train = [('I love this sandwich when I eat it in california.', 'pos'),          \n",
    "         ('this is an amazing place!', 'pos'),\n",
    "         ('I feel very good about these beers.', 'pos'),\n",
    "         ('this is my best work.', 'pos'),\n",
    "         (\"what an awesome view\", 'pos'),\n",
    "         ('I do not like this restaurant in Huaweii', 'neg'),\n",
    "         ('I am tired of this stuff.', 'neg'),\n",
    "         (\"I can't deal with this\", 'neg'),\n",
    "         ('he is my sworn enemy!', 'neg'),          \n",
    "         ('my boss is horrible.', 'neg')] \n",
    "valid =   [('the beer was good.', 'pos'),     \n",
    "         ('I do not enjoy my job', 'neg'),\n",
    "         (\"I ain't feelin dandy today.\", 'neg'),\n",
    "         (\"I feel amazing!\", 'pos'),\n",
    "         ('Gary is a good friend of mine.', 'pos'),\n",
    "         (\"I can't believe I'm doing this.\", 'neg')]\n",
    "\n",
    "X_train = [tuple_text_state[0] for tuple_text_state in train]\n",
    "y_train = [tuple_text_state[1] for tuple_text_state in train]\n",
    "\n",
    "X_valid = [tuple_text_state[0] for tuple_text_state in valid]\n",
    "y_valid = [tuple_text_state[1] for tuple_text_state in valid]\n",
    "\n",
    "print(len(X_train), len(y_train))\n",
    "print(len(X_valid), len(y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 831,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I love this sandwich when I eat it in california.\n",
      "this is an amazing place!\n",
      "I feel very good about these beers.\n",
      "this is my best work.\n",
      "what an awesome view\n",
      "I do not like this restaurant in Huaweii\n",
      "I am tired of this stuff.\n",
      "I can't deal with this\n",
      "he is my sworn enemy!\n",
      "my boss is horrible.\n",
      "\n",
      "the beer was good.\n",
      "I do not enjoy my job\n",
      "I ain't feelin dandy today.\n",
      "I feel amazing!\n",
      "Gary is a good friend of mine.\n",
      "I can't believe I'm doing this.\n"
     ]
    }
   ],
   "source": [
    "print()\n",
    "for doc in X_train :\n",
    "    print(doc)\n",
    "print()\n",
    "for doc in X_valid :\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 832,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADV not True\n"
     ]
    }
   ],
   "source": [
    "import p9_util_spacy\n",
    "\n",
    "token_string = \"not\"\n",
    "doc = p9_util_spacy.SPACY_NLP_MD(token_string)\n",
    "for token in doc :\n",
    "    print(token.pos_, token.lemma_, token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=blus>4.2. Testing Keras data preparation</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=blus>4.3. Testing Data preparation with Spacy</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import DataPreparator\n",
    "\n",
    "dataPreparator = DataPreparator.DataPreparator(min_doc_len=2)\n",
    "dataPreparator.fit(X_train,y_train)\n",
    "self = dataPreparator\n",
    "\n",
    "dataPreparator = DataPreparator.DataPreparator(min_doc_len=2)\n",
    "dataPreparator.fit(X_valid,y_valid)\n",
    "self_valid = dataPreparator\n",
    "\n",
    "self.df_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(self.is_keras_vectorizer,self.is_spacy_vectorizer)\n",
    "self.is_keras_vectorizer = self.is_spacy_vectorizer\n",
    "print(self.is_keras_vectorizer,self.is_spacy_vectorizer)\n",
    "self.is_spacy_vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=blus>4.3.1. Suspect words replacement</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.df_data['docs'] = self.df_data['docs'].apply(lambda doc: p9_util_spacy.spacy_oov_replace_suspicious(doc,replaced='suspect'))\n",
    "\n",
    "self.df_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=blus>4.3.2. Entities replacement</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.df_data['docs'] = self.df_data['docs'].apply(lambda doc: p9_util_spacy.spacy_entity_replace(doc))\n",
    "\n",
    "self.df_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=blus>4.3.3. Tokenization </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = self.transform(X_train,y)\n",
    "\n",
    "self.df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation dataset transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid_t,y_valid_t = self_valid.transform(X_valid,y_valid)\n",
    "\n",
    "self_valid.df_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=blus>4.3.4. Spacy vectorization </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "self.df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=blus>4.3.4.1. Spacy vectorization with norm vector</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 683,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docs</th>\n",
       "      <th>tokens</th>\n",
       "      <th>counting</th>\n",
       "      <th>target</th>\n",
       "      <th>vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hello , Ah ! unknown ...</td>\n",
       "      <td>[hello, ah, unknown]</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>[5.586428391408666, 6.450794597335021, 5.56039...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hello world ! The world is grey ...</td>\n",
       "      <td>[hello, world, world, grey]</td>\n",
       "      <td>4</td>\n",
       "      <td>None</td>\n",
       "      <td>[5.586428391408666, 5.974551046413699, 5.97455...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hello , The 20 , big grey dogs all named John ...</td>\n",
       "      <td>[hello, big, grey, dog, name, ate, chocolate, ...</td>\n",
       "      <td>9</td>\n",
       "      <td>None</td>\n",
       "      <td>[5.586428391408666, 5.774300767595247, 6.49603...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hello , dady 's home is a big area that is'nt ...</td>\n",
       "      <td>[hello, dady, home, big, area, is'nt, pleasant]</td>\n",
       "      <td>7</td>\n",
       "      <td>None</td>\n",
       "      <td>[5.586428391408666, 6.261249540782498, 5.94138...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hello america , like santiago 's great fish , ...</td>\n",
       "      <td>[hello, like, great, fish, inert, circle, shar...</td>\n",
       "      <td>35</td>\n",
       "      <td>None</td>\n",
       "      <td>[5.586428391408666, 4.783219844540939, 5.43959...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                docs  \\\n",
       "0                           hello , Ah ! unknown ...   \n",
       "1                Hello world ! The world is grey ...   \n",
       "2  hello , The 20 , big grey dogs all named John ...   \n",
       "3  hello , dady 's home is a big area that is'nt ...   \n",
       "4  hello america , like santiago 's great fish , ...   \n",
       "\n",
       "                                              tokens  counting target  \\\n",
       "0                               [hello, ah, unknown]         3   None   \n",
       "1                        [hello, world, world, grey]         4   None   \n",
       "2  [hello, big, grey, dog, name, ate, chocolate, ...         9   None   \n",
       "3    [hello, dady, home, big, area, is'nt, pleasant]         7   None   \n",
       "4  [hello, like, great, fish, inert, circle, shar...        35   None   \n",
       "\n",
       "                                              vector  \n",
       "0  [5.586428391408666, 6.450794597335021, 5.56039...  \n",
       "1  [5.586428391408666, 5.974551046413699, 5.97455...  \n",
       "2  [5.586428391408666, 5.774300767595247, 6.49603...  \n",
       "3  [5.586428391408666, 6.261249540782498, 5.94138...  \n",
       "4  [5.586428391408666, 4.783219844540939, 5.43959...  "
      ]
     },
     "execution_count": 683,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import p9_util\n",
    "\n",
    "self.is_keras_vectorizer = True\n",
    "if self.COLUMN_NAME_VECTOR in self.df_data.columns :\n",
    "    del(self.df_data[self.COLUMN_NAME_VECTOR])\n",
    "self.df_data\n",
    "\n",
    "self.df_data[self.COLUMN_NAME_VECTOR] \\\n",
    "= self.df_data[self.COLUMN_NAME_TOKEN].apply(lambda list_token: \\\n",
    "                                             p9_util_spacy.spacy_token_norm_vectorization(list_token))\n",
    "\n",
    "self.df_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=blus>4.3.4.2. Spacy vectorization with 2D matrix (embeddings)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 684,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docs</th>\n",
       "      <th>tokens</th>\n",
       "      <th>counting</th>\n",
       "      <th>target</th>\n",
       "      <th>vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hello , Ah ! unknown ...</td>\n",
       "      <td>[hello, ah, unknown]</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>[[0.2523300051689148, 0.10175999999046326, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hello world ! The world is grey ...</td>\n",
       "      <td>[hello, world, world, grey]</td>\n",
       "      <td>4</td>\n",
       "      <td>None</td>\n",
       "      <td>[[0.2523300051689148, 0.10175999999046326, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hello , The 20 , big grey dogs all named John ...</td>\n",
       "      <td>[hello, big, grey, dog, name, ate, chocolate, ...</td>\n",
       "      <td>9</td>\n",
       "      <td>None</td>\n",
       "      <td>[[0.2523300051689148, 0.10175999999046326, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hello , dady 's home is a big area that is'nt ...</td>\n",
       "      <td>[hello, dady, home, big, area, is'nt, pleasant]</td>\n",
       "      <td>7</td>\n",
       "      <td>None</td>\n",
       "      <td>[[0.2523300051689148, 0.10175999999046326, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hello america , like santiago 's great fish , ...</td>\n",
       "      <td>[hello, like, great, fish, inert, circle, shar...</td>\n",
       "      <td>35</td>\n",
       "      <td>None</td>\n",
       "      <td>[[0.2523300051689148, 0.10175999999046326, -0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                docs  \\\n",
       "0                           hello , Ah ! unknown ...   \n",
       "1                Hello world ! The world is grey ...   \n",
       "2  hello , The 20 , big grey dogs all named John ...   \n",
       "3  hello , dady 's home is a big area that is'nt ...   \n",
       "4  hello america , like santiago 's great fish , ...   \n",
       "\n",
       "                                              tokens  counting target  \\\n",
       "0                               [hello, ah, unknown]         3   None   \n",
       "1                        [hello, world, world, grey]         4   None   \n",
       "2  [hello, big, grey, dog, name, ate, chocolate, ...         9   None   \n",
       "3    [hello, dady, home, big, area, is'nt, pleasant]         7   None   \n",
       "4  [hello, like, great, fish, inert, circle, shar...        35   None   \n",
       "\n",
       "                                              vector  \n",
       "0  [[0.2523300051689148, 0.10175999999046326, -0....  \n",
       "1  [[0.2523300051689148, 0.10175999999046326, -0....  \n",
       "2  [[0.2523300051689148, 0.10175999999046326, -0....  \n",
       "3  [[0.2523300051689148, 0.10175999999046326, -0....  \n",
       "4  [[0.2523300051689148, 0.10175999999046326, -0....  "
      ]
     },
     "execution_count": 684,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import p9_util\n",
    "\n",
    "self.is_keras_vectorizer = True\n",
    "if self.COLUMN_NAME_VECTOR in self.df_data.columns :\n",
    "    del(self.df_data[self.COLUMN_NAME_VECTOR])\n",
    "self.df_data\n",
    "\n",
    "self.df_data[self.COLUMN_NAME_VECTOR] \\\n",
    "= self.df_data[self.COLUMN_NAME_TOKEN].apply(lambda list_token: \\\n",
    "                                             p9_util_spacy.spacy_list_token_2_matrix_2D(list_token))\n",
    "\n",
    "self.df_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=blus>4.3.4.3. `DataPreparation` built from `corpus`</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 685,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docs</th>\n",
       "      <th>tokens</th>\n",
       "      <th>counting</th>\n",
       "      <th>vector</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hello , Ah ! unknown ...</td>\n",
       "      <td>[hello, ah, unknown]</td>\n",
       "      <td>3</td>\n",
       "      <td>[-0.001115233302116394, 0.003172410577535629, ...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hello world ! The world is grey ...</td>\n",
       "      <td>[hello, world, world, grey]</td>\n",
       "      <td>4</td>\n",
       "      <td>[0.0023407859851916632, 0.004395464559396108, ...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hello , The 20 , big grey dogs all named John ...</td>\n",
       "      <td>[hello, big, grey, dog, name, ate, chocolate, ...</td>\n",
       "      <td>9</td>\n",
       "      <td>[-0.002206428349018097, -0.0029030857731898625...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hello , dady 's home is a big area that is'nt ...</td>\n",
       "      <td>[hello, dady, home, big, area, is'nt, pleasant]</td>\n",
       "      <td>7</td>\n",
       "      <td>[0.003570088545481364, 0.0054793418943881985, ...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hello america , like santiago 's great fish , ...</td>\n",
       "      <td>[hello, like, great, fish, inert, circle, shar...</td>\n",
       "      <td>35</td>\n",
       "      <td>[-0.0414570323874553, 0.009572240822017193, -0...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                docs  \\\n",
       "0                           hello , Ah ! unknown ...   \n",
       "1                Hello world ! The world is grey ...   \n",
       "2  hello , The 20 , big grey dogs all named John ...   \n",
       "3  hello , dady 's home is a big area that is'nt ...   \n",
       "4  hello america , like santiago 's great fish , ...   \n",
       "\n",
       "                                              tokens  counting  \\\n",
       "0                               [hello, ah, unknown]         3   \n",
       "1                        [hello, world, world, grey]         4   \n",
       "2  [hello, big, grey, dog, name, ate, chocolate, ...         9   \n",
       "3    [hello, dady, home, big, area, is'nt, pleasant]         7   \n",
       "4  [hello, like, great, fish, inert, circle, shar...        35   \n",
       "\n",
       "                                              vector target  \n",
       "0  [-0.001115233302116394, 0.003172410577535629, ...   None  \n",
       "1  [0.0023407859851916632, 0.004395464559396108, ...   None  \n",
       "2  [-0.002206428349018097, -0.0029030857731898625...   None  \n",
       "3  [0.003570088545481364, 0.0054793418943881985, ...   None  \n",
       "4  [-0.0414570323874553, 0.009572240822017193, -0...   None  "
      ]
     },
     "execution_count": 685,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import DataPreparator\n",
    "\n",
    "dataPreparator = DataPreparator.DataPreparator(min_doc_len=2)\n",
    "dataPreparator.fit_transform(df_corpus.X.tolist(),np.array(df_corpus.y.tolist()))\n",
    "#dataPreparator.transform(df_corpus.X,df_corpus.y)\n",
    "#dataPreparator.transform(None, None)\n",
    "self = dataPreparator\n",
    "\n",
    "self.df_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=blus>4.3.4.4. Checking Target binarization from `DataPreparation` </font>\n",
    "\n",
    "Binarization to support followings types :\n",
    "    * pd.Series\n",
    "    * list\n",
    "    * numpy.ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_value = pd.Series([0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6])\n",
    "y_bin = self.vectorValue2BinaryvectorLabel(vector_value)\n",
    "print(y_bin)\n",
    "\n",
    "vector_value = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6]\n",
    "y_bin = self.vectorValue2BinaryvectorLabel(vector_value)\n",
    "print(y_bin)\n",
    "\n",
    "vector_value = np.array([0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6])\n",
    "y_bin = self.vectorValue2BinaryvectorLabel(vector_value)\n",
    "print(y_bin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=blus>4.3.4.4. `DataPreparation` built from `df_sample` issued from train dataset</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import DataPreparator\n",
    "\n",
    "dataPreparator = DataPreparator.DataPreparator(min_doc_len=2)\n",
    "dataPreparator.fit(df_sample.X.tolist(),df_sample.y)\n",
    "dataPreparator.transform(df_sample.X,df_sample.y)\n",
    "self = dataPreparator\n",
    "\n",
    "self.df_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.df_data.tokens.iloc[0][6].isspace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for list_token in self.df_data.tokens:\n",
    "    print(list_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import test_datapreparator\n",
    "\n",
    "test_datapreparator.test_DataPreparator(df_sample[0:7].X, df_sample[0:7].y, df_sample[7:].X, df_sample[7:].y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=blus>4.4. Testing Spacy data preparation process</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=blus>4.4.0. Testing Keras vectorization no Spacy preparation</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_keras_vectorizer=True\n",
    "is_spacy_data_preparation = False\n",
    "test_DataPreparation(X_train, y_train, X_valid, y_valid, is_keras_vectorizer, is_spacy_data_preparation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=blus>4.4.1. Testing Keras vectorization wih Spacy data preparation</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_keras_vectorizer=True\n",
    "is_spacy_data_preparation = True\n",
    "test_DataPreparation(X_train, y_train, X_valid, y_valid, is_keras_vectorizer, is_spacy_data_preparation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=blus>4.4.2. Testing Spacy vectorization</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_keras_vectorizer=False\n",
    "is_spacy_data_preparation = True\n",
    "test_DataPreparation(X_train, y_train, X_valid, y_valid, is_keras_vectorizer, is_spacy_data_preparation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. TF-IDF process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 833,
   "metadata": {},
   "outputs": [],
   "source": [
    "import p9_util\n",
    "import p9_util_spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 834,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello, Ah! gaomlebdo...',\n",
       " 'Hello world! The world is grey...',\n",
       " \"hello, The 20, big grey dogs all named John don't ate all of the chocolate, but fortunately he wasn't sick!\",\n",
       " \"hello, dady's home is a big area that is'nt pleasant.\",\n",
       " \"hello america, like santiago 's great fish, is now inert and being circled by sharks : russia, china, north korea.   this time , unlike any time since the civil war , it 's existential .   and who do we have protecting our way of life ?   i 'd say the keystone kops , but they actually always   got their man .   this crew is better described by macbeth :    . . . a tale , told by an idiot , full of sound and fury , and signifying nothing . \"]"
      ]
     },
     "execution_count": 834,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_corpus = corpus.copy()\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 835,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [-0.0016253989934921264, 0.0008732408781846364...\n",
       "1    [0.0023407859851916632, 0.004395464559396108, ...\n",
       "2    [-0.002206428349018097, -0.0029030857731898625...\n",
       "3    [0.003570088545481364, 0.0054793418943881985, ...\n",
       "4    [-0.0414570323874553, 0.009572240822017193, -0...\n",
       "Name: tokens, dtype: object"
      ]
     },
     "execution_count": 835,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import p9_util_spacy\n",
    "\n",
    "ser_corpus = pd.Series(list_corpus)\n",
    "\n",
    "df_dataprep = p9_util_spacy.spacy_dataprep(ser_corpus, \\\n",
    "                             oov_keyword=\"unkonwn\", \\\n",
    "                             entity_keyword=None, \\\n",
    "                             min_token_len=2, \\\n",
    "                             max_token_len=15)\n",
    "\n",
    "tfidfVectorizer = TfidfVectorizer(tokenizer = spacy_tokenizer)\n",
    "csr_tfidf = tfidfVectorizer.fit_transform(df_dataprep.docs)\n",
    "dict_token_tfidf = dict()\n",
    "for token, index in tfidfVectorizer.vocabulary_.items() :\n",
    "    dict_token_tfidf[token] = tfidfVectorizer.idf_[index]\n",
    "#dict_token_tfidf=None\n",
    "ser_vector = p9_util_spacy.spacy_vectorizer(df_dataprep.docs, df_dataprep.tokens,dict_token_tfidf=dict_token_tfidf)\n",
    "ser_vector"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Without TFIDF\n",
    "0    [-0.009580482348790295, 0.01701878377029223, -...\n",
    "1    [0.030285689944073796, 0.041482470286276746, -...\n",
    "2    [-0.005922531309746953, -0.00942279522680284, ...\n",
    "3    [0.0241735603545043, 0.03586585382509166, -0.0...\n",
    "4    [-0.05368133102142904, 0.013170875042685258, -...\n",
    "\n",
    "With TFIDF\n",
    "0    [-0.0016253989934921264, 0.0008732408781846364...\n",
    "1    [0.0023407859851916632, 0.004395464559396108, ...\n",
    "2    [-0.002206428349018097, -0.0029030857731898625...\n",
    "3    [0.003570088545481364, 0.0054793418943881985, ...\n",
    "4    [-0.0414570323874553, 0.009572240822017193, -0..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataPreparator tranformation with TFIDF coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 836,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hello, Ah! gaomlebdo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hello world! The world is grey...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hello, The 20, big grey dogs all named John do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hello, dady's home is a big area that is'nt pl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hello america, like santiago 's great fish, is...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                docs\n",
       "0                            hello, Ah! gaomlebdo...\n",
       "1                  Hello world! The world is grey...\n",
       "2  hello, The 20, big grey dogs all named John do...\n",
       "3  hello, dady's home is a big area that is'nt pl...\n",
       "4  hello america, like santiago 's great fish, is..."
      ]
     },
     "execution_count": 836,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import DataPreparator\n",
    "\n",
    "dataPreparator = DataPreparator.DataPreparator(min_doc_len=2)\n",
    "self = dataPreparator\n",
    "\n",
    "dataPreparator.fit(corpus)\n",
    "dataPreparator.df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 838,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transform : fit OK, Corpus vectorized!\n",
      "(5, 300)\n",
      "(5,)\n",
      "[[-0.00111523  0.00317241 -0.00402532 ...  0.00214893 -0.00391827\n",
      "  -0.00010981]\n",
      " [ 0.00234079  0.00439546  0.00214962 ...  0.00251769 -0.00260394\n",
      "   0.00259491]\n",
      " [-0.00220643 -0.00290309 -0.00965031 ... -0.00460167  0.00182482\n",
      "   0.00622651]\n",
      " [ 0.00357009  0.00547934 -0.01216418 ... -0.00083535 -0.00462114\n",
      "  -0.00074339]\n",
      " [-0.04145703  0.00957224 -0.01037137 ...  0.01675679 -0.01112593\n",
      "  -0.00126493]]\n"
     ]
    }
   ],
   "source": [
    "X = dataPreparator.transform()\n",
    "\n",
    "print(X.shape)\n",
    "if y is not None : print(y.shape) \n",
    "\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "CORPUS WITH TF-IDF\n",
    "-----------------\n",
    "[[-0.00111523  0.00317241 -0.00402532 ...  0.00214893 -0.00391827\n",
    "  -0.00010981]\n",
    " [ 0.00234079  0.00439546  0.00214962 ...  0.00251769 -0.00260394\n",
    "   0.00259491]\n",
    " [-0.00220643 -0.00290309 -0.00965031 ... -0.00460167  0.00182482\n",
    "   0.00622651]\n",
    " [ 0.00357009  0.00547934 -0.01216418 ... -0.00083535 -0.00462114\n",
    "  -0.00074339]\n",
    " [-0.04145703  0.00957224 -0.01037137 ...  0.01675679 -0.01112593\n",
    "  -0.00126493]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 839,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 300)\n",
      "\n",
      "[[ 5.10165691e-04  2.29916970e-03 -1.33765548e-03  1.22265150e-03\n",
      "  -2.06125697e-03  1.53394570e-03 -1.67007565e-03 -4.49536790e-04\n",
      "  -1.18522634e-03  1.34143305e-02  5.32131096e-04  1.82439367e-03\n",
      "  -1.91085656e-03  1.14933997e-03  7.22482254e-04  1.87951724e-03\n",
      "   2.67391205e-04  9.10238107e-03  3.55462988e-03 -6.68925693e-04\n",
      "   1.21033967e-03 -1.98171953e-03 -3.94776940e-03  1.78829749e-03\n",
      "   1.57270004e-03 -1.57318970e-03  1.90386097e-03  1.81222161e-03\n",
      "   7.97262788e-04  1.65566524e-03  3.35008462e-03  6.20461752e-04\n",
      "  -1.87154253e-03 -2.57422765e-03 -1.70785069e-03  2.14953860e-03\n",
      "   1.91295505e-03  2.50161563e-04  2.40556935e-03  1.23552303e-03\n",
      "   3.43346953e-03 -5.29060165e-04 -2.59290556e-03  2.19395931e-04\n",
      "   2.48986358e-04 -2.48874426e-03  1.87161227e-03  7.88378716e-04\n",
      "  -3.49244072e-05  3.52245073e-04 -2.85222406e-04 -1.57829642e-03\n",
      "  -4.92726207e-03 -1.04035209e-03  1.24503672e-03  3.61325105e-03\n",
      "  -1.05805039e-03 -3.38548144e-03  1.49624060e-03 -3.93818592e-04\n",
      "   1.66986585e-03 -6.30017432e-05  3.96273971e-03  7.57249296e-04\n",
      "  -2.04139014e-03  2.18941232e-03 -1.63453917e-03 -2.09343572e-03\n",
      "  -1.26406421e-03  5.77845871e-04  1.68392658e-03  2.61557043e-03\n",
      "  -8.38815371e-04  8.78968761e-07  3.11518013e-03 -1.67798042e-03\n",
      "  -1.49001469e-03 -7.98172206e-04 -1.66077177e-03  4.11286031e-04\n",
      "  -8.96597107e-04 -1.35927111e-03 -4.23667868e-03  1.42048071e-03\n",
      "   1.27707551e-03 -3.59093547e-03  3.87354891e-03 -3.71034622e-03\n",
      "  -5.16454478e-04  4.28011964e-04 -7.44867474e-04 -3.96987498e-04\n",
      "  -3.91992807e-03 -2.14723016e-05  3.04844419e-03 -6.84105635e-03\n",
      "  -2.26741061e-04  1.66874667e-03  4.78091836e-04  3.45431591e-04\n",
      "  -4.31964358e-03 -1.06147806e-03  1.57689730e-03  8.33009183e-04\n",
      "   1.42740617e-03 -4.05654788e-03 -1.08351350e-03  2.50651260e-03\n",
      "   1.82929039e-03  2.69559761e-03  5.24492214e-04  1.43307249e-04\n",
      "  -6.24414136e-04  1.57780667e-03 -4.47976788e-04  1.97045704e-03\n",
      "  -1.26434403e-04  2.22173095e-03  6.37250592e-04 -2.09728320e-03\n",
      "  -2.15394576e-03  5.01477371e-04  2.16206034e-03 -2.72637705e-03\n",
      "  -2.23439256e-03  5.24862955e-04  3.20542033e-04  2.63319860e-04\n",
      "  -8.90021523e-04 -3.63269796e-04  1.25594944e-03  3.52965593e-03\n",
      "   1.11170491e-03  3.42605462e-04 -8.71203939e-04 -1.42558734e-03\n",
      "  -4.23297087e-03 -1.65909290e-03  3.22899481e-03  4.54916209e-04\n",
      "  -5.56307157e-03 -6.50856644e-04 -1.77850405e-03  2.16870586e-03\n",
      "   2.30532562e-04  1.55856937e-03  2.31574873e-03 -1.09701465e-03\n",
      "  -9.05271371e-04 -3.40471864e-04  5.92781045e-05 -3.43214035e-03\n",
      "   2.88713078e-03 -1.56703383e-03 -8.11743240e-04 -2.05265264e-03\n",
      "   3.70055282e-05  1.21957352e-03 -8.89811714e-04  5.90738406e-05\n",
      "   3.21948106e-04 -1.73254430e-03  1.48896545e-03  2.44621237e-03\n",
      "   4.86017615e-04  7.55920162e-04  6.74514969e-04  7.59138018e-04\n",
      "   5.26094119e-03 -6.46617450e-05  3.05180212e-03  2.58458098e-03\n",
      "   8.25594068e-04  1.55479193e-03 -1.90770845e-03  8.77010028e-04\n",
      "   6.30115320e-04 -4.64227041e-03 -2.70825922e-04 -1.61691089e-03\n",
      "   2.46775826e-03  3.42899243e-03  1.29862140e-03 -8.97016923e-04\n",
      "  -7.02405522e-04 -1.57640765e-03 -2.73847918e-03  2.12393562e-03\n",
      "   3.55602851e-04 -8.97576412e-04  1.40271246e-03  3.05145224e-03\n",
      "  -1.76500293e-04  2.12309619e-03  3.48173796e-05 -2.81088134e-03\n",
      "   1.97171631e-04 -1.80578589e-03  1.17046605e-03 -2.54848475e-04\n",
      "  -7.55990098e-04  1.39214943e-03 -1.69399997e-03  4.11516900e-04\n",
      "  -9.48153039e-04  1.19173199e-03  1.73198481e-03 -4.10397624e-04\n",
      "  -3.02752803e-04  1.27259851e-03  1.11499270e-03 -3.32651039e-03\n",
      "   6.64224823e-04  5.38839698e-04 -6.22756183e-04 -6.11906370e-03\n",
      "  -2.57667626e-04  1.14465306e-03 -1.69882675e-03 -3.65382393e-03\n",
      "   1.77654545e-03  1.92030013e-03  2.12372566e-04 -7.73548484e-04\n",
      "  -1.06875330e-03 -2.08790938e-03  3.19611649e-03 -5.27073493e-04\n",
      "  -8.28252335e-04 -2.18472540e-03 -7.75856972e-04 -3.26683978e-03\n",
      "   3.42129787e-03 -1.74079895e-03 -9.72147187e-04 -2.61025409e-03\n",
      "  -4.84289765e-03  3.17163289e-03 -1.34017378e-03  1.22419049e-03\n",
      "   3.74021704e-04  4.35035348e-03 -7.16466258e-04  1.07455949e-03\n",
      "   2.05768943e-04  1.60998543e-03  9.10028219e-04  1.85377419e-03\n",
      "   2.50287503e-04 -2.03922153e-03  1.24790480e-03  2.05181321e-03\n",
      "  -6.70800408e-04 -1.98129992e-03  8.29021831e-04  6.94479744e-04\n",
      "  -2.29791045e-03 -9.02053515e-04  3.17946772e-04  1.41495436e-03\n",
      "  -2.34554907e-03  1.26973043e-03  5.97055207e-04 -1.19494985e-03\n",
      "  -3.65753134e-03  4.86255437e-04  2.78066138e-03  1.38249586e-03\n",
      "   2.36261785e-03  1.90798839e-04 -1.51974509e-04  4.40442761e-03\n",
      "  -1.92841470e-03 -4.03661092e-03  3.34980488e-03  6.89988782e-04\n",
      "  -1.22712861e-03  1.53982182e-03 -3.71958017e-03 -1.26301477e-03\n",
      "   1.98346853e-03  1.14752124e-03  3.50699107e-03 -4.97049332e-03\n",
      "   4.24738129e-03 -6.75634245e-04 -1.80137893e-03  2.60626674e-03\n",
      "   1.82915052e-03 -1.28924757e-03 -3.75385801e-03 -1.25664900e-03\n",
      "  -8.49168499e-04  1.22489005e-03 -2.42130881e-04  8.63998731e-04\n",
      "  -2.30126818e-03  4.08012196e-04 -1.59151763e-03 -2.39304761e-03]]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "import p9_util\n",
    "X  = self.transform(X=[\"ign09nt\"])\n",
    "print(X.shape)\n",
    "print(\"\")\n",
    "print(X)\n",
    "print(self.llist_error_doc)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[[ 5.10165691e-04  2.29916970e-03 -1.33765548e-03  1.22265150e-03\n",
    "  -2.06125697e-03  1.53394570e-03 -1.67007565e-03 -4.49536790e-04\n",
    "  -1.18522634e-03  1.34143305e-02  5.32131096e-04  1.82439367e-03\n",
    "  -1.91085656e-03  1.14933997e-03  7.22482254e-04  1.87951724e-03\n",
    "   2.67391205e-04  9.10238107e-03  3.55462988e-03 -6.68925693e-04\n",
    "   1.21033967e-03 -1.98171953e-03 -3.94776940e-03  1.78829749e-03\n",
    "   1.57270004e-03 -1.57318970e-03  1.90386097e-03  1.81222161e-03\n",
    "   7.97262788e-04  1.65566524e-03  3.35008462e-03  6.20461752e-04\n",
    "  -1.87154253e-03 -2.57422765e-03 -1.70785069e-03  2.14953860e-03\n",
    "   1.91295505e-03  2.50161563e-04  2.40556935e-03  1.23552303e-03\n",
    "   3.43346953e-03 -5.29060165e-04 -2.59290556e-03  2.19395931e-04\n",
    "   2.48986358e-04 -2.48874426e-03  1.87161227e-03  7.88378716e-04\n",
    "  -3.49244072e-05  3.52245073e-04 -2.85222406e-04 -1.57829642e-03\n",
    "  -4.92726207e-03 -1.04035209e-03  1.24503672e-03  3.61325105e-03\n",
    "  -1.05805039e-03 -3.38548144e-03  1.49624060e-03 -3.93818592e-04\n",
    "   1.66986585e-03 -6.30017432e-05  3.96273971e-03  7.57249296e-04\n",
    "  -2.04139014e-03  2.18941232e-03 -1.63453917e-03 -2.09343572e-03\n",
    "  -1.26406421e-03  5.77845871e-04  1.68392658e-03  2.61557043e-03\n",
    "  -8.38815371e-04  8.78968761e-07  3.11518013e-03 -1.67798042e-03\n",
    "  -1.49001469e-03 -7.98172206e-04 -1.66077177e-03  4.11286031e-04\n",
    "  -8.96597107e-04 -1.35927111e-03 -4.23667868e-03  1.42048071e-03\n",
    "   1.27707551e-03 -3.59093547e-03  3.87354891e-03 -3.71034622e-03\n",
    "  -5.16454478e-04  4.28011964e-04 -7.44867474e-04 -3.96987498e-04\n",
    "  -3.91992807e-03 -2.14723016e-05  3.04844419e-03 -6.84105635e-03\n",
    "  -2.26741061e-04  1.66874667e-03  4.78091836e-04  3.45431591e-04\n",
    "  -4.31964358e-03 -1.06147806e-03  1.57689730e-03  8.33009183e-04\n",
    "   1.42740617e-03 -4.05654788e-03 -1.08351350e-03  2.50651260e-03\n",
    "   1.82929039e-03  2.69559761e-03  5.24492214e-04  1.43307249e-04\n",
    "  -6.24414136e-04  1.57780667e-03 -4.47976788e-04  1.97045704e-03\n",
    "  -1.26434403e-04  2.22173095e-03  6.37250592e-04 -2.09728320e-03\n",
    "  -2.15394576e-03  5.01477371e-04  2.16206034e-03 -2.72637705e-03\n",
    "  -2.23439256e-03  5.24862955e-04  3.20542033e-04  2.63319860e-04\n",
    "  -8.90021523e-04 -3.63269796e-04  1.25594944e-03  3.52965593e-03\n",
    "   1.11170491e-03  3.42605462e-04 -8.71203939e-04 -1.42558734e-03\n",
    "  -4.23297087e-03 -1.65909290e-03  3.22899481e-03  4.54916209e-04\n",
    "  -5.56307157e-03 -6.50856644e-04 -1.77850405e-03  2.16870586e-03\n",
    "   2.30532562e-04  1.55856937e-03  2.31574873e-03 -1.09701465e-03\n",
    "  -9.05271371e-04 -3.40471864e-04  5.92781045e-05 -3.43214035e-03\n",
    "   2.88713078e-03 -1.56703383e-03 -8.11743240e-04 -2.05265264e-03\n",
    "   3.70055282e-05  1.21957352e-03 -8.89811714e-04  5.90738406e-05\n",
    "   3.21948106e-04 -1.73254430e-03  1.48896545e-03  2.44621237e-03\n",
    "   4.86017615e-04  7.55920162e-04  6.74514969e-04  7.59138018e-04\n",
    "   5.26094119e-03 -6.46617450e-05  3.05180212e-03  2.58458098e-03\n",
    "   8.25594068e-04  1.55479193e-03 -1.90770845e-03  8.77010028e-04\n",
    "   6.30115320e-04 -4.64227041e-03 -2.70825922e-04 -1.61691089e-03\n",
    "   2.46775826e-03  3.42899243e-03  1.29862140e-03 -8.97016923e-04\n",
    "  -7.02405522e-04 -1.57640765e-03 -2.73847918e-03  2.12393562e-03\n",
    "   3.55602851e-04 -8.97576412e-04  1.40271246e-03  3.05145224e-03\n",
    "  -1.76500293e-04  2.12309619e-03  3.48173796e-05 -2.81088134e-03\n",
    "   1.97171631e-04 -1.80578589e-03  1.17046605e-03 -2.54848475e-04\n",
    "  -7.55990098e-04  1.39214943e-03 -1.69399997e-03  4.11516900e-04\n",
    "  -9.48153039e-04  1.19173199e-03  1.73198481e-03 -4.10397624e-04\n",
    "  -3.02752803e-04  1.27259851e-03  1.11499270e-03 -3.32651039e-03\n",
    "   6.64224823e-04  5.38839698e-04 -6.22756183e-04 -6.11906370e-03\n",
    "  -2.57667626e-04  1.14465306e-03 -1.69882675e-03 -3.65382393e-03\n",
    "   1.77654545e-03  1.92030013e-03  2.12372566e-04 -7.73548484e-04\n",
    "  -1.06875330e-03 -2.08790938e-03  3.19611649e-03 -5.27073493e-04\n",
    "  -8.28252335e-04 -2.18472540e-03 -7.75856972e-04 -3.26683978e-03\n",
    "   3.42129787e-03 -1.74079895e-03 -9.72147187e-04 -2.61025409e-03\n",
    "  -4.84289765e-03  3.17163289e-03 -1.34017378e-03  1.22419049e-03\n",
    "   3.74021704e-04  4.35035348e-03 -7.16466258e-04  1.07455949e-03\n",
    "   2.05768943e-04  1.60998543e-03  9.10028219e-04  1.85377419e-03\n",
    "   2.50287503e-04 -2.03922153e-03  1.24790480e-03  2.05181321e-03\n",
    "  -6.70800408e-04 -1.98129992e-03  8.29021831e-04  6.94479744e-04\n",
    "  -2.29791045e-03 -9.02053515e-04  3.17946772e-04  1.41495436e-03\n",
    "  -2.34554907e-03  1.26973043e-03  5.97055207e-04 -1.19494985e-03\n",
    "  -3.65753134e-03  4.86255437e-04  2.78066138e-03  1.38249586e-03\n",
    "   2.36261785e-03  1.90798839e-04 -1.51974509e-04  4.40442761e-03\n",
    "  -1.92841470e-03 -4.03661092e-03  3.34980488e-03  6.89988782e-04\n",
    "  -1.22712861e-03  1.53982182e-03 -3.71958017e-03 -1.26301477e-03\n",
    "   1.98346853e-03  1.14752124e-03  3.50699107e-03 -4.97049332e-03\n",
    "   4.24738129e-03 -6.75634245e-04 -1.80137893e-03  2.60626674e-03\n",
    "   1.82915052e-03 -1.28924757e-03 -3.75385801e-03 -1.25664900e-03\n",
    "  -8.49168499e-04  1.22489005e-03 -2.42130881e-04  8.63998731e-04\n",
    "  -2.30126818e-03  4.08012196e-04 -1.59151763e-03 -2.39304761e-03]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 840,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 840,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self.llist_error_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 842,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello, Ah! gaomlebdo...', 'Hello world! The world is grey...', \"hello, The 20, big grey dogs all named John don't ate all of the chocolate, but fortunately he wasn't sick!\", \"hello, dady's home is a big area that is'nt pleasant.\", \"hello america, like santiago 's great fish, is now inert and being circled by sharks : russia, china, north korea.   this time , unlike any time since the civil war , it 's existential .   and who do we have protecting our way of life ?   i 'd say the keystone kops , but they actually always   got their man .   this crew is better described by macbeth :    . . . a tale , told by an idiot , full of sound and fury , and signifying nothing . \"]\n",
      "(5, 300)\n",
      "\n",
      "                                                docs  \\\n",
      "0                           hello , Ah ! unknown ...   \n",
      "1                Hello world ! The world is grey ...   \n",
      "2  hello , The 20 , big grey dogs all named John ...   \n",
      "3  hello , dady 's home is a big area that is'nt ...   \n",
      "4  hello america , like santiago 's great fish , ...   \n",
      "\n",
      "                                              tokens  counting  \\\n",
      "0                               [hello, ah, unknown]         3   \n",
      "1                        [hello, world, world, grey]         4   \n",
      "2  [hello, big, grey, dog, name, ate, chocolate, ...         9   \n",
      "3    [hello, dady, home, big, area, is'nt, pleasant]         7   \n",
      "4  [hello, like, great, fish, inert, circle, shar...        35   \n",
      "\n",
      "                                              vector target  \n",
      "0  [-0.001115233302116394, 0.003172410577535629, ...   None  \n",
      "1  [0.0023407859851916632, 0.004395464559396108, ...   None  \n",
      "2  [-0.002206428349018097, -0.0029030857731898625...   None  \n",
      "3  [0.003570088545481364, 0.0054793418943881985, ...   None  \n",
      "4  [-0.0414570323874553, 0.009572240822017193, -0...   None  \n",
      "\n",
      "[[-0.00111523  0.00317241 -0.00402532 ...  0.00214893 -0.00391827\n",
      "  -0.00010981]\n",
      " [ 0.00234079  0.00439546  0.00214962 ...  0.00251769 -0.00260394\n",
      "   0.00259491]\n",
      " [-0.00220643 -0.00290309 -0.00965031 ... -0.00460167  0.00182482\n",
      "   0.00622651]\n",
      " [ 0.00357009  0.00547934 -0.01216418 ... -0.00083535 -0.00462114\n",
      "  -0.00074339]\n",
      " [-0.04145703  0.00957224 -0.01037137 ...  0.01675679 -0.01112593\n",
      "  -0.00126493]]\n"
     ]
    }
   ],
   "source": [
    "import DataPreparator\n",
    "print(corpus)\n",
    "dataPreparator = DataPreparator.DataPreparator(min_doc_len=2)\n",
    "self = dataPreparator\n",
    "\n",
    "X = self.fit_transform(corpus)\n",
    "print(X.shape)\n",
    "print(\"\")\n",
    "print(self.df_data)\n",
    "print(\"\")\n",
    "print(self.X)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "CORPUS WITH TF-IDF\n",
    "-----------------\n",
    "[[-0.00111523  0.00317241 -0.00402532 ...  0.00214893 -0.00391827\n",
    "  -0.00010981]\n",
    " [ 0.00234079  0.00439546  0.00214962 ...  0.00251769 -0.00260394\n",
    "   0.00259491]\n",
    " [-0.00220643 -0.00290309 -0.00965031 ... -0.00460167  0.00182482\n",
    "   0.00622651]\n",
    " [ 0.00357009  0.00547934 -0.01216418 ... -0.00083535 -0.00462114\n",
    "  -0.00074339]\n",
    " [-0.04145703  0.00957224 -0.01037137 ...  0.01675679 -0.01112593\n",
    "  -0.00126493]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    50 most frequents words not to be lematized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 868,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute from corpus most frequent terms\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def list_word_most_frequent(corpus, nb_most_frequent=100, is_verbose=False) :\n",
    "    vectorizer = CountVectorizer()\n",
    "    vectorized_corpus = vectorizer.fit_transform(corpus)\n",
    "    df_wordCounting = p9_util.get_df_wordCounting_from_vectorizer(vectorized_corpus, vectorizer)\n",
    "    df_wordCounting = p9_util.df_wordCounting_free_stopword(df_wordCounting, is_verbose = is_verbose)\n",
    "\n",
    "\n",
    "\n",
    "    ser_item_name = df_wordCounting.word\n",
    "    ser_item_count = df_wordCounting.counting\n",
    "    df_item_dict={item:count for item, count in zip(ser_item_name, ser_item_count)}\n",
    "\n",
    "    list_item_sorted = sorted(df_item_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    dict_item_sorted = dict()\n",
    "    for tuple_value in list_item_sorted :\n",
    "        dict_item_sorted[tuple_value[0]] = tuple_value[1]\n",
    "    len(dict_item_sorted)\n",
    "\n",
    "    list_frequent_word = list()\n",
    "    index = 0\n",
    "    for word in dict_item_sorted.keys() :\n",
    "        if index < nb_most_frequent : \n",
    "            list_frequent_word.append(word)\n",
    "        else :\n",
    "            break\n",
    "        index += 1\n",
    "\n",
    "    return list_frequent_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello',\n",
       " 'time',\n",
       " 'world',\n",
       " 'grey',\n",
       " 'big',\n",
       " '20',\n",
       " 'man',\n",
       " 'macbeth',\n",
       " 'like',\n",
       " 'life',\n",
       " 'korea',\n",
       " 'kops',\n",
       " 'named',\n",
       " 'keystone',\n",
       " 'inert',\n",
       " 'idiot',\n",
       " 'home',\n",
       " 'told',\n",
       " 'john',\n",
       " 'north',\n",
       " 'tale',\n",
       " 'sound',\n",
       " 'signifying',\n",
       " 'sick',\n",
       " 'sharks',\n",
       " 'santiago',\n",
       " 'russia',\n",
       " 'protecting',\n",
       " 'pleasant',\n",
       " 'nt',\n",
       " 'great',\n",
       " 'war',\n",
       " 'wasn',\n",
       " 'better',\n",
       " 'ate',\n",
       " 'area',\n",
       " 'trumpets',\n",
       " 'america',\n",
       " 'ah',\n",
       " 'actually',\n",
       " 'way',\n",
       " 'china',\n",
       " 'unlike',\n",
       " 'circled',\n",
       " 'got',\n",
       " 'gaomlebdo',\n",
       " 'fury',\n",
       " 'fortunately',\n",
       " 'fish',\n",
       " 'chocolate',\n",
       " 'existential',\n",
       " 'dogs',\n",
       " 'described',\n",
       " 'dady',\n",
       " 'crew',\n",
       " 'civil']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import p9_util\n",
    "\n",
    "list_word_most_frequent = p9_util.list_word_most_frequent(corpus, nb_most_frequent=100)\n",
    "list_word_most_frequent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello, Ah! gaomlebdo...\n",
      "Hello world! Hello trumpets! The world is grey...\n",
      "hello, The 20, big grey dogs all named John don't ate all of the chocolate, but fortunately he wasn't sick!\n",
      "hello, dady's home is a big area that is'nt pleasant.\n",
      "hello america, like santiago 's great fish, is now inert and being circled by sharks : russia, china, north korea.   this time , unlike any time since the civil war , it 's existential .   and who do we have protecting our way of life ?   i 'd say the keystone kops , but they actually always   got their man .   this crew is better described by macbeth :    . . . a tale , told by an idiot , full of sound and fury , and signifying nothing . \n"
     ]
    }
   ],
   "source": [
    "for text in corpus :\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                               [hello, ah, gaomlebdo]\n",
       "1          [hello, world, hello, trumpet, world, grey]\n",
       "2    [hello, big, grey, dog, name, ate, chocolate, ...\n",
       "3      [hello, dady, home, big, area, is'nt, pleasant]\n",
       "4    [hello, like, great, fish, inert, circle, shar...\n",
       "dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import p9_util_spacy\n",
    "p9_util_spacy.spacy_clean(corpus,1,15,list_word_most_frequent=list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                               [hello, ah, gaomlebdo]\n",
       "1         [hello, world, hello, trumpets, world, grey]\n",
       "2    [hello, big, grey, dogs, named, ate, chocolate...\n",
       "3      [hello, dady, home, big, area, is'nt, pleasant]\n",
       "4    [hello, like, great, fish, inert, circled, sha...\n",
       "dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import p9_util_spacy\n",
    "p9_util_spacy.spacy_clean(corpus,1,15,list_word_most_frequent=list_word_most_frequent)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "No Lemmatization word trumpets, 2nd row,  keep same\n",
    "---------------------------------------------------\n",
    "56\n",
    "0                               [hello, ah, gaomlebdo]\n",
    "1         [hello, world, hello, trumpets, world, grey]\n",
    "2    [hello, big, grey, dogs, named, ate, chocolate...\n",
    "3      [hello, dady, home, big, area, is'nt, pleasant]\n",
    "4    [hello, like, great, fish, inert, circled, sha...\n",
    "\n",
    "Lemmatization : word trumpets, 2nd row, is converted in trumpet\n",
    "---------------------------------------------------------------\n",
    "0\n",
    "0                               [hello, ah, gaomlebdo]\n",
    "1          [hello, world, hello, trumpet, world, grey]\n",
    "2    [hello, big, grey, dog, name, ate, chocolate, ...\n",
    "3      [hello, dady, home, big, area, is'nt, pleasant]\n",
    "4    [hello, like, great, fish, inert, circle, shar...\n",
    "dtype: object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 929,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello, Ah! gaomlebdo...', 'Hello world! Hello trumpets! The world is grey...', \"hello, The 20, big grey dogs all named John don't ate all of the chocolate, but fortunately he wasn't sick!\", \"hello, dady's home is a big area that is'nt pleasant.\", \"hello america, like santiago 's great fish, is now inert and being circled by sharks : russia, china, north korea.   this time , unlike any time since the civil war , it 's existential .   and who do we have protecting our way of life ?   i 'd say the keystone kops , but they actually always   got their man .   this crew is better described by macbeth :    . . . a tale , told by an idiot , full of sound and fury , and signifying nothing . \"]\n",
      "55\n",
      "3\n",
      "4\n",
      "11\n",
      "7\n",
      "36\n",
      "(5, 300)\n",
      "\n",
      "                                                docs  \\\n",
      "0                           hello , Ah ! unknown ...   \n",
      "1  Hello world ! Hello trumpets ! The world is gr...   \n",
      "2  hello , The 20 , big grey dogs all named John ...   \n",
      "3  hello , dady 's home is a big area that is'nt ...   \n",
      "4  hello america , like santiago 's great fish , ...   \n",
      "\n",
      "                                              tokens  counting  \\\n",
      "0                               [hello, ah, unknown]         3   \n",
      "1       [hello, world, hello, trumpets, world, grey]         6   \n",
      "2  [hello, big, grey, dogs, named, ate, chocolate...         9   \n",
      "3    [hello, dady, home, big, area, is'nt, pleasant]         7   \n",
      "4  [hello, like, great, fish, inert, circled, sha...        35   \n",
      "\n",
      "                                              vector target  \n",
      "0  [-0.001115233302116394, 0.003172410577535629, ...   None  \n",
      "1  [0.0028404417634010316, 0.004527391617496808, ...   None  \n",
      "2  [-0.0015704785784085593, -0.003943675880630811...   None  \n",
      "3  [0.003570088545481364, 0.0054793418943881985, ...   None  \n",
      "4  [-0.026643124396602314, 0.0012822251891096432,...   None  \n",
      "\n",
      "[[-1.11523330e-03  3.17241058e-03 -4.02532473e-03 ...  2.14892822e-03\n",
      "  -3.91826505e-03 -1.09807054e-04]\n",
      " [ 2.84044176e-03  4.52739162e-03 -1.38072992e-03 ...  5.59997385e-03\n",
      "  -2.90898862e-03  1.55821764e-03]\n",
      " [-1.57047858e-03 -3.94367588e-03 -6.88491004e-03 ... -6.17553101e-03\n",
      "   4.45910196e-04  1.05588149e-03]\n",
      " [ 3.57008855e-03  5.47934189e-03 -1.21641764e-02 ... -8.35349162e-04\n",
      "  -4.62114010e-03 -7.43390943e-04]\n",
      " [-2.66431244e-02  1.28222519e-03  5.78991603e-05 ...  2.07061447e-02\n",
      "  -1.96218033e-02 -4.28440660e-03]]\n"
     ]
    }
   ],
   "source": [
    "import DataPreparator\n",
    "print(corpus)\n",
    "dataPreparator = DataPreparator.DataPreparator(min_doc_len=2,nb_word_most_frequent=2)\n",
    "self = dataPreparator\n",
    "\n",
    "X = self.fit_transform(corpus)\n",
    "print(X.shape)\n",
    "print(\"\")\n",
    "print(self.df_data)\n",
    "print(\"\")\n",
    "print(self.X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. DataPreparator_v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Get features from DataPreparator and in addition allows to build a padded matrix from list of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 bangui bangui 133M août  19 20:44 ./data/DataPreparator_train.dump\r\n",
      "-rw-r--r-- 1 bangui bangui 134M août  19 20:44 ./data/DataPreparator_test.dump\r\n",
      "-rw-r--r-- 1 bangui bangui 137M août  23 00:43 ./data/DataPreparator_train_spacy_20000.dump\r\n",
      "-rw-r--r-- 1 bangui bangui 137M août  23 01:35 ./data/DataPreparator_test_spacy_20000.dump\r\n",
      "-rw-r--r-- 1 bangui bangui 339M août  26 22:57 ./data/DataPreparator_train_spacy_15000.dump\r\n",
      "-rw-r--r-- 1 bangui bangui 339M août  26 23:32 ./data/DataPreparator_test_spacy_15000.dump\r\n",
      "-rw-r--r-- 1 bangui bangui 1,5G août  30 17:08 ./data/DataPreparator_valid_v2_spacy_5000.dump\r\n",
      "-rw-r--r-- 1 bangui bangui 335M sept.  1 22:28 ./data/DataPreparator_valid_v2_spacy_15000.dump\r\n"
     ]
    }
   ],
   "source": [
    "! ls -alrth ./data/DataPreparator_*.dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sample = 15000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p5_util.object_load : fileName= ./data/DataPreparator_train_spacy_15000.dump\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/bangui/.local/lib/python3.6/site-packages/matplotlib/__init__.py:886: MatplotlibDeprecationWarning: \n",
      "examples.directory is deprecated; in the future, examples will be found relative to the 'datapath' directory.\n",
      "  \"found relative to the 'datapath' directory.\".format(key))\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0903 18:18:09.356244 140421264844608 lazy_loader.py:50] \n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "W0903 18:18:09.357200 140421264844608 deprecation_wrapper.py:119] From /home/bangui/anaconda3/envs/python36/lib/python3.6/site-packages/adanet/tf_compat/__init__.py:96: The name tf.losses.Reduction is deprecated. Please use tf.compat.v1.losses.Reduction instead.\n",
      "\n",
      "W0903 18:18:09.393854 140421264844608 deprecation_wrapper.py:119] From /home/bangui/Dropbox/Perso/Formation/openclassrooms/OC_Datascientist/Kaggle/p8_util_config.py:137: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataPretarator train lenth= 13623\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docs</th>\n",
       "      <th>tokens</th>\n",
       "      <th>counting</th>\n",
       "      <th>vector</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4347</th>\n",
       "      <td>they do n't have time to write stories mostly ...</td>\n",
       "      <td>[time, write, story, post, story, source, cut,...</td>\n",
       "      <td>8</td>\n",
       "      <td>[-0.02369131033619245, 0.027803367575009664, -...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   docs  \\\n",
       "4347  they do n't have time to write stories mostly ...   \n",
       "\n",
       "                                                 tokens  counting  \\\n",
       "4347  [time, write, story, post, story, source, cut,...         8   \n",
       "\n",
       "                                                 vector  target  \n",
       "4347  [-0.02369131033619245, 0.027803367575009664, -...     0.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import p5_util\n",
    "\n",
    "filename_train = './data/DataPreparator_train_spacy_'+str(n_sample)+'.dump'\n",
    "\n",
    "dataPreparator_train = p5_util.object_load(filename_train)\n",
    "    \n",
    "print(\"\")\n",
    "print(\"DataPretarator train lenth= {}\".format(len(dataPreparator_train)))\n",
    "\n",
    "dataPreparator_train.df_data.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    }
   ],
   "source": [
    "list_doc_sample = dataPreparator_train.df_data['docs'].sample(5000).tolist()\n",
    "print(len(list_doc_sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation with `DataPreparator_v2` sample from `DataPreparator`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slice_length : 10000\n",
      "min_token_len : 2\n",
      "max_token_len : 15\n",
      "min_doc_len : 5\n",
      "max_doc_len : 500\n",
      "max_length : 80\n",
      "_is_keras_vectorizer : False\n",
      "_is_spacy_vectorizer : True\n",
      "_is_matrix_2D : True\n",
      "_is_spacy_data_preparation : True\n",
      "_oov_keyword : unknown\n",
      "_entity_keyword : None\n",
      "nb_word_most_frequent : 100\n",
      "is_tfidf : False\n",
      "xpca : None\n",
      "is_df_copied : False\n",
      "fit : list_word_most_frequent length= 100\n",
      "Index(['docs', 'tokens', 'counting', 'vector', 'target'], dtype='object')\n",
      "\n",
      "build_padded_matrix : Tokens to tensor transformation...\n",
      "\n",
      "build_padded_matrix : Padd / truncate transformation...\n",
      "Index(['docs', 'tokens', 'counting', 'vector', 'target', 'matrix_padded'], dtype='object')\n",
      "Epoch= 1277\n",
      "Array PCA shape = (383120, 300)\n",
      "Transform : 1277/1277\r"
     ]
    }
   ],
   "source": [
    "import DataPreparator_v2\n",
    "\n",
    "dataPreparator_v2 = DataPreparator_v2.DataPreparator_v2(max_padding_length=80)\n",
    "\n",
    "#print(dataPreparator_v2.get_params())\n",
    "for key, value in dataPreparator_v2.get_params().items() :\n",
    "    print(\"{} : {}\".format(key, value))\n",
    "\n",
    "X = dataPreparator_v2.fit_transform(list_doc_sample)\n",
    "\n",
    "print(dataPreparator_v2.df_data.columns)\n",
    "\n",
    "dataPreparator_v2.build_padded_matrix()\n",
    "print(dataPreparator_v2.df_data.columns)\n",
    "\n",
    "batch_size=100\n",
    "percent_var = 0.8\n",
    "#dataPreparator_v2.xpca = None\n",
    "build_matrix_padded_truncated(dataPreparator_v2, batch_size, percent_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "p3_util_plot.plot_pca_variance(dataPreparator_v2.xpca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
