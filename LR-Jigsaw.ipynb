{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler,MaxAbsScaler\n",
    "from sklearn.metrics import mean_squared_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import p5_util\n",
    "def tfidf_vectorizer(X_, vectorizer=None, scaler=None):\n",
    "    if vectorizer is None :\n",
    "        # When building vacabulary, terms with frequency document < p_min_df are ignored.\n",
    "        p_min_df = 0.001\n",
    "\n",
    "        # When building vacabulary, terms with frequency document> p_max_df are ignored.\n",
    "        p_max_df = 1.\n",
    "\n",
    "        print(\"MIN DF= \"+str(p_min_df)+\"  MAX DF = \"+str(p_max_df))\n",
    "        ngram1=1\n",
    "        ngram2=1\n",
    "        vectorizer=TfidfVectorizer(norm=\"l2\", use_idf=True, min_df=p_min_df, max_df=p_max_df, ngram_range=(ngram1, ngram2))\n",
    "    if scaler is None :\n",
    "        scaler = MaxAbsScaler()\n",
    "\n",
    "    csr_matrix_tfidf_ngram1 = vectorizer.fit_transform(X_)\n",
    "\n",
    "    print(csr_matrix_tfidf_ngram1.shape)\n",
    "    \n",
    "    X_scaled = scaler.fit_transform(csr_matrix_tfidf_ngram1)\n",
    "    return X_scaled, vectorizer, scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read data from files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_test = pd.read_csv('./data/test.csv.zip', compression='zip', header=0, sep=',', quotechar='\"')\n",
    "df_train = pd.read_csv('./data/train.csv.zip', compression='zip', header=0, sep=',', quotechar='\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replace Nan values depending column type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replace all nan values with 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "for col in df_train.columns:\n",
    "    nan_rows = df_train[df_train[col].isnull()][col]\n",
    "    if nan_rows.shape[0] > 0 :\n",
    "        print(col)\n",
    "        df_train[col] = df_train[col].replace('nan', np.nan).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test =train_test_split(df_train['comment_text'],df_train['target'],test_size=0.33, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=blue> Standardization</font>\n",
    "\n",
    "Standardization is applied in a such way: \n",
    "* Verbs from sentences are removed\n",
    "* Stop words are removed\n",
    "* Lemmatization is applied\n",
    "* English stemming is applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import p5_util\n",
    "import p6_util\n",
    "import p6_util_plot\n",
    "\n",
    "file_name='./data/ser_train_std.dump'\n",
    "\n",
    "if False :\n",
    "    ser_train_std = p6_util.p6_df_standardization(X_train\\\n",
    "                                                  , is_sentence_filter=False\\\n",
    "                                                  , is_stemming=True\\\n",
    "                                                  , is_lem=True\n",
    "                                                  , list_to_keep=list()\n",
    "                                                  , is_lxml = False)\n",
    "    p5_util.object_dump(ser_train_std, file_name)\n",
    "else :\n",
    "    ser_train_std = p5_util.object_load(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser_train_std.head()\n",
    "print(ser_train_std.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p5_util.object_load : fileName= ./data/X_train_token.dump\n",
      "p5_util.object_load : fileName= ./data/X_test_token.dump\n",
      "p5_util.object_load : fileName= ./data/y_train_token.dump\n",
      "p5_util.object_load : fileName= ./data/y_test_token.dump\n",
      "\n",
      "X_train_encoded shape = 362779\n",
      "X_test_encoded shape  = 178682\n",
      "Y train shape= 362779\n",
      "Y test shape= 178682\n"
     ]
    }
   ],
   "source": [
    "import p5_util\n",
    "extension='_full'\n",
    "extension=''\n",
    "\n",
    "filename = './data/X_train_token'+extension+'.dump'\n",
    "X_train_token = p5_util.object_load(filename)\n",
    "\n",
    "\n",
    "filename = './data/X_test_token'+extension+'.dump'\n",
    "X_test_token = p5_util.object_load(filename)\n",
    "\n",
    "filename = './data/y_train_token'+extension+'.dump'\n",
    "y_train_token = p5_util.object_load(filename)\n",
    "\n",
    "\n",
    "filename = './data/y_test_token'+extension+'.dump'\n",
    "y_test_token = p5_util.object_load(filename)\n",
    "\n",
    "print(\"\\nX_train_encoded shape = {}\".format(len(X_train_token)))\n",
    "print(\"X_test_encoded shape  = {}\".format(len(X_test_token)))\n",
    "print(\"Y train shape= {}\".format(len(y_train_token)))\n",
    "print(\"Y test shape= {}\".format(len(y_test_token)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_X_train = [\" \".join(list_token) for list_token in X_train_token]\n",
    "list_X_test = [\" \".join(list_token) for list_token in X_test_token]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=blue>Linear Regression </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=blue>Building an estimator pipeline </font>\n",
    "\n",
    "* Estimator pipeline is built with TF-IDF BOW.\n",
    "* Standard scaling is applied in order mean for features to be 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "pipeline_lr = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    #('tfidf',  TfidfTransformer()),\n",
    "    ('scale', StandardScaler(with_mean=False)),\n",
    "    ('lr', LinearRegression()),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bangui/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "model_lr = pipeline_lr.fit(np.array(list_X_train), np.array(y_train_token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bangui/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "y_predict = model_lr.predict(list_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.007290387463792958"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2_score(np.round(y_test_token,1), np.round(y_predict,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.038980647183264126"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "\n",
    "sklearn.metrics.mean_squared_error(np.round(y_test_token,1), np.round(y_predict,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__abstractmethods__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_abc_cache',\n",
       " '_abc_negative_cache',\n",
       " '_abc_negative_cache_version',\n",
       " '_abc_registry',\n",
       " '_estimator_type',\n",
       " '_final_estimator',\n",
       " '_fit',\n",
       " '_get_param_names',\n",
       " '_get_params',\n",
       " '_inverse_transform',\n",
       " '_pairwise',\n",
       " '_replace_estimator',\n",
       " '_set_params',\n",
       " '_transform',\n",
       " '_validate_names',\n",
       " '_validate_steps',\n",
       " 'classes_',\n",
       " 'decision_function',\n",
       " 'fit',\n",
       " 'fit_predict',\n",
       " 'fit_transform',\n",
       " 'get_params',\n",
       " 'inverse_transform',\n",
       " 'memory',\n",
       " 'named_steps',\n",
       " 'predict',\n",
       " 'predict_log_proba',\n",
       " 'predict_proba',\n",
       " 'score',\n",
       " 'set_params',\n",
       " 'steps',\n",
       " 'transform']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(pipeline_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128879,)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_lr.named_steps['lr'].coef_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.1  0.   0.  -0.   0.1  0.2  0.1  0.5  0.   0.1]\n",
      "[0.  0.  0.  0.  0.  0.  0.  0.2 0.2 0. ]\n"
     ]
    }
   ],
   "source": [
    "print(np.round(y_predict[100:110],1))\n",
    "print(np.round(y_test_token[100:110],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_predict))\n",
    "print(\"RMSE for Linear Regression: %.4f\" % rmse)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# No standardization\n",
    "RMSE for Linear Regression: 0.1477"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "pipeline_gbr = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf',  TfidfTransformer()),\n",
    "    ('scale', preprocessing.StandardScaler(with_mean=False)),\n",
    "    ('gbr', GradientBoostingRegressor()),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipeline_gbr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "y_predict = model.predict(X_test)\n",
    "r2_score(y_test_token, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mse = mean_squared_error(y_test, y_predict)\n",
    "print(\"MSE: %.4f\" % mse)\n",
    "np.sqrt(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled.min(),X_scaled.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbr = GradientBoostingRegressor()\n",
    "X_scaled, scaler = tfidf_vectorizer(X_train)\n",
    "model = gbr.fit(X_scaled,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "X_test_scaled = tfidf_vectorizer(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = model.predict(X_test_scaled)\n",
    "mse = mean_squared_error(y_test, y_predict)\n",
    "print(\"MSE: %.4f\" % mse)\n",
    "np.sqrt(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import xgboost as xgb\n",
    "\n",
    "pipeline_xgbr = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf',  TfidfTransformer()),\n",
    "    ('scale', preprocessing.StandardScaler(with_mean=False)),\n",
    "    ('xgbr', xgb.XGBRegressor(objective ='reg:squarederror', colsample_bytree = 0.3, learning_rate = 0.1,\n",
    "                max_depth = 5, alpha = 10, n_estimators = 100)),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_xgbr = pipeline_xgbr.fit(X_train, y_train)\n",
    "\n",
    "y_preds = model_xgbr.predict(X_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_preds))\n",
    "print(\"RMSE: %f\" % (rmse))\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#n_estimators = 10:\n",
    "RMSE: 0.230416\n",
    "\n",
    "#n_estimators = 100:\n",
    "RMSE: 0.156580"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBOOST with TFIDF NGRAM=(2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn import preprocessing\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "ngram1=2\n",
    "ngram2=2\n",
    "p_min_df = 0.01\n",
    "# When building vacabulary, terms with frequency document> p_max_df are ignored.\n",
    "p_max_df = 1.\n",
    "\n",
    "vectorizer = TfidfVectorizer(norm=\"l2\", use_idf=True, min_df=p_min_df, max_df=p_max_df, ngram_range=(ngram1, ngram2))\n",
    "pipeline_xgbr = Pipeline([\n",
    "    ('tfidf',  vectorizer),\n",
    "    ('scale', preprocessing.StandardScaler(with_mean=False)),\n",
    "    ('xgbr', xgb.XGBRegressor(objective ='reg:squarederror', colsample_bytree = 0.3, learning_rate = 0.1,\n",
    "                max_depth = 5, alpha = 10, n_estimators = 100)),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_xgbr = pipeline_xgbr.fit(X_train, y_train)\n",
    "\n",
    "y_preds = model_xgbr.predict(X_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_preds))\n",
    "print(\"RMSE: %f\" % (rmse))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#n_estimators=100\n",
    "#TFIDF NGRAM=(2,2) \n",
    "RMSE: 0.195239"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBOOST with Standardized Xtrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name='./data/ser_train_std.dump'\n",
    "ser_train = p5_util.object_load(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn import preprocessing\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "ngram1=1\n",
    "ngram2=1\n",
    "p_min_df = 0.\n",
    "# When building vacabulary, terms with frequency document> p_max_df are ignored.\n",
    "p_max_df = 1.\n",
    "\n",
    "vectorizer = TfidfVectorizer(norm=\"l2\", use_idf=True, min_df=p_min_df, max_df=p_max_df, ngram_range=(ngram1, ngram2))\n",
    "pipeline_xgbr = Pipeline([\n",
    "    ('tfidf',  vectorizer),\n",
    "    ('scale', preprocessing.StandardScaler(with_mean=False)),\n",
    "    ('xgbr', xgb.XGBRegressor(objective ='reg:squarederror', colsample_bytree = 0.3, learning_rate = 0.01,\n",
    "                max_depth = 5, alpha = 10, n_estimators = 100, nthread=-1)),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_xgbr=pipeline_xgbr.fit(ser_train.values, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = model_xgbr.predict(X_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_preds))\n",
    "print(\"RMSE: %f\" % (rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
