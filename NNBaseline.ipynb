{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import gensim\n",
    "\n",
    "\n",
    "import p8_util\n",
    "import p8_util_config\n",
    "import p9_util\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = './tmp/baseline'\n",
    "OUTPUT_DIR_TB = './tmp'\n",
    "is_tensorboard = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensorboard processes are killed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kill: (28097): Aucun processus de ce type\n",
      "kill: (28103)"
     ]
    }
   ],
   "source": [
    "# this will kill the processes for Tensorboard\n",
    "#is_tensorboard = True\n",
    "if is_tensorboard is True :\n",
    "    !ps aux | grep tensorboard | awk '{print $2}' | xargs kill\n",
    "# this will kill the processes for ngrok\n",
    "if is_tensorboard is True :\n",
    "    !ps aux | grep ngrok | awk '{print $2}' | xargs kill    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import p5_util\n",
    "\n",
    "filename = './data/X_train_encoded.dump'\n",
    "x_train = p5_util.object_load(filename)\n",
    "\n",
    "filename = './data/X_test_encoded.dump'\n",
    "x_test = p5_util.object_load(filename)\n",
    "\n",
    "filename = './data/y_test.dump'\n",
    "y_test = p5_util.object_load(filename)\n",
    "\n",
    "filename = './data/y_train.dump'\n",
    "y_train = p5_util.object_load(filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "print(x_train.shape, np.array(y_train).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    From now, x_train.shape[1] is used to configure `p8_util_config.ADANET_FEATURE_SHAPE`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building AdaNet builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import p8_util\n",
    "print(\"Input output dir= {}\".format(OUTPUT_DIR))\n",
    "\n",
    "oNNAdaNetBuilder = p8_util.create_nn_builder(OUTPUT_DIR)\n",
    "oNNAdaNetBuilder.show()\n",
    "params = {'net_builder': oNNAdaNetBuilder}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a Baseline Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Number of convolutional layers= 1\n",
      "\n",
      "*** NNAdaNetBuilder : NN Type=RNN\n",
      "\n",
      "*** get_tf_head() : feature shape= (1, 37)\n",
      "\n",
      "*** get_tf_head() : feature columns= [NumericColumn(key='texts', shape=(1, 37), default_value=None, dtype=tf.float32, normalizer_fn=None)]\n",
      "\n",
      "*** make_config() : output dir= ./tmp/baseline/RNN/SGRU\n",
      "\n",
      "Max steps= 10 / Number of EPOCH=3\n",
      "\n",
      "\n",
      "Global steps  : ................................... 10\n",
      "NN type              : ............................ RNN\n",
      "Features shape       : ............................ (1, 37)\n",
      "Adanet outputdir     : ............................ ./tmp/baseline/RNN\n",
      "Adanet output log    : ............................ ./tmp/baseline/RNN/SGRU\n",
      "Adanet boosting iter.: ............................ 40\n",
      "Adanet iter per boost: ............................ 0\n",
      "Number of layers     : ............................ 1\n",
      "Dropout rate         : ............................ 0.0\n",
      "Seed value           : ............................ 42\n",
      "Nb of classes (logit): ............................ 1\n",
      "Adanet regularization: ............................ 1e-05\n",
      "Weights initializer  : ............................ truncated_normal\n",
      "Batch normalization  : ............................ True\n",
      "Learn mixture weights: ............................ True\n",
      "\n",
      "\n",
      "Cell type            : ............................ SGRU\n",
      "Hidden units         : ............................ 128\n",
      "Stacked cells        : ............................ 1\n",
      "Time steps           : ............................ 37\n",
      "\n",
      "\n",
      "\n",
      "INFO:tensorflow:Using config: {'_model_dir': './tmp/baseline/RNN/SGRU', '_tf_random_seed': 42, '_save_summary_steps': 10, '_save_checkpoints_steps': 10, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f0680152be0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "I0702 09:48:17.323605 139667762362176 estimator.py:201] Using config: {'_model_dir': './tmp/baseline/RNN/SGRU', '_tf_random_seed': 42, '_save_summary_steps': 10, '_save_checkpoints_steps': 10, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f0680152be0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "import BaselineEstimator\n",
    "\n",
    "oNNAdaNetBuilder = p8_util.create_nn_builder(OUTPUT_DIR)\n",
    "oNNAdaNetBuilder.show()\n",
    "params = {'net_builder': oNNAdaNetBuilder}\n",
    "#oNNAdaNetBuilder = None\n",
    "oBaselineEstimator = BaselineEstimator.BaselineEstimator( params, IS_DEBUG=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p5_util.object_load : fileName= ./data/keras_tokenizer.dump\n",
      "Vocabulary size= 129260\n"
     ]
    }
   ],
   "source": [
    "import p5_util\n",
    "\n",
    "filename = './data/keras_tokenizer.dump'\n",
    "keras_tokenizer = p5_util.object_load(filename)\n",
    "\n",
    "vocab_size = len(keras_tokenizer.word_index) + 1\n",
    "\n",
    "print(\"Vocabulary size= {}\".format(vocab_size))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import tensorflow as tf\n",
    " \n",
    "num_hidden_units = 128\n",
    "rnn_cell = tf.keras.layers.GRUCell(num_hidden_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bangui/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:9: UserWarning: The `dropout` argument is no longer support in `Embedding`. You can apply a `keras.layers.SpatialDropout1D` layer right after the `Embedding` layer to get the same behavior.\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, 37, 128)           16545280  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 16,676,993\n",
      "Trainable params: 16,676,993\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "embed_dim = 128\n",
    "lstm_out = 1\n",
    "batch_size = 32\n",
    "\n",
    "model =  keras.models.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size, embed_dim, input_length = 37, dropout = 0.2))\n",
    "#model.add(keras.layers.LSTM(lstm_out, dropout_U = 0.2, dropout_W = 0.2))\n",
    "model.add(keras.layers.LSTM(embed_dim))\n",
    "model.add(keras.layers.Dense(1,activation='linear'))\n",
    "model.compile(loss = 'mean_squared_error', optimizer='sgd',metrics = ['mae'])\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-76fcef651886>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'x_train' is not defined"
     ]
    }
   ],
   "source": [
    "batch_size = x_train.shape[0]//100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train, batch_size =batch_size, nb_epoch = 1,  verbose = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results, start_time, end_time  = oBaselineEstimator.train_and_evaluate()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "help(tf.metrics.mean_squared_error)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "help(tf.reduce_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(oBaselineEstimator._features))\n",
    "oBaselineEstimator._features"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<class 'dict'>\n",
    "{'images': <tf.Tensor 'IteratorGetNext:0' shape=(?, 28, 28) dtype=float32>}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"\\n------------------------------------------------\")\n",
    "print(\"Time (sec)\", end_time-start_time)\n",
    "for key in results.keys() :\n",
    "    title =key.upper()\n",
    "    print(\"{}: {}\".format(title, results[key]))\n",
    "print(\"------------------------------------------------\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir_log = './tmp/baseline/RNN/RNN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oNNAdaNetBuilder.output_dir_log, is_tensorboard\n",
    "output_dir_log = oNNAdaNetBuilder.output_dir_log\n",
    "output_dir_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_tensorboard is True :\n",
    "    get_ipython().system_raw(\n",
    "        'tensorboard --logdir {} --host localhost --port 6007 &'\n",
    "        .format(output_dir_log)\n",
    "    )\n",
    "\n",
    "\n",
    "    get_ipython().system_raw('./assets/ngrok http 6007 &')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_tensorboard,output_dir_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
