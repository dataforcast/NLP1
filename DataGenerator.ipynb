{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, Data Generator are built for train and validation dataset, in order to feed CNN Keras models.\n",
    "\n",
    "Such objects depends on DataPreparator_v2 object, that has been previously built."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=blus>Software engineering</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/SoftwareEngineeringDataGenerator.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>CNN Data generator</b>\n",
       "<div id=\"my_id_menu_nb\">run previous cell, wait for 2 seconds</div>\n",
       "<script>\n",
       "function repeat_indent_string(n){\n",
       "    var a = \"\" ;\n",
       "    for ( ; n > 0 ; --n)\n",
       "        a += \"    \";\n",
       "    return a;\n",
       "}\n",
       "// look up into all sections and builds an automated menu //\n",
       "var update_menu_string = function(begin, lfirst, llast, sformat, send, keep_item, begin_format, end_format) {\n",
       "    var anchors = document.getElementsByClassName(\"section\");\n",
       "    if (anchors.length == 0) {\n",
       "        anchors = document.getElementsByClassName(\"text_cell_render rendered_html\");\n",
       "    }\n",
       "    var i,t;\n",
       "    var text_menu = begin;\n",
       "    var text_memo = \"<pre>\\nlength:\" + anchors.length + \"\\n\";\n",
       "    var ind = \"\";\n",
       "    var memo_level = 1;\n",
       "    var href;\n",
       "    var tags = [];\n",
       "    var main_item = 0;\n",
       "    var format_open = 0;\n",
       "    for (i = 0; i <= llast; i++)\n",
       "        tags.push(\"h\" + i);\n",
       "\n",
       "    for (i = 0; i < anchors.length; i++) {\n",
       "        text_memo += \"**\" + anchors[i].id + \"--\\n\";\n",
       "\n",
       "        var child = null;\n",
       "        for(t = 0; t < tags.length; t++) {\n",
       "            var r = anchors[i].getElementsByTagName(tags[t]);\n",
       "            if (r.length > 0) {\n",
       "child = r[0];\n",
       "break;\n",
       "            }\n",
       "        }\n",
       "        if (child == null) {\n",
       "            text_memo += \"null\\n\";\n",
       "            continue;\n",
       "        }\n",
       "        if (anchors[i].hasAttribute(\"id\")) {\n",
       "            // when converted in RST\n",
       "            href = anchors[i].id;\n",
       "            text_memo += \"#1-\" + href;\n",
       "            // passer à child suivant (le chercher)\n",
       "        }\n",
       "        else if (child.hasAttribute(\"id\")) {\n",
       "            // in a notebook\n",
       "            href = child.id;\n",
       "            text_memo += \"#2-\" + href;\n",
       "        }\n",
       "        else {\n",
       "            text_memo += \"#3-\" + \"*\" + \"\\n\";\n",
       "            continue;\n",
       "        }\n",
       "        var title = child.textContent;\n",
       "        var level = parseInt(child.tagName.substring(1,2));\n",
       "\n",
       "        text_memo += \"--\" + level + \"?\" + lfirst + \"--\" + title + \"\\n\";\n",
       "\n",
       "        if ((level < lfirst) || (level > llast)) {\n",
       "            continue ;\n",
       "        }\n",
       "        if (title.endsWith('¶')) {\n",
       "            title = title.substring(0,title.length-1).replace(\"<\", \"&lt;\")\n",
       "         .replace(\">\", \"&gt;\").replace(\"&\", \"&amp;\");\n",
       "        }\n",
       "        if (title.length == 0) {\n",
       "            continue;\n",
       "        }\n",
       "\n",
       "        while (level < memo_level) {\n",
       "            text_menu += end_format + \"</ul>\\n\";\n",
       "            format_open -= 1;\n",
       "            memo_level -= 1;\n",
       "        }\n",
       "        if (level == lfirst) {\n",
       "            main_item += 1;\n",
       "        }\n",
       "        if (keep_item != -1 && main_item != keep_item + 1) {\n",
       "            // alert(main_item + \" - \" + level + \" - \" + keep_item);\n",
       "            continue;\n",
       "        }\n",
       "        while (level > memo_level) {\n",
       "            text_menu += \"<ul>\\n\";\n",
       "            memo_level += 1;\n",
       "        }\n",
       "        text_menu += repeat_indent_string(level-2);\n",
       "        text_menu += begin_format + sformat.replace(\"__HREF__\", href).replace(\"__TITLE__\", title);\n",
       "        format_open += 1;\n",
       "    }\n",
       "    while (1 < memo_level) {\n",
       "        text_menu += end_format + \"</ul>\\n\";\n",
       "        memo_level -= 1;\n",
       "        format_open -= 1;\n",
       "    }\n",
       "    text_menu += send;\n",
       "    //text_menu += \"\\n\" + text_memo;\n",
       "\n",
       "    while (format_open > 0) {\n",
       "        text_menu += end_format;\n",
       "        format_open -= 1;\n",
       "    }\n",
       "    return text_menu;\n",
       "};\n",
       "var update_menu = function() {\n",
       "    var sbegin = \"\";\n",
       "    var sformat = '<a href=\"#__HREF__\">__TITLE__</a>';\n",
       "    var send = \"\";\n",
       "    var begin_format = '<li>';\n",
       "    var end_format = '</li>';\n",
       "    var keep_item = -1;\n",
       "    var text_menu = update_menu_string(sbegin, 1, 4, sformat, send, keep_item,\n",
       "       begin_format, end_format);\n",
       "    var menu = document.getElementById(\"my_id_menu_nb\");\n",
       "    menu.innerHTML=text_menu;\n",
       "};\n",
       "window.setTimeout(update_menu,2000);\n",
       "            </script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from jyquickhelper import add_notebook_menu\n",
    "add_notebook_menu(first_level=1, last_level=4, header=\"CNN Data generator\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=blus>0. Notebook configuration</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Display an item from tokenized corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=blus>1. Loading dataset</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=blus> 1.1. Load dataset from manual data-preparation</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    This dataframe is used in ordre to complete handed data-preparation with target column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('./data/train.csv.zip', compression='zip', header=0, sep=',', quotechar='\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    List of handed data-preparation files is built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p5_util.object_load : fileName= ./data/dict_param_preparator.dill\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'binary_threshold': 0.5,\n",
       " 'n_sample_per_class': 71019,\n",
       " 'max_row': 100,\n",
       " 'nb_step': 141,\n",
       " 'bulk_row': 1000,\n",
       " 'remain_step': 651,\n",
       " 'embedding': 300}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import p5_util\n",
    "filename = './data/dict_param_preparator.dill'\n",
    "dict_param_preparator = p5_util.object_load(filename, is_verbose=True)\n",
    "dict_param_preparator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=blus> 1.1.1 Update target vectors into binaries labels vectors</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    This operation is required to build DataGenerators objects in which, target vectors \n",
    "    will be target binaries vectors.\n",
    "    \n",
    "    In the sequence below, DataFrame files issued from manual data-preparation are loaded.\n",
    "    \n",
    "    Such files contains target vectors as well as digitalized texts.\n",
    "    \n",
    "    Target vectors are converted into binary labeled vectors with a threshold fixed into \n",
    "    configuration parameters.\n",
    "    \n",
    "    Files with binary target vectors are dumped over harddisk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Range processed : 0 --> 1000 Elapsed time : 0.000600\r",
      "Range processed : 1000 --> 2000 Elapsed time : 0.000998\r",
      "Range processed : 2000 --> 3000 Elapsed time : 0.001023\r",
      "Range processed : 3000 --> 4000 Elapsed time : 0.001042\r",
      "Range processed : 4000 --> 5000 Elapsed time : 0.001059\r",
      "Range processed : 5000 --> 6000 Elapsed time : 0.001075\r",
      "Range processed : 6000 --> 7000 Elapsed time : 0.001574\r",
      "Range processed : 7000 --> 8000 Elapsed time : 0.001602\r",
      "Range processed : 8000 --> 9000 Elapsed time : 0.001619\r",
      "Range processed : 9000 --> 10000 Elapsed time : 0.001635\r",
      "Range processed : 10000 --> 11000 Elapsed time : 0.001650\r",
      "Range processed : 11000 --> 12000 Elapsed time : 0.001917\r",
      "Range processed : 12000 --> 13000 Elapsed time : 0.001935\r",
      "Range processed : 13000 --> 14000 Elapsed time : 0.001951\r",
      "Range processed : 14000 --> 15000 Elapsed time : 0.001966\r",
      "Range processed : 15000 --> 16000 Elapsed time : 0.001981\r",
      "Range processed : 16000 --> 17000 Elapsed time : 0.001996\r",
      "Range processed : 17000 --> 18000 Elapsed time : 0.002010\r",
      "Range processed : 18000 --> 19000 Elapsed time : 0.002025\r",
      "Range processed : 19000 --> 20000 Elapsed time : 0.002040\r",
      "Range processed : 20000 --> 21000 Elapsed time : 0.002055\r",
      "Range processed : 21000 --> 22000 Elapsed time : 0.002070\r",
      "Range processed : 22000 --> 23000 Elapsed time : 0.002084\r",
      "Range processed : 23000 --> 24000 Elapsed time : 0.002099\r",
      "Range processed : 24000 --> 25000 Elapsed time : 0.002114\r",
      "Range processed : 25000 --> 26000 Elapsed time : 0.002130\r",
      "Range processed : 26000 --> 27000 Elapsed time : 0.002144\r",
      "Range processed : 27000 --> 28000 Elapsed time : 0.003106\r",
      "Range processed : 28000 --> 29000 Elapsed time : 0.003139\r",
      "Range processed : 29000 --> 30000 Elapsed time : 0.003159\r",
      "Range processed : 30000 --> 31000 Elapsed time : 0.003176\r",
      "Range processed : 31000 --> 32000 Elapsed time : 0.003194\r",
      "Range processed : 32000 --> 33000 Elapsed time : 0.003211\r",
      "Range processed : 33000 --> 34000 Elapsed time : 0.003229\r",
      "Range processed : 34000 --> 35000 Elapsed time : 0.003247\r",
      "Range processed : 35000 --> 36000 Elapsed time : 0.003265\r",
      "Range processed : 36000 --> 37000 Elapsed time : 0.003282\r",
      "Range processed : 37000 --> 38000 Elapsed time : 0.003299\r",
      "Range processed : 38000 --> 39000 Elapsed time : 0.003316\r",
      "Range processed : 39000 --> 40000 Elapsed time : 0.003334\r",
      "Range processed : 40000 --> 41000 Elapsed time : 0.003351\r",
      "Range processed : 41000 --> 42000 Elapsed time : 0.003369\r",
      "Range processed : 42000 --> 43000 Elapsed time : 0.003386\r",
      "Range processed : 43000 --> 44000 Elapsed time : 0.003404\r",
      "Range processed : 44000 --> 45000 Elapsed time : 0.003422\r",
      "Range processed : 45000 --> 46000 Elapsed time : 0.003441\r",
      "Range processed : 46000 --> 47000 Elapsed time : 0.003459\r",
      "Range processed : 47000 --> 48000 Elapsed time : 0.003477\r",
      "Range processed : 48000 --> 49000 Elapsed time : 0.003495\r",
      "Range processed : 49000 --> 50000 Elapsed time : 0.003513\r",
      "Range processed : 50000 --> 51000 Elapsed time : 0.003532\r",
      "Range processed : 51000 --> 52000 Elapsed time : 0.003550\r",
      "Range processed : 52000 --> 53000 Elapsed time : 0.003707\r",
      "Range processed : 53000 --> 54000 Elapsed time : 0.003727\r",
      "Range processed : 54000 --> 55000 Elapsed time : 0.003744\r",
      "Range processed : 55000 --> 56000 Elapsed time : 0.003762\r",
      "Range processed : 56000 --> 57000 Elapsed time : 0.003780\r",
      "Range processed : 57000 --> 58000 Elapsed time : 0.003797\r",
      "Range processed : 58000 --> 59000 Elapsed time : 0.003815\r",
      "Range processed : 59000 --> 60000 Elapsed time : 0.003832\r",
      "Range processed : 60000 --> 61000 Elapsed time : 0.003849\r",
      "Range processed : 61000 --> 62000 Elapsed time : 0.003883\r",
      "Range processed : 62000 --> 63000 Elapsed time : 0.003899\r",
      "Range processed : 63000 --> 64000 Elapsed time : 0.003916\r",
      "Range processed : 64000 --> 65000 Elapsed time : 0.003932\r",
      "Range processed : 65000 --> 66000 Elapsed time : 0.003949\r",
      "Range processed : 66000 --> 67000 Elapsed time : 0.003964\r",
      "Range processed : 67000 --> 68000 Elapsed time : 0.003981\r",
      "Range processed : 68000 --> 69000 Elapsed time : 0.003998\r",
      "Range processed : 69000 --> 70000 Elapsed time : 0.004014\r",
      "Range processed : 70000 --> 71000 Elapsed time : 0.004032\r",
      "Range processed : 71000 --> 72000 Elapsed time : 0.004049\r",
      "Range processed : 72000 --> 73000 Elapsed time : 0.006729\r",
      "Range processed : 73000 --> 74000 Elapsed time : 0.006757\r",
      "Range processed : 74000 --> 75000 Elapsed time : 0.006773\r",
      "Range processed : 75000 --> 76000 Elapsed time : 0.006789\r",
      "Range processed : 76000 --> 77000 Elapsed time : 0.006804\r",
      "Range processed : 77000 --> 78000 Elapsed time : 0.006819\r",
      "Range processed : 78000 --> 79000 Elapsed time : 0.006834\r",
      "Range processed : 79000 --> 80000 Elapsed time : 0.006850\r",
      "Range processed : 80000 --> 81000 Elapsed time : 0.006865\r",
      "Range processed : 81000 --> 82000 Elapsed time : 0.006880\r",
      "Range processed : 82000 --> 83000 Elapsed time : 0.006895\r",
      "Range processed : 83000 --> 84000 Elapsed time : 0.006911\r",
      "Range processed : 84000 --> 85000 Elapsed time : 0.006926\r",
      "Range processed : 85000 --> 86000 Elapsed time : 0.006941\r",
      "Range processed : 86000 --> 87000 Elapsed time : 0.006956\r",
      "Range processed : 87000 --> 88000 Elapsed time : 0.006971\r",
      "Range processed : 88000 --> 89000 Elapsed time : 0.006986\r",
      "Range processed : 89000 --> 90000 Elapsed time : 0.007001\r",
      "Range processed : 90000 --> 91000 Elapsed time : 0.007016\r",
      "Range processed : 91000 --> 92000 Elapsed time : 0.007031\r",
      "Range processed : 92000 --> 93000 Elapsed time : 0.007047\r",
      "Range processed : 93000 --> 94000 Elapsed time : 0.007061\r",
      "Range processed : 94000 --> 95000 Elapsed time : 0.007077\r",
      "Range processed : 95000 --> 96000 Elapsed time : 0.007092\r",
      "Range processed : 96000 --> 97000 Elapsed time : 0.007107\r",
      "Range processed : 97000 --> 98000 Elapsed time : 0.007122\r",
      "Range processed : 98000 --> 99000 Elapsed time : 0.007137\r",
      "Range processed : 99000 --> 100000 Elapsed time : 0.007152\r",
      "Range processed : 100000 --> 101000 Elapsed time : 0.007167\r",
      "Range processed : 101000 --> 102000 Elapsed time : 0.007183\r",
      "Range processed : 102000 --> 103000 Elapsed time : 0.007198\r",
      "Range processed : 103000 --> 104000 Elapsed time : 0.007213\r",
      "Range processed : 104000 --> 105000 Elapsed time : 0.007228\r",
      "Range processed : 105000 --> 106000 Elapsed time : 0.007244\r",
      "Range processed : 106000 --> 107000 Elapsed time : 0.007259\r",
      "Range processed : 107000 --> 108000 Elapsed time : 0.007275\r",
      "Range processed : 108000 --> 109000 Elapsed time : 0.007289\r",
      "Range processed : 109000 --> 110000 Elapsed time : 0.007304\r",
      "Range processed : 110000 --> 111000 Elapsed time : 0.007319\r",
      "Range processed : 111000 --> 112000 Elapsed time : 0.007334\r",
      "Range processed : 112000 --> 113000 Elapsed time : 0.007349\r",
      "Range processed : 113000 --> 114000 Elapsed time : 0.007363\r",
      "Range processed : 114000 --> 115000 Elapsed time : 0.007378\r",
      "Range processed : 115000 --> 116000 Elapsed time : 0.007393\r",
      "Range processed : 116000 --> 117000 Elapsed time : 0.007408\r",
      "Range processed : 117000 --> 118000 Elapsed time : 0.007423\r",
      "Range processed : 118000 --> 119000 Elapsed time : 0.007438\r",
      "Range processed : 119000 --> 120000 Elapsed time : 0.007453\r",
      "Range processed : 120000 --> 121000 Elapsed time : 0.007468\r",
      "Range processed : 121000 --> 122000 Elapsed time : 0.007482\r",
      "Range processed : 122000 --> 123000 Elapsed time : 0.007497\r",
      "Range processed : 123000 --> 124000 Elapsed time : 0.007512\r",
      "Range processed : 124000 --> 125000 Elapsed time : 0.007527\r",
      "Range processed : 125000 --> 126000 Elapsed time : 0.007542\r",
      "Range processed : 126000 --> 127000 Elapsed time : 0.007556\r",
      "Range processed : 127000 --> 128000 Elapsed time : 0.007571\r",
      "Range processed : 128000 --> 129000 Elapsed time : 0.007586\r",
      "Range processed : 129000 --> 130000 Elapsed time : 0.007601\r",
      "Range processed : 130000 --> 131000 Elapsed time : 0.007616\r",
      "Range processed : 131000 --> 132000 Elapsed time : 0.007630\r",
      "Range processed : 132000 --> 133000 Elapsed time : 0.007645\r",
      "Range processed : 133000 --> 134000 Elapsed time : 0.007660\r",
      "Range processed : 134000 --> 135000 Elapsed time : 0.007675\r",
      "Range processed : 135000 --> 136000 Elapsed time : 0.010927\r",
      "Range processed : 136000 --> 137000 Elapsed time : 0.010956\r",
      "Range processed : 137000 --> 138000 Elapsed time : 0.011143\r",
      "Range processed : 138000 --> 139000 Elapsed time : 0.011160\r",
      "Range processed : 139000 --> 140000 Elapsed time : 0.011174\r",
      "Range processed : 140000 --> 141000 Elapsed time : 0.011189\r\n",
      "Range processed : 141000 --> 141651 Elapsed time : 0.000022\r"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import p5_util\n",
    "import p9_util\n",
    "import p9_util_spacy\n",
    "import DataPreparator_v2\n",
    "\n",
    "root_filename = './data/df_dataprep_matrix_'\n",
    "start_time = time.time()\n",
    "bulk_id_start = 0\n",
    "\n",
    "nb_step = dict_param_preparator['nb_step']\n",
    "remain_step = dict_param_preparator['remain_step']\n",
    "bulk_row = dict_param_preparator['remain_step']\n",
    "binary_threshold = dict_param_preparator['bulk_row']\n",
    "\n",
    "df_dataprep_matrix = pd.DataFrame()\n",
    "\n",
    "list_df_data_file = list()\n",
    "for bulk_id in range(bulk_id_start,nb_step) :\n",
    "    item_start = bulk_id*bulk_row\n",
    "    item_end = item_start+bulk_row\n",
    "    filename = root_filename+str(bulk_id)+'.dill'\n",
    "    list_df_data_file.append(filename)\n",
    "    if True :\n",
    "        df_dataprep_matrix = p5_util.object_load(filename)\n",
    "        #filename = './data/df_dataprep_matrix_0_new.dill'\n",
    "        \n",
    "        df_dataprep_matrix['target'] = df_train['target'].loc[df_dataprep_matrix.index]\n",
    "\n",
    "        #-----------------------------------------------------------\n",
    "        # Convert target to binary label\n",
    "        #-----------------------------------------------------------\n",
    "        df_dataprep_matrix['target'] = p9_util.convert_vectorFloat_2_binaryLabel(df_dataprep_matrix['target'].values, \\\n",
    "                                                                         threshold=binary_threshold, \\\n",
    "                                                                         direction = 1, \\\n",
    "                                                                         decimal_count = 1)\n",
    "        p5_util.object_dump(df_dataprep_matrix, filename, is_verbose=False)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    #start_time = time.time()\n",
    "    print(\"Range processed : {} --> {} Elapsed time : {:2f}\".format(item_start,item_end, elapsed_time), end='\\r')\n",
    "    \n",
    "\n",
    "if True :    \n",
    "    if 0 < remain_step  :\n",
    "        start_time = time.time()\n",
    "        print(\"\")\n",
    "        bulk_id+=1\n",
    "        item_start = item_end\n",
    "        item_end = item_start+remain_step\n",
    "        filename = root_filename+str(bulk_id)+'.dill'\n",
    "        list_df_data_file.append(filename)\n",
    "\n",
    "        if False :\n",
    "            df_dataprep_matrix = p5_util.object_load(filename)\n",
    "            #filename = './data/df_dataprep_matrix_0_new.dill'\n",
    "            list_df_data_file.append(filename)\n",
    "\n",
    "            df_dataprep_matrix['target'] = df_train['target'].loc[df_dataprep_matrix.index]\n",
    "\n",
    "            #-----------------------------------------------------------\n",
    "            # Convert target to binary label\n",
    "            #-----------------------------------------------------------\n",
    "            df_dataprep_matrix['target'] = p9_util.convert_vectorFloat_2_binaryLabel(df_dataprep_matrix['target'].values, \\\n",
    "                                                                             threshold=binary_threshold, \\\n",
    "                                                                             direction = 1, \\\n",
    "                                                                             decimal_count = 1)\n",
    "            p5_util.object_dump(df_dataprep_matrix, filename, is_verbose=False)\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(\"Range processed : {} --> {} Elapsed time : {:2f}\".format(item_start,item_end, elapsed_time), end='\\r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=blus>1.1.2. Build DataPreparator_v2 objects for train and validation</font>\n",
    "    \n",
    "    DataPreparator_v2 objects are required for building DataGenerators objects.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "130"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Number of files= {}\".format(len(list_df_data_file)))\n",
    "import random\n",
    "\n",
    "set_df_data_file = set()\n",
    "for idfile in range(len(list_df_data_file)//10) :\n",
    "    idrand = random.randrange(0, len(list_df_data_file))\n",
    "    set_df_data_file.add(list_df_data_file[idrand])\n",
    "\n",
    "list_valid_df_data_file = list(set_df_data_file)\n",
    "print(\"Number of file for validation= {}\".format(len(list_valid_df_data_file)))\n",
    "\n",
    "list_train_df_data_file = [train_df_data_file for train_df_data_file in list_df_data_file if train_df_data_file not in list_valid_df_data_file]\n",
    "print(\"Number of file for validation= {}\".format(len(list_train_df_data_file)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get configuration files issued from manual data-preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'other': None,\n",
       " 'slice_length': 10000,\n",
       " 'min_token_len': 2,\n",
       " 'max_token_len': -1,\n",
       " 'min_doc_len': 1,\n",
       " 'max_doc_len': -1,\n",
       " 'spacy_model_language': 'en_core_web_lg',\n",
       " 'tokenizer': None,\n",
       " 'max_padding_length': 100,\n",
       " 'oov_keyword': None,\n",
       " 'entity_keyword': None,\n",
       " 'nb_word_most_frequent': 0,\n",
       " 'is_df_copied': False,\n",
       " 'is_tfidf': False,\n",
       " 'threshold': 0.5}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datapreparator_config\n",
    "dict_param_dataprep = datapreparator_config.dict_param_dataprep\n",
    "dict_param_dataprep['max_padding_length'] = dict_param_preparator['max_row']\n",
    "dict_param_dataprep['threshold'] = dict_param_preparator['binary_threshold']\n",
    "dict_param_dataprep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build DataPreparator_v2 objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataPreparator_train = DataPreparator_v2.DataPreparator_v2(**dict_param_dataprep)\n",
    "dataPreparator_train.list_df_data_file = list_train_df_data_file.copy()\n",
    "\n",
    "dataPreparator_valid = DataPreparator_v2.DataPreparator_v2(**dict_param_dataprep)\n",
    "dataPreparator_valid.list_df_data_file = list_valid_df_data_file.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "130"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataPreparator_train.list_df_data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Target vector is loaded from files into DataPreparator_v2 attribute.\n",
    "\n",
    "    This is a trick in order to save time for next access to target vectors.\n",
    "    Target vectors are read from list of files into DataPreparator_v2 and cached into \n",
    "    object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0., 1., 0., 1., 0., 1., 1., 0.])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataPreparator_valid.y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 0., 0., 1., 0., 0., 0., 0., 1.])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataPreparator_train.y[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save DataPreparator_v2 objects for train and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "object_dump: file name= ./data/dataPreparator_train_129651.dill\n",
      "object_dump: file name= ./data/dataPreparator_valid_12000.dill\n"
     ]
    }
   ],
   "source": [
    "import p5_util\n",
    "p5_util.object_dump(dataPreparator_train, './data/dataPreparator_train_'+str(dataPreparator_train.total_row)+'.dill', is_verbose=True)\n",
    "p5_util.object_dump(dataPreparator_valid, './data/dataPreparator_valid_'+str(dataPreparator_valid.total_row)+'.dill', is_verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=blus>1.1.3 Build data generator for train and validation dataset</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load DataPreparator objects for train and validation previously built"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p5_util.object_load : fileName= ./data/dataPreparator_train_129651.dill\n",
      "p5_util.object_load : fileName= ./data/dataPreparator_valid_12000.dill\n"
     ]
    }
   ],
   "source": [
    "import p5_util\n",
    "dataPreparator_train = p5_util.object_load('./data/dataPreparator_train_129651.dill', is_verbose=True)\n",
    "dataPreparator_valid = p5_util.object_load('./data/dataPreparator_valid_12000.dill', is_verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load configuration file issued from manual data-preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p5_util.object_load : fileName= ./data/dict_param_preparator.dill\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'binary_threshold': 0.5,\n",
       " 'n_sample_per_class': 71019,\n",
       " 'max_row': 100,\n",
       " 'nb_step': 141,\n",
       " 'bulk_row': 1000,\n",
       " 'remain_step': 651,\n",
       " 'embedding': 300}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import p5_util\n",
    "filename = './data/dict_param_preparator.dill'\n",
    "dict_param_preparator = p5_util.object_load(filename, is_verbose=True)\n",
    "dict_param_preparator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tune configuration file for building DataGenerator object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data_type': 'train',\n",
       " 'partition_size': 1000,\n",
       " 'is_dimension_mux': False,\n",
       " 'proj_dimension': None,\n",
       " 'batch_size': 1000,\n",
       " 'n_classes': 2,\n",
       " 'n_channels': 0,\n",
       " 'is_shuffle': False,\n",
       " 'keras_nb_channel': 1,\n",
       " 'list_keras_channel': [],\n",
       " 'keras_input_dim': (100, 300),\n",
       " 'binary_threshold': 0.5}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import p9_util\n",
    "import p9_util_config\n",
    "\n",
    "#----------------------------------------------------------------------------\n",
    "# Common parameters for DataGenerator objects.\n",
    "#----------------------------------------------------------------------------\n",
    "dict_param_generator = p9_util_config.dict_param_generator\n",
    "\n",
    "nbClasses = 2\n",
    "dict_param_generator['n_classes'] = nbClasses\n",
    "dict_param_generator['partition_size'] = 1000\n",
    "dict_param_generator['binary_threshold'] = dict_param_preparator['binary_threshold']\n",
    "dict_param_generator['keras_input_dim'] = (dict_param_preparator['max_row'],dict_param_preparator['embedding'])\n",
    "dict_param_generator['batch_size'] = 1000\n",
    "\n",
    "dict_param_generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build DataGenerator object used for training\n",
    "\n",
    "    Object is dumped on harddisk once operation is completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Partition size = 1000\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_0.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_1.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_2.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_3.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_4.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_5.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_6.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_7.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_8.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_9.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_10.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_11.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_12.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_13.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_14.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_15.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_16.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_17.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_18.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_19.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_20.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_21.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_22.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_23.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_24.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_25.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_26.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_27.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_28.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_29.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_31.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_33.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_34.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_35.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_37.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_38.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_39.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_40.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_41.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_42.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_43.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_44.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_45.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_46.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_47.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_48.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_49.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_50.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_51.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_53.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_54.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_55.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_56.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_58.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_59.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_60.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_61.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_62.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_63.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_64.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_65.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_67.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_68.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_69.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_70.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_71.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_72.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_73.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_74.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_75.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_76.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_77.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_78.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_80.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_81.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_83.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_84.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_85.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_87.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_88.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_89.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_90.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_91.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_92.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_93.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_94.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_95.dill\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_96.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_97.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_98.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_99.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_100.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_101.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_102.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_103.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_104.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_105.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_106.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_107.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_108.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_110.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_111.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_112.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_113.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_114.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_115.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_116.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_117.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_119.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_120.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_121.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_123.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_124.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_125.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_126.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_127.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_128.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_129.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_130.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_131.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_132.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_133.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_134.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_135.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_136.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_137.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_138.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_139.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_140.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_141.dill\n",
      "Building train partition...\n",
      "\n",
      "Partition file : 0 / 1 Done!\r"
     ]
    }
   ],
   "source": [
    "import test_datapreparator\n",
    "dict_param_generator['data_type'] = 'train'\n",
    "\n",
    "generator = test_datapreparator.build_generator(dataPreparator_train, \\\n",
    "                     dict_param_generator,\\\n",
    "                     data_column_name='matrix_padded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'keras_input_dim': (100, 300),\n",
       " 'batch_size': 1000,\n",
       " 'n_classes': 2,\n",
       " 'n_channels': 0,\n",
       " 'is_shuffle': False,\n",
       " 'keras_nb_channel': 1,\n",
       " 'list_keras_channel': [],\n",
       " 'data_type': 'train',\n",
       " 'proj_dimension': None,\n",
       " 'is_dimension_mux': False,\n",
       " 'binary_threshold': 0.5}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build DataGenerator object used for validation\n",
    "    Object is dumped on harddisk once operation is completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data_type': 'valid',\n",
       " 'partition_size': 1000,\n",
       " 'is_dimension_mux': False,\n",
       " 'proj_dimension': None,\n",
       " 'batch_size': 1000,\n",
       " 'n_classes': 2,\n",
       " 'n_channels': 0,\n",
       " 'is_shuffle': False,\n",
       " 'keras_nb_channel': 1,\n",
       " 'list_keras_channel': [],\n",
       " 'keras_input_dim': (100, 300),\n",
       " 'binary_threshold': 0.5}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_param_generator['data_type'] = 'valid'\n",
    "dict_param_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Partition size = 1000\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_32.dill\n",
      "Building valid partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_82.dill\n",
      "Building valid partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_57.dill\n",
      "Building valid partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_122.dill\n",
      "Building valid partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_118.dill\n",
      "Building valid partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_86.dill\n",
      "Building valid partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_66.dill\n",
      "Building valid partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_52.dill\n",
      "Building valid partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_30.dill\n",
      "Building valid partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_79.dill\n",
      "Building valid partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_36.dill\n",
      "Building valid partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_dataprep_matrix_109.dill\n",
      "Building valid partition...\n",
      "\n",
      "Partition file : 0 / 1 Done!\r"
     ]
    }
   ],
   "source": [
    "import test_datapreparator\n",
    "generator = test_datapreparator.build_generator(dataPreparator_valid, \\\n",
    "                     dict_param_generator,\\\n",
    "                     data_column_name='matrix_padded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=blus> 1.2. Load automation dataset from `DataPreparator_v2`</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=blus> 1.2.1 Load configuration file issue from `DataPreparator_v2`</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    DataPreparator_v2 objects contain digitalized corpus for train and validation data.\n",
    "    \n",
    "    File names to access DataPreparator_v2 objects are built from configuration file that have \n",
    "    provided parameters for data-reparation.\n",
    "    \n",
    "    Keep filename from DataPreparator notebook from section 1.2.4 (Save datapreparator configuration).\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p5_util.object_load : fileName= ./data/dict_param_sequence_train_60000.dump\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'step': 1,\n",
       " 'step_end': 2,\n",
       " 'previous_step_file_name': './data/DataPreparator_v2_40Tokens_spacy__en_core_web_lg__train_60000_step1.dill',\n",
       " 'dict_param_step': {1: {'dataset_filename': './data/X_y_balanced',\n",
       "   'dict_param_dataprep': {'other': None,\n",
       "    'slice_length': 10000,\n",
       "    'min_token_len': 2,\n",
       "    'max_token_len': -1,\n",
       "    'min_doc_len': 1,\n",
       "    'max_doc_len': -1,\n",
       "    'spacy_model_language': 'en_core_web_lg',\n",
       "    'tokenizer': None,\n",
       "    'max_padding_length': 105,\n",
       "    'oov_keyword': None,\n",
       "    'entity_keyword': None,\n",
       "    'nb_word_most_frequent': 0,\n",
       "    'is_df_copied': False,\n",
       "    'is_tfidf': False}},\n",
       "  2: {'dataprep_step_filename': None,\n",
       "   'dict_param_dataprep': {'other': None,\n",
       "    'slice_length': 10000,\n",
       "    'min_token_len': 2,\n",
       "    'max_token_len': -1,\n",
       "    'min_doc_len': 1,\n",
       "    'max_doc_len': -1,\n",
       "    'spacy_model_language': 'en_core_web_lg',\n",
       "    'tokenizer': None,\n",
       "    'max_padding_length': 105,\n",
       "    'oov_keyword': None,\n",
       "    'entity_keyword': None,\n",
       "    'nb_word_most_frequent': 0,\n",
       "    'is_df_copied': False,\n",
       "    'is_tfidf': False},\n",
       "   'bulk_row': 5000,\n",
       "   'dict_restart_step': {'id_bulk_row': 0},\n",
       "   'dict_param_subsequence': {'start_substep': 0,\n",
       "    'end_substep': 0,\n",
       "    'dict_param_step': {1: {'fixed_count_file': 0}}}},\n",
       "  3: {'dataprep_step_filename': None,\n",
       "   'dict_param_dataprep': {'other': None,\n",
       "    'slice_length': 10000,\n",
       "    'min_token_len': 2,\n",
       "    'max_token_len': -1,\n",
       "    'min_doc_len': 1,\n",
       "    'max_doc_len': -1,\n",
       "    'spacy_model_language': 'en_core_web_lg',\n",
       "    'tokenizer': None,\n",
       "    'max_padding_length': 105,\n",
       "    'oov_keyword': None,\n",
       "    'entity_keyword': None,\n",
       "    'nb_word_most_frequent': 0,\n",
       "    'is_df_copied': False,\n",
       "    'is_tfidf': False},\n",
       "   'ipca_batch_size': 10000,\n",
       "   'percent_var': 0.9,\n",
       "   'xpca': None,\n",
       "   'dict_param_subsequence': {'start_substep': 2,\n",
       "    'end_substep': 2,\n",
       "    'dict_param_step': {1: {'method': <function DataPreparator_v2.DataPreparator_v2.build_ipca_operator(self, batch_size)>},\n",
       "     2: {'method': <function DataPreparator_v2.DataPreparator_v2.matrixpadded_truncate(self, percent_var)>}}}}},\n",
       " 'data_type': 'train',\n",
       " 'root_file_name': './data/DataPreparator_v2_MaxTokens_spacy__en_core_web_lg_',\n",
       " 'n_sample_train': 60000,\n",
       " 'n_sample_valid': 10000,\n",
       " 'file_format': '.dill'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import p5_util\n",
    "\n",
    "filename = './data/dict_param_sequence_train_60000.dump'\n",
    "dict_param_sequence = p5_util.object_load(filename)\n",
    "dict_param_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Build DataPreparator file name for validation dataset and read object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/DataPreparator_v2_MaxTokens_spacy__en_core_web_lg__valid_10000_step2.dill\n",
      "p5_util.object_load : fileName= ./data/DataPreparator_v2_MaxTokens_spacy__en_core_web_lg__valid_10000_step2.dill\n"
     ]
    }
   ],
   "source": [
    "import p5_util\n",
    "import test_datapreparator\n",
    "\n",
    "data_type = 'valid'\n",
    "#n_sample = 10000\n",
    "#dict_param_sequence['data_type'] = data_type\n",
    "#dict_param_sequence['n_sample_valid'] = n_sample\n",
    "valid_filename = test_datapreparator.datapreparator_filename(data_type, dict_param_sequence, step=2)\n",
    "print(valid_filename)\n",
    "\n",
    "\n",
    "dataPreparator_v2_valid = p5_util.object_load(valid_filename, is_verbose=True)\n",
    "if dataPreparator_v2_valid is not None :\n",
    "    # Remove duplicated indexes\n",
    "    dataPreparator_v2_valid.df_data = \\\n",
    "    dataPreparator_v2_valid.df_data.loc[~dataPreparator_v2_valid.df_data.index.duplicated(keep='first')]\n",
    "else :\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Build DataPreparator file name for train dataset and read object."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "nbClasses =  dataPreparator_v2_valid.df_data.target.max()-dataPreparator_v2_valid.df_data.target.min()+1\n",
    "nbClasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/DataPreparator_v2_MaxTokens_spacy__en_core_web_lg__train_60000_step2.dill\n",
      "p5_util.object_load : fileName= ./data/DataPreparator_v2_MaxTokens_spacy__en_core_web_lg__train_60000_step2.dill\n"
     ]
    }
   ],
   "source": [
    "import p5_util\n",
    "import test_datapreparator\n",
    "\n",
    "data_type = 'train'\n",
    "dict_param_sequence['data_type'] = data_type\n",
    "\n",
    "train_filename = test_datapreparator.datapreparator_filename(data_type, dict_param_sequence, step=2)\n",
    "print(train_filename)\n",
    "dataPreparator_v2_train = p5_util.object_load(train_filename)\n",
    "if dataPreparator_v2_train is not None :\n",
    "    # Remove duplicated indexes\n",
    "    dataPreparator_v2_train.df_data = \\\n",
    "    dataPreparator_v2_train.df_data.loc[~dataPreparator_v2_train.df_data.index.duplicated(keep='first')]\n",
    "else :\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=blus>1.2. Building targets as labels </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update target in `DataPreparator_v2` objects and convert them as abel format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold=0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataPreparator_v2_train.targetUpdate2BinaryLabel(threshold=threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 0, 1, 1, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataPreparator_v2_train.y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataPreparator_v2_valid.targetUpdate2BinaryLabel(threshold=threshold)filename_valid_blanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 0, 1, 1, 0, 1, 1, 0])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataPreparator_v2_valid.y[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=blus>1.3. Building DataGenerator objects</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "percent_var = 0.\n",
    "print(percent_var)\n",
    "\n",
    "#---------------------------------------------\n",
    "# Used to build filename for DataPreparator_v2\n",
    "# When None then the latest file name is built.\n",
    "#---------------------------------------------\n",
    "#step = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bangui/.local/lib/python3.6/site-packages/matplotlib/__init__.py:886: MatplotlibDeprecationWarning: \n",
      "examples.directory is deprecated; in the future, examples will be found relative to the 'datapath' directory.\n",
      "  \"found relative to the 'datapath' directory.\".format(key))\n",
      "Using TensorFlow backend.\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1015 12:10:32.029115 139765984094016 lazy_loader.py:50] \n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "W1015 12:10:32.030022 139765984094016 deprecation_wrapper.py:119] From /home/bangui/anaconda3/envs/python36/lib/python3.6/site-packages/adanet/tf_compat/__init__.py:96: The name tf.losses.Reduction is deprecated. Please use tf.compat.v1.losses.Reduction instead.\n",
      "\n",
      "W1015 12:10:32.044456 139765984094016 deprecation_wrapper.py:119] From /home/bangui/Dropbox/Perso/Formation/openclassrooms/OC_Datascientist/Kaggle/p8_util_config.py:137: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'data_type': 'valid',\n",
       " 'partition_size': 1000,\n",
       " 'is_dimension_mux': False,\n",
       " 'proj_dimension': None,\n",
       " 'batch_size': 100,\n",
       " 'n_classes': 2,\n",
       " 'n_channels': 0,\n",
       " 'is_shuffle': False,\n",
       " 'keras_nb_channel': 1,\n",
       " 'list_keras_channel': [],\n",
       " 'keras_input_dim': (None, None)}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import p9_util\n",
    "import p9_util_config\n",
    "\n",
    "#----------------------------------------------------------------------------\n",
    "# Common parameters for DataGenerator objects.\n",
    "#----------------------------------------------------------------------------\n",
    "dict_param_generator = p9_util_config.dict_param_generator\n",
    "\n",
    "nbClasses = 2\n",
    "dict_param_generator['n_classes'] = nbClasses\n",
    "dict_param_generator['partition_size'] = 1000\n",
    "dict_param_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm ./data/train_X_*.*\n",
    "!rm ./data/valid_X_*.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using DataPreparator given as parameter...\n",
      "\n",
      "DataPreparator Dataframe shape= (59860, 1)\n",
      "p5_util.object_load : fileName= ./data/df_data_df_train_60000_step2_11.dill\n",
      "\n",
      "data_type : train\n",
      "partition_size : 5000\n",
      "is_dimension_mux : False\n",
      "proj_dimension : None\n",
      "batch_size : 500\n",
      "n_classes : 2\n",
      "n_channels : 0\n",
      "is_shuffle : False\n",
      "keras_nb_channel : 0\n",
      "list_keras_channel : []\n",
      "keras_input_dim : (105, 300)\n",
      "\n",
      "Building datagenerator...\n",
      "\n",
      "*** Partition size = 5000\n",
      "p5_util.object_load : fileName= ./data/df_data_df_train_60000_step2_0.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_data_df_train_60000_step2_1.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_data_df_train_60000_step2_2.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_data_df_train_60000_step2_3.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_data_df_train_60000_step2_4.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_data_df_train_60000_step2_5.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_data_df_train_60000_step2_6.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_data_df_train_60000_step2_7.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_data_df_train_60000_step2_8.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_data_df_train_60000_step2_9.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_data_df_train_60000_step2_10.dill\n",
      "Building train partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_data_df_train_60000_step2_11.dill\n",
      "Building train partition...\n",
      "\n",
      "Partition file : 0 / 1 Done!\r"
     ]
    }
   ],
   "source": [
    "#!rm ./data/train_X_*.*\n",
    "data_type = 'train'\n",
    "#---------------------------------------------------------------------------\n",
    "# Build file name allowing to access DataPreparator_v2 object.\n",
    "#  Filename issued from automated process of data preparation.\n",
    "#  Filename is built from automated data-preparation configuration, \n",
    "#  implemented into dictionary dict_param_sequence.\n",
    "#---------------------------------------------------------------------------\n",
    "filename_datapreparator = test_datapreparator.datapreparator_filename(data_type,dict_param_sequence, step=None)\n",
    "\n",
    "#----------------------------------------------------------------------------\n",
    "# DataGenerator for train dataset\n",
    "#----------------------------------------------------------------------------\n",
    "dict_param_generator['partition_size'] = dict_param_sequence['dict_param_step'][2]['bulk_row']\n",
    "\n",
    "dict_param_generator['batch_size'] = 500\n",
    "dict_param_generator['data_type'] = data_type\n",
    "\n",
    "#----------------------------------------------------------------------------\n",
    "# Parameters configuration in order to build Datagenerator object for \n",
    "# train dataset.\n",
    "#----------------------------------------------------------------------------\n",
    "dict_cnn_preprocess={\n",
    "    'filename_datapreparator' : filename_datapreparator,\\\n",
    "    'percent_var' : percent_var,\\\n",
    "}\n",
    "train_generator = test_datapreparator.build_cnn_datagenerator(**dict_cnn_preprocess, \\\n",
    "                                                              dataPreparator_v2 = dataPreparator_v2_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: impossible de supprimer './data/valid_X_*.*': Aucun fichier ou dossier de ce type\n",
      "Using DataPreparator given as parameter...\n",
      "\n",
      "DataPreparator Dataframe shape= (9980, 1)\n",
      "p5_util.object_load : fileName= ./data/df_data_df_valid_10000_step2_1.dill\n",
      "\n",
      "data_type : valid\n",
      "partition_size : 5000\n",
      "is_dimension_mux : False\n",
      "proj_dimension : None\n",
      "batch_size : 500\n",
      "n_classes : 2\n",
      "n_channels : 0\n",
      "is_shuffle : False\n",
      "keras_nb_channel : 0\n",
      "list_keras_channel : []\n",
      "keras_input_dim : (105, 300)\n",
      "\n",
      "Building datagenerator...\n",
      "\n",
      "*** Partition size = 5000\n",
      "p5_util.object_load : fileName= ./data/df_data_df_valid_10000_step2_0.dill\n",
      "Building valid partition...\n",
      "\n",
      "p5_util.object_load : fileName= ./data/df_data_df_valid_10000_step2_1.dill\n",
      "Building valid partition...\n",
      "\n",
      "Partition file : 0 / 1 Done!\r"
     ]
    }
   ],
   "source": [
    "!rm ./data/valid_X_*.*    \n",
    "import test_datapreparator\n",
    "\n",
    "data_type = 'valid'\n",
    "#----------------------------------------------------------------------------\n",
    "# DataGenerator for validation dataset configuration\n",
    "#----------------------------------------------------------------------------\n",
    "dict_param_generator['partition_size'] = dict_param_sequence['dict_param_step'][2]['bulk_row']\n",
    "#dict_param_generator['partition_size'] = 1000\n",
    "dict_param_generator['batch_size'] = 500\n",
    "dict_param_generator['data_type'] = data_type\n",
    "\n",
    "#----------------------------------------------------------------------------\n",
    "#  Filename issued from automated process of data preparation.\n",
    "#  Filename is built from automated data-preparation configuration, \n",
    "# implemented into dictionary dict_param_sequence.\n",
    "#----------------------------------------------------------------------------\n",
    "filename_datapreparator = test_datapreparator.datapreparator_filename(data_type,dict_param_sequence, step=None)\n",
    "\n",
    "#----------------------------------------------------------------------------\n",
    "# Parameters configuration in order to build Datagenerator object for \n",
    "# validation dataset.\n",
    "#----------------------------------------------------------------------------\n",
    "dict_cnn_preprocess={\n",
    "    'filename_datapreparator' : filename_datapreparator,\\\n",
    "    'percent_var' : percent_var,\\\n",
    "}\n",
    "valid_generator = test_datapreparator.build_cnn_datagenerator(**dict_cnn_preprocess,\\\n",
    "                                                             dataPreparator_v2 = dataPreparator_v2_valid,\\\n",
    "                                                             dict_param_generator = dict_param_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NoneType"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataPreparator_v2_valid.xpca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save / Restore DataGenerator objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p5_util.object_load : fileName= ./data/train_generator.dill\n",
      "p5_util.object_load : fileName= ./data/valid_generator.dill\n"
     ]
    }
   ],
   "source": [
    "import p5_util\n",
    "\n",
    "filename_train = './data/train_generator.dill'\n",
    "filename_valid = './data/valid_generator.dill'\n",
    "if False :\n",
    "    train_generator = p5_util.object_load(filename_train)\n",
    "    valid_generator = p5_util.object_load(filename_valid)\n",
    "else :\n",
    "    p5_util.object_dump(train_generator, filename_train, is_verbose=True)\n",
    "    p5_util.object_dump(valid_generator, filename_valid, is_verbose=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Switch to DataGenerator new version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import p5_util\n",
    "\n",
    "filename_train = './data/train_generator.dill'\n",
    "train_generator = p5_util.object_load(filename_train)\n",
    "\n",
    "is_train = True\n",
    "is_valid = not is_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p5_util.object_load : fileName= ./data/valid_generator.dill\n"
     ]
    }
   ],
   "source": [
    "import p5_util\n",
    "\n",
    "filename_valid = './data/valid_generator.dill'\n",
    "valid_generator = p5_util.object_load(filename_valid)\n",
    "\n",
    "is_train = False\n",
    "is_valid = not is_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Train, Valid)= (False,True)\n"
     ]
    }
   ],
   "source": [
    "print(\"(Train, Valid)= ({},{})\".format(is_train, is_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#datagen_new = train_generator\n",
    "if is_train :\n",
    "    datagen_new = train_generator\n",
    "else :     \n",
    "    datagen_new = valid_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import DataGenerator\n",
    "datagen = DataGenerator.DataGenerator(dict_X=datagen_new.dict_X, dict_label=datagen_new.dict_label,\\\n",
    "                                      nb_record = datagen_new.nb_record, other=datagen_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import DataGenerator\n",
    "datagen_new = DataGenerator.DataGenerator(dict_X=datagen.dict_X, dict_label=datagen.dict_label,\\\n",
    "                                      nb_record = datagen.nb_record, other=datagen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "object_dump: file name= ./data/valid_generator.dill\n"
     ]
    }
   ],
   "source": [
    "if is_train :\n",
    "    p5_util.object_dump(datagen_new, filename_train, is_verbose=True)\n",
    "else : \n",
    "    p5_util.object_dump(datagen_new, filename_valid, is_verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
