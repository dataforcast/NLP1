{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, CNN estimator is used as a regressor model.\n",
    "This choice is leaded by the fact that :\n",
    "\n",
    "<pre>\n",
    "<li>NN model are non-linear models, then it is supposed to capture non-linear relations in between features </li>\n",
    "<li>CNN model provides results while extracting semantic structures from text.</li>\n",
    "</pre>\n",
    "\n",
    "Multiple dataset are used in order to benchmark assumptions forged about model.\n",
    "\n",
    "Data used for feeding model are read from mass storage.\n",
    "\n",
    "Transformations that may occure over data are used only for the purpose of this notebooK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>CNN Classifier</b>\n",
       "<div id=\"my_id_menu_nb\">run previous cell, wait for 2 seconds</div>\n",
       "<script>\n",
       "function repeat_indent_string(n){\n",
       "    var a = \"\" ;\n",
       "    for ( ; n > 0 ; --n)\n",
       "        a += \"    \";\n",
       "    return a;\n",
       "}\n",
       "// look up into all sections and builds an automated menu //\n",
       "var update_menu_string = function(begin, lfirst, llast, sformat, send, keep_item, begin_format, end_format) {\n",
       "    var anchors = document.getElementsByClassName(\"section\");\n",
       "    if (anchors.length == 0) {\n",
       "        anchors = document.getElementsByClassName(\"text_cell_render rendered_html\");\n",
       "    }\n",
       "    var i,t;\n",
       "    var text_menu = begin;\n",
       "    var text_memo = \"<pre>\\nlength:\" + anchors.length + \"\\n\";\n",
       "    var ind = \"\";\n",
       "    var memo_level = 1;\n",
       "    var href;\n",
       "    var tags = [];\n",
       "    var main_item = 0;\n",
       "    var format_open = 0;\n",
       "    for (i = 0; i <= llast; i++)\n",
       "        tags.push(\"h\" + i);\n",
       "\n",
       "    for (i = 0; i < anchors.length; i++) {\n",
       "        text_memo += \"**\" + anchors[i].id + \"--\\n\";\n",
       "\n",
       "        var child = null;\n",
       "        for(t = 0; t < tags.length; t++) {\n",
       "            var r = anchors[i].getElementsByTagName(tags[t]);\n",
       "            if (r.length > 0) {\n",
       "child = r[0];\n",
       "break;\n",
       "            }\n",
       "        }\n",
       "        if (child == null) {\n",
       "            text_memo += \"null\\n\";\n",
       "            continue;\n",
       "        }\n",
       "        if (anchors[i].hasAttribute(\"id\")) {\n",
       "            // when converted in RST\n",
       "            href = anchors[i].id;\n",
       "            text_memo += \"#1-\" + href;\n",
       "            // passer à child suivant (le chercher)\n",
       "        }\n",
       "        else if (child.hasAttribute(\"id\")) {\n",
       "            // in a notebook\n",
       "            href = child.id;\n",
       "            text_memo += \"#2-\" + href;\n",
       "        }\n",
       "        else {\n",
       "            text_memo += \"#3-\" + \"*\" + \"\\n\";\n",
       "            continue;\n",
       "        }\n",
       "        var title = child.textContent;\n",
       "        var level = parseInt(child.tagName.substring(1,2));\n",
       "\n",
       "        text_memo += \"--\" + level + \"?\" + lfirst + \"--\" + title + \"\\n\";\n",
       "\n",
       "        if ((level < lfirst) || (level > llast)) {\n",
       "            continue ;\n",
       "        }\n",
       "        if (title.endsWith('¶')) {\n",
       "            title = title.substring(0,title.length-1).replace(\"<\", \"&lt;\")\n",
       "         .replace(\">\", \"&gt;\").replace(\"&\", \"&amp;\");\n",
       "        }\n",
       "        if (title.length == 0) {\n",
       "            continue;\n",
       "        }\n",
       "\n",
       "        while (level < memo_level) {\n",
       "            text_menu += end_format + \"</ul>\\n\";\n",
       "            format_open -= 1;\n",
       "            memo_level -= 1;\n",
       "        }\n",
       "        if (level == lfirst) {\n",
       "            main_item += 1;\n",
       "        }\n",
       "        if (keep_item != -1 && main_item != keep_item + 1) {\n",
       "            // alert(main_item + \" - \" + level + \" - \" + keep_item);\n",
       "            continue;\n",
       "        }\n",
       "        while (level > memo_level) {\n",
       "            text_menu += \"<ul>\\n\";\n",
       "            memo_level += 1;\n",
       "        }\n",
       "        text_menu += repeat_indent_string(level-2);\n",
       "        text_menu += begin_format + sformat.replace(\"__HREF__\", href).replace(\"__TITLE__\", title);\n",
       "        format_open += 1;\n",
       "    }\n",
       "    while (1 < memo_level) {\n",
       "        text_menu += end_format + \"</ul>\\n\";\n",
       "        memo_level -= 1;\n",
       "        format_open -= 1;\n",
       "    }\n",
       "    text_menu += send;\n",
       "    //text_menu += \"\\n\" + text_memo;\n",
       "\n",
       "    while (format_open > 0) {\n",
       "        text_menu += end_format;\n",
       "        format_open -= 1;\n",
       "    }\n",
       "    return text_menu;\n",
       "};\n",
       "var update_menu = function() {\n",
       "    var sbegin = \"\";\n",
       "    var sformat = '<a href=\"#__HREF__\">__TITLE__</a>';\n",
       "    var send = \"\";\n",
       "    var begin_format = '<li>';\n",
       "    var end_format = '</li>';\n",
       "    var keep_item = -1;\n",
       "    var text_menu = update_menu_string(sbegin, 1, 4, sformat, send, keep_item,\n",
       "       begin_format, end_format);\n",
       "    var menu = document.getElementById(\"my_id_menu_nb\");\n",
       "    menu.innerHTML=text_menu;\n",
       "};\n",
       "window.setTimeout(update_menu,2000);\n",
       "            </script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from jyquickhelper import add_notebook_menu\n",
    "add_notebook_menu(first_level=1, last_level=4, header=\"CNN Classifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=blus>0. Notebook configuration</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import gensim\n",
    "\n",
    "\n",
    "import p8_util\n",
    "import p8_util_config\n",
    "import p9_util\n",
    "import p9_util_spacy\n",
    "import DataPreparator\n",
    "extension='_part'\n",
    "\n",
    "OUTPUT_DIR = './tmp/baseline'\n",
    "OUTPUT_DIR_TB = './tmp'\n",
    "is_tensorboard = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensorboard processes are killed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will kill the processes for Tensorboard\n",
    "#is_tensorboard = True\n",
    "if is_tensorboard is True :\n",
    "    !ps aux | grep tensorboard | awk '{print $2}' | xargs kill\n",
    "# this will kill the processes for ngrok\n",
    "if is_tensorboard is True :\n",
    "    !ps aux | grep ngrok | awk '{print $2}' | xargs kill    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Display an item from tokenized corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=blus>1. Loading dataset</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls -alrth ./data/DataPreparator_*.dump"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=blus>1.1. Loading train DataPreparator </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vocab_size = len(dataPreparator_train.kerasTokenizer.tokenizer.word_index) + 1\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import p5_util\n",
    "\n",
    "filename_train = './data/DataPreparator_train_spacy_5000.dump'\n",
    "\n",
    "if False :\n",
    "    p5_util.object_dump(dataPreparator_train,filename_train)\n",
    "else : \n",
    "    dataPreparator_train = p5_util.object_load(filename_train)\n",
    "    \n",
    "print(\"\")\n",
    "print(\"DataPretarator train lenth= {}\".format(len(dataPreparator_train)))\n",
    "\n",
    "\n",
    "vocab_size = len(dataPreparator_train.kerasTokenizer.tokenizer.word_index) + 1\n",
    "print(\"Vocabulary size= {}\".format(vocab_size))\n",
    "\n",
    "X_train, y_train = dataPreparator_train.transform(None, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=blus>1.2. Loading test DataPreparator </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import numpy as np\n",
    "import p5_util\n",
    "\n",
    "#filename_test = './data/DataPreparator_test.dump'\n",
    "filename_test = './data/DataPreparator_test_spacy_5000.dump'\n",
    "\n",
    "if False :\n",
    "    p5_util.object_dump(dataPreparator_test,filename_test)\n",
    "else : \n",
    "    dataPreparator_test = p5_util.object_load(filename_test)\n",
    "    \n",
    "print(\"\")\n",
    "print(\"DataPretarator test lenth=  {}\".format(len(dataPreparator_test)))\n",
    "\n",
    "\n",
    "X_test, y_test = dataPreparator_test.transform(None, None)\n",
    "\n",
    "valid_sample = X_train.shape[0]//3\n",
    "valid_sample\n",
    "X_test = X_test[:valid_sample,:]\n",
    "y_test = y_test[:valid_sample]\n",
    "print(\"\")\n",
    "print(\"Sample of test dataset : X_test shape= {}, y_test shape= {}\".format(X_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dimension reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False :\n",
    "    X_dimension = 100\n",
    "    X_train = X_train[:,:X_dimension]\n",
    "    X_test  = X_test[:,:X_dimension]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(dataPreparator_train.df_data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentences tokens counts distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "df = dataPreparator_train.df_data['counting']\n",
    "print(df.describe())\n",
    "z_=sns.violinplot( y=df )\n",
    "\n",
    "print(\" \")\n",
    "df = dataPreparator_test.df_data['counting']\n",
    "print(dataPreparator_test.df_data['counting'].describe())\n",
    "z_=sns.violinplot( y=df )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=blus>1.x. Dataset standardization </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import p9_util\n",
    "import random\n",
    "if False :\n",
    "    scaler, X_train_std, X_test_std = p9_util.data_scale(X_train, X_test, scaler_name='Standard')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=blus>2. Targets transformation for binary classification</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=blus>2.1. Conversion of targets values to binaries values</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`y_train` and `y_test` are vectors with float values ranging from `0.0` to `1.0`. \n",
    "\n",
    "`decimal_count` is the value allowing to take into account the number of decimals when converting continuous values into classes.\n",
    "\n",
    "As a result, `y_train_label` and `y_test_label` are `[N x Classes]` matrix issued from One-hot-encoding were position of value `1` in a column indicates the class value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_label_vect_bin_sample = dataPreparator_train.vectorValue2BinaryvectorLabel()\n",
    "y_test_label_vect_bin_sample = dataPreparator_test.vectorValue2BinaryvectorLabel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nNumber of safe texts for train sample  = {}\".format(len((np.where(y_train_label_vect_bin_sample==0)[0]))))\n",
    "print(\"Number of toxics texts for train sample= {}\".format(len((np.where(y_train_label_vect_bin_sample==1)[0]))))\n",
    "\n",
    "print(\"\\nNumber of safe texts for test sample  = {}\".format(len((np.where(y_test_label_vect_bin_sample==0)[0]))))\n",
    "print(\"Number of toxics texts for test sample= {}\".format(len((np.where(y_test_label_vect_bin_sample==1)[0]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train_label_vect_bin_sample.copy()\n",
    "y_test = y_test_label_vect_bin_sample.copy()\n",
    "y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train[0:3])\n",
    "print(y_test[0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=blus>1.3. Building data generator </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=blus>1.3.1 Making disks partitions </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm ./data/train_X_*.*\n",
    "!rm ./data/test_X_*.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import p9_util\n",
    "\n",
    "partition_size = 1000\n",
    "dict_train_partition, dict_train_label = p9_util.make_partition(X_train, \\\n",
    "                                            y_train, partition_size,\\\n",
    "                                            data_type=\"train\", data_format='ndarray' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import p9_util\n",
    "\n",
    "dict_test_partition, dict_test_label = p9_util.make_partition(X_test, \\\n",
    "                                              y_test,partition_size,\\\n",
    "                                              data_type=\"test\", data_format='ndarray' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=blus>1.3.2 Building train and test `DataGenerator` </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import p9_util\n",
    "\n",
    "embedding_dim=0\n",
    "dimension = X_train.shape[1]\n",
    "params = {'dim': (dimension,embedding_dim),\n",
    "          'batch_size': 100,\n",
    "          'n_classes': 2,\n",
    "          'n_channels': 0,\n",
    "          'shuffle': False}\n",
    "print(params)\n",
    "\n",
    "# Data Generators\n",
    "import DataGenerator\n",
    "len_train = X_train.shape[0]\n",
    "train_generator = DataGenerator.DataGenerator(dict_train_partition, dict_train_label, partition_size, len_train,**params)\n",
    "\n",
    "len_test = X_test.shape[0]\n",
    "test_generator = DataGenerator.DataGenerator(dict_test_partition, dict_test_label, partition_size,len_test, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm ./data/*_generator_*.dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import p5_util\n",
    "nb_class = params['n_classes']\n",
    "\n",
    "nb_row = X_train.shape[0]\n",
    "filename_train = \"./data/train_generator_\"+str(nb_row)+\"_\"+str(nb_class)+\".dump\"\n",
    "\n",
    "nb_row = X_test.shape[0]\n",
    "filename_test = \"./data/test_generator_\"+str(nb_row)+\"_\"+str(nb_class)+\".dump\"\n",
    "\n",
    "p5_util.object_dump(train_generator, filename_train)\n",
    "p5_util.object_dump(test_generator, filename_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=blus>3. CNN processing using Keras</font>\n",
    "\n",
    "    Convolutional layers are built with strides >0 and BOW representation of texts.\n",
    "    This is equivalent to process Bag Of Words with n-gram.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=blus>3.1. Configuration of classifier model</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Print some parameters for CNN learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nX_train_bow shape = {}\".format(X_train.shape))\n",
    "print(\"Y train shape =       {}\".format(y_train.shape))\n",
    "print()\n",
    "\n",
    "print(\"X_test_bow shape  = {}\".format(X_test.shape))\n",
    "print(\"Y test shape =      {}\".format(y_test.shape))\n",
    "print()\n",
    "print(\"Vocabulary size= {}\".format(vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" \")\n",
    "print(\"Batch size= {}\".format(params['batch_size']))   \n",
    "vector_dimension = params['dim'][0]\n",
    "print(\"Input data size= {}\".format(vector_dimension))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=blus>2.2 Modeling a regression model</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=blus>2.2.1. Loading train and test data generators</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -alrth ./data/*_generator_*.dump"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=blus>2.2.2. CNN classifier building</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import p5_util\n",
    "\n",
    "filename_train = \"./data/train_generator_4571_2.dump\"\n",
    "\n",
    "filename_test = \"./data/test_generator_1523_2.dump\"\n",
    "\n",
    "train_generator = p5_util.object_load( filename_train)\n",
    "test_generator = p5_util.object_load( filename_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dir(train_generator)\n",
    "dict_param = train_generator.get_params()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_param= {'dim': (300, 0),\n",
    " 'batch_size': 100,\n",
    " 'n_classes': 2,\n",
    " 'n_channels': 0,\n",
    " 'shuffle': False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vector_dimension = dict_param['dim'][0]\n",
    "nbClasses = dict_param['n_classes']\n",
    "nb_channel = dict_param['n_channels']\n",
    "\n",
    "dict_param"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ouptput layer dimensions for convolutional network is computed as following : \n",
    "\n",
    "*  <code>C = [(I-F + 2*P)/S] +1</code> where :\n",
    "    * <code>C</code> is the dimension of the ouput convolutional layer;\n",
    "    * <code>I</code> is the input size\n",
    "    * <code>F</code> is the convolutional filter size\n",
    "    * <code>P</code> is the padding size (0 here)\n",
    "    * <code>S</code> is the srtides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_size = 3\n",
    "strides = filter_size-1\n",
    "\n",
    "print(\"Expected defaut dimension for 1st hidden layer : {}\".format(round((vector_dimension-filter_size+2*0)/strides +1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strides value of <code>S</code> allows to regard a bulk of <code>S</code> consecutive words for sentences patterns exploration. \n",
    "\n",
    "For compilation, a learning rate of <code>0.01</code> increases the speed of learning. \n",
    "\n",
    "Batch normalization insures a smooth learning.\n",
    "\n",
    "Batch size is selected in a such way an epoch is formed with 10 mini-batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import p9_util\n",
    "nb_channel=0\n",
    "model = p9_util.keras_cnn_build(dict_param['dim'],nbClasses=nbClasses, \\\n",
    "                                strides=strides, nb_filter=128, filter_size=strides, \\\n",
    "                                lr=1.e-2, dropout_rate=0., \\\n",
    "                                conv_layer=1,nb_dense_neuron=128,\\\n",
    "                                dense_layer=2)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model on dataset\n",
    "history = model.fit_generator(generator=train_generator,\n",
    "                    validation_data=test_generator,\n",
    "                    use_multiprocessing=False,\n",
    "                    workers=1, verbose=1, epochs=10)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "454/454 [==============================] - 17s 38ms/step - loss: 0.0368 - mean_absolute_error: 0.1385 - val_loss: 0.0401 - val_mean_absolute_error: 0.1442\n",
    "\n",
    "Epoch 10/10\n",
    "454/454 [==============================] - 9s 20ms/step - loss: 0.0394 - mean_absolute_error: 0.1459 - val_loss: 0.0400 - val_mean_absolute_error: 0.1443"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN model and history are either saved or restored depending a Boolean flag.\n",
    "\n",
    "`core_name` is a parameter identifying a model through its name.\n",
    "\n",
    "This name may be changed depending model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "core_name = \"cnn_class_6layers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import p5_util\n",
    "\n",
    "if True:\n",
    "    model.save(\"./data/model_\"+core_name+\".h5\")\n",
    "    p5_util.object_dump(history, \"history_\"+core_name+\".dump\")\n",
    "    #p5_util.object_dump(max_length, \"max_length_\"+core_name+\".dump\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import p5_util\n",
    "\n",
    "if False :\n",
    "    pass\n",
    "else :\n",
    "    model = keras.models.load_model(\"./data/model_\"+core_name+\".h5\")\n",
    "    history = p5_util.object_load(\"history_\"+core_name+\".dump\")\n",
    "    #max_length = p5_util.object_load(\"max_length_\"+core_name+\".dump\")\n",
    "\n",
    "#print(\"\\nInput data length= {}\".format(max_length))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Metric and loss curves are displayed for train and test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, values in history.history.items() :\n",
    "    history.history[key] = values[1:]\n",
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import p7_util\n",
    "max_length=''\n",
    "list_model_acc_value = p7_util.p7_plot_cnn_history(model, None, None, history=history, legend='length='+str(max_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import p7_util\n",
    "max_length=''\n",
    "list_model_acc_value = p7_util.p7_plot_cnn_history(model, None, None, history=history, legend='length='+str(max_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=blus>2.3. Words embeddings</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=blus>2.3.1. Building embeddings with Glove</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dictionary `{word:glove_coefficient}` is built from Glove file name.\n",
    "\n",
    "Glove file has been prealably downloaded.\n",
    "\n",
    "Once built, dictionary allows to build a vector for every word \n",
    "in vocabulary issued from tokenizer.\n",
    "\n",
    "Using glove file defined here-under, each word is a vector of dimension 100. This dimension is \n",
    "referenced in the Glove file name part as `100d`\n",
    "\n",
    "Endly, weights matrix is built from vocabulary issued from tokenizer. \n",
    "\n",
    "Such process is summarized with sequences here-under :\n",
    "    * dict_glove_word_coeff <-- processing Glove file name\n",
    "    * vocabulary_word, index <-- tokenizer\n",
    "    * weight_vector = dict_glove_word_coeff[vocabulary_word]\n",
    "    * weight_matrix[index] = weight_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import p5_util\n",
    "\n",
    "filename = './data/embbeding_glove.dump'\n",
    "\n",
    "weight_matrix = p5_util.object_load(filename)\n",
    "    \n",
    "print(\"\")\n",
    "print(\"Embedding shape=  {}\".format(weight_matrix.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import p9_util\n",
    "\n",
    "embedding_dim=100\n",
    "dimension = X_train.shape[1]\n",
    "params = {'dim': (dimension,embedding_dim),\n",
    "          'batch_size': 100,\n",
    "          'n_classes': 0,\n",
    "          'n_channels': 0,\n",
    "          'shuffle': False}\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Generators\n",
    "import DataGenerator\n",
    "len_train = X_train.shape[0]\n",
    "train_generator = DataGenerator.DataGenerator(dict_train_partition, dict_train_label, partition_size, len_train,**params)\n",
    "\n",
    "len_test = X_test.shape[0]\n",
    "test_generator = DataGenerator.DataGenerator(dict_test_partition, dict_test_label, partition_size,len_test, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_size = 9\n",
    "strides = filter_size-1\n",
    "nb_filter=128\n",
    "print(\"Expected defaut dimension for 1st hidden layer : {}\".format(round((vector_dimension-filter_size+2*0)/strides +1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_dimension\n",
    "vocab_size=weight_matrix.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import p9_util\n",
    "embedding_dim=100\n",
    "model = p9_util.keras_cnn_build((vector_dimension, embedding_dim),nbClasses=1, \\\n",
    "                                strides=strides, nb_filter=nb_filter, filter_size=strides, lr=1.e-2,\\\n",
    "                               isWordEmbedding = True, vocab_size = vocab_size,weight_matrix=weight_matrix)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model on dataset\n",
    "history = model.fit_generator(generator=train_generator,\n",
    "                    validation_data=test_generator,\n",
    "                    use_multiprocessing=False,\n",
    "                    workers=4, verbose=1, epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=blus>2.3.2. Building embeddings with Scapy</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scapy_weight_matrix_build(spacy_nlp,llist_token_string):\n",
    "    set_token_string = set()\n",
    "    index = 0\n",
    "    \n",
    "    for list_token_string in llist_token_string :\n",
    "        for token_string in list_token_string :\n",
    "            set_token_string.add(token_string)\n",
    "\n",
    "    vocab_size = len(set_token_string)\n",
    "    vector_dimension = spacy_nlp.vocab.vectors_length\n",
    "    weight_matrix = np.zeros((vocab_size, vector_dimension))\n",
    "    dict_index_tokenstring = dict()\n",
    "    for token_string, index in zip(set_token_string, range(vocab_size)) :\n",
    "        if nlp.vocab.has_vector(token_string):\n",
    "            weight_matrix[index] = nlp.vocab.get_vector(token_string)\n",
    "            dict_index_tokenstring[index] = token_string\n",
    "    return weight_matrix,dict_index_tokenstring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import p9_util\n",
    "\n",
    "\n",
    "nlp = p9_util.SPACY_NLP_MD\n",
    "print(nlp.vocab.length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_name = dataPreparator_train.COLUMN_NAME_TOKEN\n",
    "llist_token = dataPreparator_train.df_data[column_name].tolist()\n",
    "spacy_weigth_matrix, dict_index_token = scapy_weight_matrix_build(nlp,llist_token_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import p9_util\n",
    "\n",
    "embedding_dim=300\n",
    "dimension = spacy_weigth_matrix.shape[1]\n",
    "params = {'dim': (dimension,embedding_dim),\n",
    "          'batch_size': 100,\n",
    "          'n_classes': 0,\n",
    "          'n_channels': 0,\n",
    "          'shuffle': False}\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Generators\n",
    "import DataGenerator\n",
    "len_train = X_train.shape[0]\n",
    "train_generator = DataGenerator.DataGenerator(dict_train_partition, dict_train_label, partition_size, len_train,**params)\n",
    "\n",
    "len_test = X_test.shape[0]\n",
    "test_generator = DataGenerator.DataGenerator(dict_test_partition, dict_test_label, partition_size,len_test, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_size = 9\n",
    "strides = filter_size-1\n",
    "nb_filter=128\n",
    "print(\"Expected defaut dimension for 1st hidden layer : {}\".format(round((vector_dimension-filter_size+2*0)/strides +1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_dimension\n",
    "vocab_size=weight_matrix.shape[0]\n",
    "vector_dimension,vocab_size,embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import p9_util\n",
    "embedding_dim=100\n",
    "model = p9_util.keras_cnn_build((vector_dimension, embedding_dim),nbClasses=1, \\\n",
    "                                strides=strides, nb_filter=nb_filter, filter_size=strides, lr=1.e-2,\\\n",
    "                               isWordEmbedding = True, vocab_size = vocab_size,weight_matrix=weight_matrix)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model on dataset\n",
    "history = model.fit_generator(generator=train_generator,\n",
    "                    validation_data=test_generator,\n",
    "                    use_multiprocessing=False,\n",
    "                    workers=4, verbose=1, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
